%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=0,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{1. Introduction}}

\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Data Science - A practical Approach}
\date{Nov 24, 2021}
\release{}
\author{Lorenz Feyen}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{foreword::doc}}


\sphinxAtStartPar
pdf version can be found \sphinxhref{https://github.com/LorenzF/data-science-practical-approach/raw/main/src/\_build/latex/book.pdf}{here}.


\part{1. Introduction}


\chapter{Introduction}
\label{\detokenize{c1_introduction/introduction:introduction}}\label{\detokenize{c1_introduction/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\section{Structured vs Unstructured}
\label{\detokenize{c1_introduction/introduction:structured-vs-unstructured}}
\sphinxAtStartPar
When performing data preparation an important aspect is to consider with the type of data we are working with.
In general there are 2 types of data, but you could consider a third.


\subsection{Structured data}
\label{\detokenize{c1_introduction/introduction:structured-data}}
\sphinxAtStartPar
Structured data is data that adheres to a pre\sphinxhyphen{}defined data model and is therefore straightforward to analyze.
This data model is the description of our data, each record has to be conform to the model.
A table in a spreadsheet is a good example of the concept of structured data however often no data types are enforced, meaning a column can contain e.g. both numbers and text.
Later we will see that a mixture of data types is often problematic therefor the need of a data model.


\subsection{Unstructured data}
\label{\detokenize{c1_introduction/introduction:unstructured-data}}
\sphinxAtStartPar
In contrast to structured data, there is no apparent data model but this does not mean the data is unusable or cluttered.
Usually it means either no data model has yet been applied or we are dealing with data that is difficult to confine in a model.
A great example of this would be images, or more general (binary) files.
These obviously are hard to sort yet often data structures also contain metadata from these files, with data describing things as when the file was uploaded, what is shown in the file, …
In turn the metadata can be structured and a data model can be related to the unstructured data.


\subsection{Semi\sphinxhyphen{}structured data}
\label{\detokenize{c1_introduction/introduction:semi-structured-data}}
\sphinxAtStartPar
As an intermediate option, we have what is called semi\sphinxhyphen{}structured data.
The reasoning behind this is that the concept of tables is not always applicable, in some occasions e.g. data lakes there is no complex structure present compared to a database.
In a data lake files are stored similar to the folder structure in your computer, with no fancy infrastructure behind it, thus reducing operation costs.
This implies that a data model can not be enforced and the data is stored in generic files.


\section{Data Structures}
\label{\detokenize{c1_introduction/introduction:data-structures}}
\sphinxAtStartPar
There are several structures in which data can be stored and accessed, here we cover the 3 most important.


\subsection{Data Lake}
\label{\detokenize{c1_introduction/introduction:data-lake}}
\sphinxAtStartPar
As mentioned earlier a data lake would be the most cost efficient method as it relies on the least infrastructure and can be serverless.
The concept behind a data lake is straight\sphinxhyphen{}forward, the data is stored in simple files with a specific notation e.g. parquet, csv, xml,…
What is important when designing a data lake would be partitioning, this can be achieved by using subfolders and saving parts of the data in different files.
To make this more tangible, take a look at this symbolic \sphinxhref{https://github.com/LorenzF/data-science-practical-approach/tree/main/src/c2\_data\_preparation/data/temperatures}{example} I provided.
Instead of putting all data in one csv file, subfolder divide the data in Country, City and then the year.
We could even further partition yet the data is here in daily frequency so that would create many small partitions.
The difficulty for a data lake lies in the method of interacting, when adding new data one has to adhere to a agreed upon data model that is not enforced, meaning you could create incorrect data which then need to be cleaned.
On the other hand efficiency of you data lake depends on good partitioning, as the order of divisioning of your folders. We could have also divided first on year and then on country and city.
As a data scientist seeing the data lake might not be as common, as this is rather an engineering task, however using the concepts of a data lake in experimental projects can make a big difference.


\subsection{Database}
\label{\detokenize{c1_introduction/introduction:database}}
\sphinxAtStartPar
Another interesting data structure is the database, widely used for exceptional speeds and ease of use, yet costly in storage.
Numerous implementations of servers using the SQL language are developed over the years with each their own dialect and advantages.
The important take home message here is that you can easily perform queries on the database that pre\sphinxhyphen{}handles the data to retrieve the information you need.
these operations include filtering, grouping categories, joining tables, ordering and much more, as SQL is a complete language on its own.
As a data scientist these databases are much more common, so SQL is a good asset to learn!


\subsection{Data Warehouse}
\label{\detokenize{c1_introduction/introduction:data-warehouse}}
\sphinxAtStartPar
A next step towards data analysis is the data warehouse, where a database is composed of the most pragmatic method of storing your data a data warehouse consist of multiple views on your data.
Based upon the data of a dataset the data warehouse transforms this data into a new format that displays the data in a new way.
Let me illustrate with with a simple example, we have a database with a table that contains the rentals of books from multiple libraries.
This table has a few columns: a timestamp, the library, the action (rent, return, …), the client\_id and the book\_id.
If you would want to know if a book is available this database is perfect for your needs as you just have to find the last event for that book and if its a return the book is (or should be) there.
Now image we would want to know how many books are being rented per month this database is insufficient, yet our data warehouse might contain such a view!
It is up to the data engineer/scientist to create a computation that displays the amount of books rented per month.
If they also would like to subdivided it per category of books, you would need to incorporate another table of the database where information of the books is stored.
More on these operations of a data warehouse will be seen in the data preprocessing chapter.
One last remark about data warehousing, it is important to optimize between memory and computation.
Tables in our data warehouse compared to database can be computed in place reducing memory costs yet increasing computation costs.
If a visualization tool often queries a table in your warehouse it is favorable to create it as a table in your database.


\section{OLTP and OLAP}
\label{\detokenize{c1_introduction/introduction:oltp-and-olap}}
\sphinxAtStartPar
From the previous section you might have deduced that a database and Data Warehouse serve 2 different purposes.
These are denoted as OnLine Transaction Processing and OnLine Analytical Processing, as the names suggest these are used for transactional and analytical processes.


\subsection{OLTP}
\label{\detokenize{c1_introduction/introduction:oltp}}
\sphinxAtStartPar
For this method the database structure is optimal, let us review the example where we have libraries renting out books.
Renting out a book would send a message to our OLTP system creating a new record stating that specific book is at this moment rented out from our library.
OLTP handles day\sphinxhyphen{}to\sphinxhyphen{}day operational data that can be both written and read from our database.


\subsection{OLAP}
\label{\detokenize{c1_introduction/introduction:olap}}
\sphinxAtStartPar
In the case we would like to analyse data from the libraries we would use the OLAP method, creating multi\sphinxhyphen{}dimensional views from our transactional data.
Our dimensions would be the date (aggregated per month), the library and the category of book, the chapter of data preprocessing will use these operations practically.
I could write a whole chapter on OLAP operations however they are well described in \sphinxhref{https://en.wikipedia.org/wiki/OLAP\_cube}{this} wikipedia page.


\part{2. Data Preparation}


\chapter{Introduction}
\label{\detokenize{c2_data_preparation/introduction:introduction}}\label{\detokenize{c2_data_preparation/introduction::doc}}
\sphinxAtStartPar
When performing data science, we often do not elaborate about the preparation that went into the dataset.
It is considered tedious and irrelevant to the story of the analysis,
however it is often the most important part of data analysis.
Data Preparation is the metaphorical foundation of your construction, if you fail to prepare data, you prepare to fail your analysis.
\begin{quote}

\sphinxAtStartPar
Good data beats a fancy algorithm
\end{quote}

\sphinxAtStartPar
If you would perform an analysis and insert unprepared data, you will mostly be disappointed with the result.


\section{why Data Preparation?}
\label{\detokenize{c2_data_preparation/introduction:why-data-preparation}}
\sphinxAtStartPar
Aside from metaphors let us make the reasoning behind this step more tangile, to explain the relevance of this step, we partitioned the answer into a few key points.


\subsection{Accuracy}
\label{\detokenize{c2_data_preparation/introduction:accuracy}}
\sphinxAtStartPar
There is no excuse for incorrect data and accuracy is the most important attribute.
Let us assume that we have a dataset where for some reason the result are not accurate.
This would led us to an analysis where we conclude a result that contains a bias.
An example would be a dataset of sold cars, where the listed price is that of the stock car without options.
Options are not incorporated in the price and we are perhaps training an algorithm that predicts the stock price.
If you as a data scientist fail to report/correct this, your predictions are making sense, but always underestimate!


\subsection{Consistency}
\label{\detokenize{c2_data_preparation/introduction:consistency}}
\sphinxAtStartPar
They usually say something such as ‘consistency is key’ and with data preparation that is likewise true.
A dataset where we do not have consistent results will never converge towards a particular answer.
Note however that it might not be a problem of consistency but rather you are missing crucial information.
If we would have a dataset where local temperatures are logged, we would like to see a consistency each 24 hours.
However we do see there are day to day fluctuations, so perhaps we need to keep track of cloud and rain data to make the dataset more complete.
We could then see that the results are more consistent yet the possibility of outliers is still present.
Equally possible would be that our temperature sensor is not sensitive enough or has large fluctuations in readings, it is the task of the data scientist to figure this out.

\sphinxAtStartPar
To get a visual  about accuracy and consistency this picture might help:

\sphinxAtStartPar
\sphinxincludegraphics{{accuracy}.png}


\subsection{Completeness}
\label{\detokenize{c2_data_preparation/introduction:completeness}}
\sphinxAtStartPar
As hinted in the previous point, completeness is something you have to be aware of.
Having ‘complete’ data is crucial for you narrative to give a correct answer, as you might otherwise lose detail.
Note that you never will know if your data is complete as there might always be more data to mine.
Yet you have to make a consideration between collecting more data and the effort required.
This collecting can happen in multiple methods, as an example we use a survey where we asked several people 10 different questions, we could:
\begin{itemize}
\item {} 
\sphinxAtStartPar
gather new data, here our data grows ‘longer’ by asking the 10 question to more people.
It might be that our sample of people were only students at a campus, so our data was not complete.

\item {} 
\sphinxAtStartPar
gather new feature, by asking more questions to the same people (in case we could still find them).
By doing this we get a better understanding of their opinion, again making our data more complete.

\item {} 
\sphinxAtStartPar
fill missing values, by imputing the abstained questions with answers of similar records.
When someone answered they did not want to answer we could figure out what they would have answered by looking at what persons answered that reply in a similar way.

\end{itemize}


\subsection{Timeliness}
\label{\detokenize{c2_data_preparation/introduction:timeliness}}
\sphinxAtStartPar
For some datasets we are dealing with data that is time related.
It can happen that data at specific timepoints is missing or delayed, resulting in a failure to use machine learning algorithms.
A well\sphinxhyphen{}organised data pipeline utilises techniques of data preparation to circumvent these outages, usually this would be to retain the last successful datapoint.
However in hindsight we could use more complex strategies to fill in these gaps or correct datetimes in our dataset,

\sphinxAtStartPar
In this example the data stream is interrupted and data preparation is there to handle these outages before we can perform analysis.

\sphinxAtStartPar
\sphinxincludegraphics{{timeliness}.png}


\subsection{Believability}
\label{\detokenize{c2_data_preparation/introduction:believability}}
\sphinxAtStartPar
You could collect the most intricate dataset possible, but if the narrative that you are conducting contradicts itself, you will end up nowhere.
During the process of data analytics it is important to apply a critical mind to what your dataset is telling you.
Obviously this is not a reason to mask or mold the data so it agrees with your opinion.
Rather you should be wary when conflicts happen and act accordingly, unfortunately it is impossible to write a generic tactic for this.
As a data scientist your experience of the underlying subject should help create understanding of the topic, remember, gathering information from experts in the field is crucial here!


\subsection{Interpretability}
\label{\detokenize{c2_data_preparation/introduction:interpretability}}
\sphinxAtStartPar
Another problem that might arise when you are diving deep into the data might be that you have created something no human could ever interpret.
The Machine Learning algorithms outputs plausible and believable results, but it is impossible to understand the reasoning behind.
For some this is perfectly acceptible, for some this is undesirable.
It is your task as a data scientist to cater the wishes of the product operator and if they desire understanding as they would like to learn from the data driven process you need to unfold the process.
Usually this comes down to which data transformations are used as some do produce an output that only makes mathematical sense.


\subsection{In conclusion}
\label{\detokenize{c2_data_preparation/introduction:in-conclusion}}
\sphinxAtStartPar
There are multiple ways to deteriorate the quality of your data and raw formats of data often contain multiple.
Before we can do anything with it these problems need to be resolved, if you fail to do so, the final output fails too.




\section{Further reading}
\label{\detokenize{c2_data_preparation/introduction:further-reading}}
\sphinxAtStartPar
\sphinxhref{https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4}{Towards Data Science}


\chapter{Missing Data}
\label{\detokenize{c2_data_preparation/missing_data:missing-data}}\label{\detokenize{c2_data_preparation/missing_data::doc}}
\sphinxAtStartPar
In this notebook we will look at a few datasets where values from columns are missing.
It is crucial for data science and machine learning to have a dataset where no values are missing as algorithms are usually not able to handle data with information missing.

\sphinxAtStartPar
For python, we will be using the pandas library to handle our dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}


\section{Kamyr digester}
\label{\detokenize{c2_data_preparation/missing_data:kamyr-digester}}
\sphinxAtStartPar
The first dataset we will be looking at is taken from a physical device equiped with numerous sensors, each timepoint (1 hour) these sensors are read out and the data is collected. Let’s have a look at the general structure

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/kamyr\PYGZhy{}digester.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  Observation  Y\PYGZhy{}Kappa  ChipRate  BF\PYGZhy{}CMratio  BlowFlow  ChipLevel4   \PYGZbs{}
0    31\PYGZhy{}00:00    23.10    16.520     121.717  1177.607      169.805   
1    31\PYGZhy{}01:00    27.60    16.810      79.022  1328.360      341.327   
2    31\PYGZhy{}02:00    23.19    16.709      79.562  1329.407      239.161   
3    31\PYGZhy{}03:00    23.60    16.478      81.011  1334.877      213.527   
4    31\PYGZhy{}04:00    22.90    15.618      93.244  1334.168      243.131   

   T\PYGZhy{}upperExt\PYGZhy{}2   T\PYGZhy{}lowerExt\PYGZhy{}2    UCZAA  WhiteFlow\PYGZhy{}4   ...  SteamFlow\PYGZhy{}4   \PYGZbs{}
0        358.282         329.545  1.443       599.253  ...        67.122   
1        351.050         329.067  1.549       537.201  ...        60.012   
2        350.022         329.260  1.600       549.611  ...        61.304   
3        350.938         331.142  1.604       623.362  ...        68.496   
4        351.640         332.709    NaN       638.672  ...        70.022   

   Lower\PYGZhy{}HeatT\PYGZhy{}3  Upper\PYGZhy{}HeatT\PYGZhy{}3   ChipMass\PYGZhy{}4   WeakLiquorF   BlackFlow\PYGZhy{}2   \PYGZbs{}
0        329.432         303.099      175.964      1127.197      1319.039   
1        330.823         304.879      163.202       665.975      1297.317   
2        329.140         303.383      164.013       677.534      1327.072   
3        328.875         302.254      181.487       767.853      1324.461   
4        328.352         300.954      183.929       888.448      1343.424   

   WeakWashF   SteamHeatF\PYGZhy{}3   T\PYGZhy{}Top\PYGZhy{}Chips\PYGZhy{}4   SulphidityL\PYGZhy{}4   
0     257.325         54.612         252.077             NaN  
1     241.182         46.603         251.406           29.11  
2     237.272         51.795         251.335             NaN  
3     239.478         54.846         250.312           29.02  
4     215.372         54.186         249.916           29.01  

[5 rows x 23 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Interesting, there seem to be 22 sensor values and 1 timestamp for each record. As mechanical devices are prone to noise and dropouts of sensors we would be foolish to assume no missing values are present.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{divide}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{kamyr\PYGZus{}df}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{100}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Observation         0.00
Y\PYGZhy{}Kappa             0.00
ChipRate            1.33
BF\PYGZhy{}CMratio          4.65
BlowFlow            4.32
ChipLevel4          0.33
T\PYGZhy{}upperExt\PYGZhy{}2        0.33
T\PYGZhy{}lowerExt\PYGZhy{}2        0.33
UCZAA               7.97
WhiteFlow\PYGZhy{}4         0.33
AAWhiteSt\PYGZhy{}4        46.84
AA\PYGZhy{}Wood\PYGZhy{}4           0.33
ChipMoisture\PYGZhy{}4      0.33
SteamFlow\PYGZhy{}4         0.33
Lower\PYGZhy{}HeatT\PYGZhy{}3       0.33
Upper\PYGZhy{}HeatT\PYGZhy{}3       0.33
ChipMass\PYGZhy{}4          0.33
WeakLiquorF         0.33
BlackFlow\PYGZhy{}2         0.33
WeakWashF           0.33
SteamHeatF\PYGZhy{}3        0.33
T\PYGZhy{}Top\PYGZhy{}Chips\PYGZhy{}4       0.33
SulphidityL\PYGZhy{}4      46.84
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
As expected, the datapoint ‘AAWhiteSt\sphinxhyphen{}4’ even has 46\% of data missing!
It seems we only have 300 datapoints and presumably these missing values occur in different records our dataset will be decimated if we just drop all rows with missing values.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(301, 23)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(131, 23)
\end{sphinxVerbatim}

\sphinxAtStartPar
As we drop all rows with missing values, we are left with only 131 records.
Whilst this might be good enough for some purposes, there are more viable options.

\sphinxAtStartPar
Perhaps we can first remove the column with the most missing values and then drop all remaining

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AAWhiteSt\PYGZhy{}4 }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SulphidityL\PYGZhy{}4 }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(263, 21)
\end{sphinxVerbatim}

\sphinxAtStartPar
Significantly better, although we lost the information of 2 sensors we now have a complete dataset with 263 records. For purposes where those 2 sensors are irrelevant this is a viable option, keep in mind that this dataset is still 100\% truthful, as we have not imputed any values.

\sphinxAtStartPar
Another option, where we retain all our records would be using the timely nature of our dataset, each record is a measurement with an interval of 1 hour. I have no knowledge of this dataset but one might make the assumption that the interval of 1 hour is taken as the state of the machine does not alter much in 1 hour. Therefore we could do what is called a forward fill, where we fill in the missing values with the same value of the sensor for the previous measurement.

\sphinxAtStartPar
This would solve nearly all nan values as there might be a problem where the first value is missing. This is shown below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{fillna}\PYG{p}{(}\PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ffill}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SulphidityL\PYGZhy{}4 }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0        NaN
1      29.11
2      29.11
3      29.02
4      29.01
       ...  
296    30.43
297    30.29
298    30.47
299    30.47
300    30.46
Name: SulphidityL\PYGZhy{}4 , Length: 301, dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
Although our dataset is not fully the truth, we can see that little to no changes occur in the sensor and using a forward fill is arguably the most suitable option.


\section{Travel times}
\label{\detokenize{c2_data_preparation/missing_data:travel-times}}
\sphinxAtStartPar
Another dataset from the same source contains a collection of recorded travel times and specific information about the travel itself as e.g.: the day of the week, where they were going, …

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/travel\PYGZhy{}times.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{travel\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
          Date StartTime  DayOfWeek GoingTo  Distance  MaxSpeed  AvgSpeed  \PYGZbs{}
0     1/6/2012     16:37     Friday    Home     51.29     127.4      78.3   
1     1/6/2012     08:20     Friday     GSK     51.63     130.3      81.8   
2     1/4/2012     16:17  Wednesday    Home     51.27     127.4      82.0   
3     1/4/2012     07:53  Wednesday     GSK     49.17     132.3      74.2   
4     1/3/2012     18:57    Tuesday    Home     51.15     136.2      83.4   
..         ...       ...        ...     ...       ...       ...       ...   
200  7/18/2011     08:09     Monday     GSK     54.52     125.6      49.9   
201  7/14/2011     08:03   Thursday     GSK     50.90     123.7      76.2   
202  7/13/2011     17:08  Wednesday    Home     51.96     132.6      57.5   
203  7/12/2011     17:51    Tuesday    Home     53.28     125.8      61.6   
204  7/11/2011     16:56     Monday    Home     51.73     125.0      62.8   

     AvgMovingSpeed FuelEconomy  TotalTime  MovingTime Take407All Comments  
0              84.8         NaN       39.3        36.3         No      NaN  
1              88.9         NaN       37.9        34.9         No      NaN  
2              85.8         NaN       37.5        35.9         No      NaN  
3              82.9         NaN       39.8        35.6         No      NaN  
4              88.1         NaN       36.8        34.8         No      NaN  
..              ...         ...        ...         ...        ...      ...  
200            82.4        7.89       65.5        39.7         No      NaN  
201            95.1        7.89       40.1        32.1        Yes      NaN  
202            76.7         NaN       54.2        40.6        Yes      NaN  
203            87.6         NaN       51.9        36.5        Yes      NaN  
204            92.5         NaN       49.5        33.6        Yes      NaN  

[205 rows x 13 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
we have a total of 205 records and we can already see that the FuelEconomy column seems pretty bad, let’s quantify that.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{divide}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{travel\PYGZus{}df}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{100}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Date               0.00
StartTime          0.00
DayOfWeek          0.00
GoingTo            0.00
Distance           0.00
MaxSpeed           0.00
AvgSpeed           0.00
AvgMovingSpeed     0.00
FuelEconomy        8.29
TotalTime          0.00
MovingTime         0.00
Take407All         0.00
Comments          88.29
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
In the end, it doesn’t seem that bad, but there are comments and nearly none of them are filled in. Which in perspective is understandable. Let’s see what the comments look like

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{o}{\PYGZti{}}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{Comments}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
15                                  Put snow tires on
39                                         Heavy rain
49                                Huge traffic backup
50      Pumped tires up: check fuel economy improved?
52                                Backed up at Bronte
54                                Backed up at Bronte
60                                              Rainy
78                                   Rain, rain, rain
91                                   Rain, rain, rain
92         Accident: backup from Hamilton to 407 ramp
110                                           Raining
132                           Back to school traffic?
133                Took 407 all the way (to McMaster)
150                             Heavy volume on Derry
156                        Start early to run a batch
158    Accident at 403/highway 6; detour along Dundas
165                                      Detour taken
166                                    Must be Friday
172                             Medium amount of rain
174                                         New tires
182                              Turn around on Derry
184                                       Empty roads
187                            Police slowdown on 403
189                         Accident blocked 407 exit
Name: Comments, dtype: object
\end{sphinxVerbatim}

\sphinxAtStartPar
As you would expect, these comments are text based. Now imagine we would like to run some Natural Language Processing (NLP) on these, it would be a pain to perform string operations on it when it is riddled with missing values.

\sphinxAtStartPar
Here a simple example where we select all records containing the word ‘rain’, with no avail.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{lower}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ValueError}\PYG{g+gWhitespace}{                                }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}6376}\PYG{o}{/}\PYG{l+m+mf}{1298831137.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{lower}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/frame.py} in \PYG{n+ni}{\PYGZus{}\PYGZus{}getitem\PYGZus{}\PYGZus{}}\PYG{n+nt}{(self, key)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{3446} 
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{3447}         \PYG{c+c1}{\PYGZsh{} Do we have a (boolean) 1d indexer?}
\PYG{n+ne}{\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{3448}         \PYG{k}{if} \PYG{n}{com}\PYG{o}{.}\PYG{n}{is\PYGZus{}bool\PYGZus{}indexer}\PYG{p}{(}\PYG{n}{key}\PYG{p}{)}\PYG{p}{:}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{3449}             \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}getitem\PYGZus{}bool\PYGZus{}array}\PYG{p}{(}\PYG{n}{key}\PYG{p}{)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{3450} 

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/common.py} in \PYG{n+ni}{is\PYGZus{}bool\PYGZus{}indexer}\PYG{n+nt}{(key)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{137}                     \PYG{c+c1}{\PYGZsh{} Don\PYGZsq{}t raise on e.g. [\PYGZdq{}A\PYGZdq{}, \PYGZdq{}B\PYGZdq{}, np.nan], see}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{138}                     \PYG{c+c1}{\PYGZsh{}  test\PYGZus{}loc\PYGZus{}getitem\PYGZus{}list\PYGZus{}of\PYGZus{}labels\PYGZus{}categoricalindex\PYGZus{}with\PYGZus{}na}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{139}                     \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{n}{na\PYGZus{}msg}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{140}                 \PYG{k}{return} \PYG{k+kc}{False}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{141}             \PYG{k}{return} \PYG{k+kc}{True}

\PYG{n+ne}{ValueError}: Cannot mask with non\PYGZhy{}boolean array containing NA / NaN values
\end{sphinxVerbatim}

\sphinxAtStartPar
The last line of the python error traceback gives us the reason it failed, because there were NaN values present.

\sphinxAtStartPar
Luckily the string variable has more or less it’s on ‘null’ value, being an empty string, this way these operations are still possible, most of the comments will just contain nothing.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments} \PYG{o}{=} \PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{fillna}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{lower}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date StartTime  DayOfWeek GoingTo  Distance  MaxSpeed  AvgSpeed  \PYGZbs{}
39   11/29/2011     07:23    Tuesday     GSK     51.74     112.2      55.3   
60    11/9/2011     16:15  Wednesday    Home     51.28     121.4      65.9   
78   10/25/2011     17:24    Tuesday    Home     52.87     123.5      65.1   
91   10/12/2011     17:47  Wednesday    Home     51.40     114.4      59.7   
110   9/27/2011     07:36    Tuesday     GSK     50.65     128.1      86.3   
172    8/9/2011     08:15    Tuesday     GSK     49.08     134.8      60.5   

     AvgMovingSpeed FuelEconomy  TotalTime  MovingTime Take407All  \PYGZbs{}
39             61.0         NaN       56.2        50.9         No   
60             71.8        9.35       46.7        42.1         No   
78             72.4        8.97       48.7        43.8         No   
91             65.8        8.75       51.7        46.9         No   
110            88.6        8.31       35.2        34.3        Yes   
172            67.2        8.54       48.7        43.8         No   

                  Comments  
39              Heavy rain  
60                   Rainy  
78        Rain, rain, rain  
91        Rain, rain, rain  
110                Raining  
172  Medium amount of rain  
\end{sphinxVerbatim}

\sphinxAtStartPar
Fixed! now we can use the comments for analysis.

\sphinxAtStartPar
We still have to fix the FuelEconomy, let us take a look at the non NaN values

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{o}{\PYGZti{}}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{FuelEconomy}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date StartTime  DayOfWeek GoingTo  Distance  MaxSpeed  AvgSpeed  \PYGZbs{}
6      1/2/2012     17:31     Monday    Home     51.37     123.2      82.9   
7      1/2/2012     07:34     Monday     GSK     49.01     128.3      77.5   
8    12/23/2011     08:01     Friday     GSK     52.91     130.3      80.9   
9    12/22/2011     17:19   Thursday    Home     51.17     122.3      70.6   
10   12/22/2011     08:16   Thursday     GSK     49.15     129.4      74.0   
..          ...       ...        ...     ...       ...       ...       ...   
197   7/20/2011     08:24  Wednesday     GSK     48.50     125.8      75.7   
198   7/19/2011     17:17    Tuesday    Home     51.16     126.7      92.2   
199   7/19/2011     08:11    Tuesday     GSK     50.96     124.3      82.3   
200   7/18/2011     08:09     Monday     GSK     54.52     125.6      49.9   
201   7/14/2011     08:03   Thursday     GSK     50.90     123.7      76.2   

     AvgMovingSpeed FuelEconomy  TotalTime  MovingTime Take407All Comments  
6              87.3           \PYGZhy{}       37.2        35.3         No           
7              85.9           \PYGZhy{}       37.9        34.3         No           
8              88.3        8.89       39.3        36.0         No           
9              78.1        8.89       43.5        39.3         No           
10             81.4        8.89       39.8        36.2         No           
..              ...         ...        ...         ...        ...      ...  
197            87.3        7.89       38.5        33.3        Yes           
198           102.6        7.89       33.3        29.9        Yes           
199            96.4        7.89       37.2        31.7        Yes           
200            82.4        7.89       65.5        39.7         No           
201            95.1        7.89       40.1        32.1        Yes           

[188 rows x 13 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems that aside NaN values there are also other intruders, a quick check on the data type (Dtype) reveils it is not recognised as a number!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 205 entries, 0 to 204
Data columns (total 13 columns):
 \PYGZsh{}   Column          Non\PYGZhy{}Null Count  Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}          \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   Date            205 non\PYGZhy{}null    object 
 1   StartTime       205 non\PYGZhy{}null    object 
 2   DayOfWeek       205 non\PYGZhy{}null    object 
 3   GoingTo         205 non\PYGZhy{}null    object 
 4   Distance        205 non\PYGZhy{}null    float64
 5   MaxSpeed        205 non\PYGZhy{}null    float64
 6   AvgSpeed        205 non\PYGZhy{}null    float64
 7   AvgMovingSpeed  205 non\PYGZhy{}null    float64
 8   FuelEconomy     188 non\PYGZhy{}null    object 
 9   TotalTime       205 non\PYGZhy{}null    float64
 10  MovingTime      205 non\PYGZhy{}null    float64
 11  Take407All      205 non\PYGZhy{}null    object 
 12  Comments        205 non\PYGZhy{}null    object 
dtypes: float64(6), object(7)
memory usage: 20.9+ KB
\end{sphinxVerbatim}

\sphinxAtStartPar
The column is noted as an object or string type, meaning that these numbers are given as ‘9.24’ instead of 9.24 and numerical operations are not possible.
We can cast them to numeric but have to warn pandas to coerce errors, meaning errors will be converted to NaN values.
Later we’ll handle the NaN’s.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{FuelEconomy} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}numeric}\PYG{p}{(}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{FuelEconomy}\PYG{p}{,} \PYG{n}{errors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{coerce}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 205 entries, 0 to 204
Data columns (total 13 columns):
 \PYGZsh{}   Column          Non\PYGZhy{}Null Count  Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}          \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   Date            205 non\PYGZhy{}null    object 
 1   StartTime       205 non\PYGZhy{}null    object 
 2   DayOfWeek       205 non\PYGZhy{}null    object 
 3   GoingTo         205 non\PYGZhy{}null    object 
 4   Distance        205 non\PYGZhy{}null    float64
 5   MaxSpeed        205 non\PYGZhy{}null    float64
 6   AvgSpeed        205 non\PYGZhy{}null    float64
 7   AvgMovingSpeed  205 non\PYGZhy{}null    float64
 8   FuelEconomy     186 non\PYGZhy{}null    float64
 9   TotalTime       205 non\PYGZhy{}null    float64
 10  MovingTime      205 non\PYGZhy{}null    float64
 11  Take407All      205 non\PYGZhy{}null    object 
 12  Comments        205 non\PYGZhy{}null    object 
dtypes: float64(7), object(6)
memory usage: 20.9+ KB
\end{sphinxVerbatim}

\sphinxAtStartPar
Wonderful, now the column is numerical and we can see 2 more missing values have popped up!
We could easily drop these 19 records and have a complete dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date StartTime  DayOfWeek GoingTo  Distance  MaxSpeed  AvgSpeed  \PYGZbs{}
8    12/23/2011     08:01     Friday     GSK     52.91     130.3      80.9   
9    12/22/2011     17:19   Thursday    Home     51.17     122.3      70.6   
10   12/22/2011     08:16   Thursday     GSK     49.15     129.4      74.0   
11   12/21/2011     07:45  Wednesday     GSK     51.77     124.8      71.7   
12   12/20/2011     16:05    Tuesday    Home     51.45     130.1      75.2   
..          ...       ...        ...     ...       ...       ...       ...   
197   7/20/2011     08:24  Wednesday     GSK     48.50     125.8      75.7   
198   7/19/2011     17:17    Tuesday    Home     51.16     126.7      92.2   
199   7/19/2011     08:11    Tuesday     GSK     50.96     124.3      82.3   
200   7/18/2011     08:09     Monday     GSK     54.52     125.6      49.9   
201   7/14/2011     08:03   Thursday     GSK     50.90     123.7      76.2   

     AvgMovingSpeed  FuelEconomy  TotalTime  MovingTime Take407All Comments  
8              88.3         8.89       39.3        36.0         No           
9              78.1         8.89       43.5        39.3         No           
10             81.4         8.89       39.8        36.2         No           
11             78.9         8.89       43.3        39.4         No           
12             82.7         8.89       41.1        37.3         No           
..              ...          ...        ...         ...        ...      ...  
197            87.3         7.89       38.5        33.3        Yes           
198           102.6         7.89       33.3        29.9        Yes           
199            96.4         7.89       37.2        31.7        Yes           
200            82.4         7.89       65.5        39.7         No           
201            95.1         7.89       40.1        32.1        Yes           

[186 rows x 13 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
However im leaving them as an excercise for you to apply a technique we will see in the next part


\section{Material properties}
\label{\detokenize{c2_data_preparation/missing_data:material-properties}}
\sphinxAtStartPar
Another dataset from the same source contains the material properties from 30 samples, this time there is not timestamp as the samples are not related in time with each other.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{material\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/raw\PYGZhy{}material\PYGZhy{}properties.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{material\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    Sample  size1  size2  size3  density1  density2  density3
0   X12558  0.696   2.69   6.38      41.8     17.18      3.90
1   X14728  0.636   2.30   5.14      38.1     12.73      3.89
2   X15468  0.841   2.85   5.20      37.6     13.58      3.98
3   X21364  0.609   2.13   4.62      34.2     11.12      4.02
4   X23671  0.684   2.16   4.87      36.4     12.24      3.92
5   X24055  0.762   2.81   6.36      38.1     13.28      3.89
6   X24905  0.552   2.34   5.03      41.3     16.71      3.86
7   X25917  0.501   2.17   5.09       NaN       NaN       NaN
8   X27871  0.619   2.11   5.13       NaN       NaN       NaN
9   X28690  0.610   2.10   4.18      35.0     12.15      3.86
10  X31385  0.532   2.09   4.93       NaN       NaN       NaN
11  X31813  0.738   2.29   5.47       NaN       NaN       NaN
12  X32807  0.779   2.62   5.59       NaN       NaN       NaN
13  X33943  0.537   2.23   5.41      35.2     11.34      3.99
14  X35035  0.702   2.05   5.10      34.2     10.54      4.02
15  X39223  0.768   2.51   5.09      34.9     12.55      3.90
16  X40503  0.714   2.56   6.03      35.6     12.20      4.02
17  X41400  0.621   2.42   5.10      38.7     14.27      3.98
18  X42988  0.726   2.11   4.69      37.1     13.14      3.98
19  X44749  0.698   2.36   5.40      36.6     12.16      4.01
20  X45295    NaN    NaN    NaN      38.1     13.34      3.89
21  X46965  0.759   2.47   4.83      38.7     14.83      3.89
22  X49666  0.535   2.13   5.23       NaN       NaN       NaN
23  X50678  0.716   2.29   5.45      37.3     13.70      3.92
24  X52894  0.635   2.08   4.94       NaN       NaN       NaN
25  X53925  0.598   2.12   4.69      37.9     13.45      3.78
26  X54254  0.700   2.47   5.22      38.8     14.72      3.92
27  X54272  0.957   2.96   7.37      36.2     13.38      4.20
28  X54394  0.759   2.66   5.36      35.2     12.19      3.98
29  X55408  0.661   2.10   4.27       NaN       NaN       NaN
30  X56952  0.646   2.38   4.51      40.1     15.68      3.86
31  X57095  0.662   2.34   4.71      35.0     12.37      3.90
32  X57128  0.749   2.43   5.16      37.3     13.04      3.92
33  X61870  0.598   2.21   4.90       NaN       NaN       NaN
34  X61888  0.619   2.59   5.81       NaN       NaN       NaN
35  X72736  0.693   2.05   5.02      39.6     15.55      3.94
\end{sphinxVerbatim}

\sphinxAtStartPar
let us quantify the amount of missing data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{material\PYGZus{}df}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{divide}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{material\PYGZus{}df}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{100}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Sample       0.00
size1        2.78
size2        2.78
size3        2.78
density1    27.78
density2    27.78
density3    27.78
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
Unfortunately that is a lot of missing data, covered in all records, dropping here seems almost impossible if we want to keep a healthy amount of records.

\sphinxAtStartPar
Here it would be wise to go for a more elaborate method of imputation, I opted for the K\sphinxhyphen{}nearest neighbours method, which looks at the K most similar records in the dataset to make an educated guess on what the missing value could be, this because we can assume that records with similar data are also similar over all the properties (columns).

\sphinxAtStartPar
Im using the sklearn library for this, which has more imputation techniques such as MICE.
More info can be found \sphinxhref{https://scikit-learn.org/stable/modules/impute.html}{here}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{impute} \PYG{k+kn}{import} \PYG{n}{KNNImputer}
\end{sphinxVerbatim}

\sphinxAtStartPar
im creating an imputer object and specify that i want to use the 5 most similar records and weigh them by distance from the to imputed record, meaning closer neighbours are more important.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{imputer} \PYG{o}{=} \PYG{n}{KNNImputer}\PYG{p}{(}\PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{weights}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{distance}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
As the imputer only takes numerical values I had to do some pandas magic and drop the first column, which I then added again. The result is a fully filled dataset, you can recognise the new values as they are not rounded.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}
    \PYG{n}{imputer}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{material\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} 
    \PYG{n}{columns}\PYG{o}{=}\PYG{n}{material\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       size1     size2     size3   density1   density2  density3
0   0.696000  2.690000  6.380000  41.800000  17.180000  3.900000
1   0.636000  2.300000  5.140000  38.100000  12.730000  3.890000
2   0.841000  2.850000  5.200000  37.600000  13.580000  3.980000
3   0.609000  2.130000  4.620000  34.200000  11.120000  4.020000
4   0.684000  2.160000  4.870000  36.400000  12.240000  3.920000
5   0.762000  2.810000  6.360000  38.100000  13.280000  3.890000
6   0.552000  2.340000  5.030000  41.300000  16.710000  3.860000
7   0.501000  2.170000  5.090000  38.495282  14.029399  3.931180
8   0.619000  2.110000  5.130000  37.405275  13.157346  3.943667
9   0.610000  2.100000  4.180000  35.000000  12.150000  3.860000
10  0.532000  2.090000  4.930000  37.811132  13.646072  3.908364
11  0.738000  2.290000  5.470000  37.088833  13.255412  3.941654
12  0.779000  2.620000  5.590000  36.540567  12.889902  3.970973
13  0.537000  2.230000  5.410000  35.200000  11.340000  3.990000
14  0.702000  2.050000  5.100000  34.200000  10.540000  4.020000
15  0.768000  2.510000  5.090000  34.900000  12.550000  3.900000
16  0.714000  2.560000  6.030000  35.600000  12.200000  4.020000
17  0.621000  2.420000  5.100000  38.700000  14.270000  3.980000
18  0.726000  2.110000  4.690000  37.100000  13.140000  3.980000
19  0.698000  2.360000  5.400000  36.600000  12.160000  4.010000
20  0.733097  2.653959  5.881504  38.100000  13.340000  3.890000
21  0.759000  2.470000  4.830000  38.700000  14.830000  3.890000
22  0.535000  2.130000  5.230000  37.391815  13.089536  3.944335
23  0.716000  2.290000  5.450000  37.300000  13.700000  3.920000
24  0.635000  2.080000  4.940000  37.254724  13.206262  3.933904
25  0.598000  2.120000  4.690000  37.900000  13.450000  3.780000
26  0.700000  2.470000  5.220000  38.800000  14.720000  3.920000
27  0.957000  2.960000  7.370000  36.200000  13.380000  4.200000
28  0.759000  2.660000  5.360000  35.200000  12.190000  3.980000
29  0.661000  2.100000  4.270000  36.172345  12.755632  3.887375
30  0.646000  2.380000  4.510000  40.100000  15.680000  3.860000
31  0.662000  2.340000  4.710000  35.000000  12.370000  3.900000
32  0.749000  2.430000  5.160000  37.300000  13.040000  3.920000
33  0.598000  2.210000  4.900000  37.865882  13.826029  3.887021
34  0.619000  2.590000  5.810000  35.932339  12.318210  3.989911
35  0.693000  2.050000  5.020000  39.600000  15.550000  3.940000
\end{sphinxVerbatim}

\sphinxAtStartPar
This concludes the part of missing values, perhaps you can try yourself and impute the missing values for the FuelEconomy using the SimpleImputer or even the IterativeImputer.


\chapter{Concatenation and deduplication}
\label{\detokenize{c2_data_preparation/concatenation_deduplication:concatenation-and-deduplication}}\label{\detokenize{c2_data_preparation/concatenation_deduplication::doc}}
\sphinxAtStartPar
In this notebook we are going to investigate the concepts of stitching data files (concatenation) and verifying the integrity of our data concercing duplicates


\section{Concatenation}
\label{\detokenize{c2_data_preparation/concatenation_deduplication:concatenation}}
\sphinxAtStartPar
When dealing with large amounts of data, fractioning is often the only solution.
Not only does this tidy up your data space, but it also benefits computation.
Aside from that, appending new data to your data lake is independent of the historical data.
However if you want to perform historical analysis this means you will need to perform additional operations.

\sphinxAtStartPar
In this notebook we have a setup of a very small data lake containing daily minimal temperatures.
If you would look closely in the url you would see the following structure.
\begin{quote}

\sphinxAtStartPar
data/temperature/australia/melbourne/1981.csv
\end{quote}

\sphinxAtStartPar
This is a straight\sphinxhyphen{}forward but perfect example on how fragmentation works, in our data lake we have:
temperatures data fractioned by country, city and year. As we are working with daily temperatures further fractioning would not be interesting, but you could fraction e.g. per month.

\sphinxAtStartPar
In the cells below, we read our both 1981 and 1982 data and concatenate them using python.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{melbourne\PYGZus{}1981\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/temperatures/australia/melbourne/1981.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{melbourne\PYGZus{}1982\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/temperatures/australia/melbourne/1982.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}
    \PYG{p}{[}
        \PYG{n}{melbourne\PYGZus{}1981\PYGZus{}df}\PYG{p}{,}
        \PYG{n}{melbourne\PYGZus{}1982\PYGZus{}df}\PYG{p}{,}
    \PYG{p}{]}
\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date  Temp
0    1981\PYGZhy{}01\PYGZhy{}01  20.7
1    1981\PYGZhy{}01\PYGZhy{}02  17.9
2    1981\PYGZhy{}01\PYGZhy{}03  18.8
3    1981\PYGZhy{}01\PYGZhy{}04  14.6
4    1981\PYGZhy{}01\PYGZhy{}05  15.8
..          ...   ...
360  1982\PYGZhy{}12\PYGZhy{}27  15.3
361  1982\PYGZhy{}12\PYGZhy{}28  16.3
362  1982\PYGZhy{}12\PYGZhy{}29  15.8
363  1982\PYGZhy{}12\PYGZhy{}30  17.7
364  1982\PYGZhy{}12\PYGZhy{}31  16.3

[730 rows x 2 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
And there you have it! we now have a dataframe containing both data from 1981 as 1982.
Can you figure out what I calculated in the next cell? Do you think there might be a more ‘clean’ solution?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{Date}\PYG{o}{.}\PYG{n}{str}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{:}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{Temp}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
17.140322580645158
\end{sphinxVerbatim}

\sphinxAtStartPar
As an exercise I would ask you now to create a small python script that given a begin and end year (between 1981 and 1990) can automatically concatenate all the necessary data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1982}\PYG{p}{,}\PYG{l+m+mi}{1987}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{i}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1982
1983
1984
1985
1986
\end{sphinxVerbatim}


\section{Deduplication}
\label{\detokenize{c2_data_preparation/concatenation_deduplication:deduplication}}
\sphinxAtStartPar
Another important aspect of data cleaning is the removal of duplicates.
Here we fragment of a dataset from activity on a popular games platform.
We can see which user has either bought or played specific games and how often.
Unfortunately for some reason, entries might have duplicates which we have to deal with as otherwise users might have e.g. bought a game twice.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/steam.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game    action  freq
0      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
1      11373749                   Sid Meier\PYGZsq{}s Civilization IV      play   0.1
2      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
3      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
4      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
...         ...                                           ...       ...   ...
1834  112845094                                        Arma 2  purchase   1.0
1835  112845094                  Grand Theft Auto San Andreas  purchase   1.0
1836  112845094                    Grand Theft Auto Vice City  purchase   1.0
1837  112845094                    Grand Theft Auto Vice City  purchase   1.0
1838  112845094                          Grand Theft Auto III  purchase   1.0

[1839 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
We have a dataframe with 1839 interactions, you can see that the freq either notes the amount they bought (which always 1 as there is not use in buying it more) or the amount in hours they played.

\sphinxAtStartPar
Let us straightforward ask pandas to remove all rows that have an exact duplicate

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game    action  freq
0      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
1      11373749                   Sid Meier\PYGZsq{}s Civilization IV      play   0.1
3      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
5      11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
7      56038151                       Tom Clancy\PYGZsq{}s H.A.W.X. 2  purchase   1.0
...         ...                                           ...       ...   ...
1831  112845094                  Grand Theft Auto San Andreas  purchase   1.0
1832  112845094                  Grand Theft Auto San Andreas      play   0.2
1833  112845094                          Grand Theft Auto III  purchase   1.0
1834  112845094                                        Arma 2  purchase   1.0
1836  112845094                    Grand Theft Auto Vice City  purchase   1.0

[1132 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Alright! this seemed to have dropped 707 rows from our dataset, but we would like to know more about those.
Let’s ask which rows the algorithm has dropped:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game    action  freq
2      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
4      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
6      11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
10     56038151                  Grand Theft Auto San Andreas  purchase   1.0
12     56038151                    Grand Theft Auto Vice City  purchase   1.0
...         ...                                           ...       ...   ...
1827   39146470          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
1830   48666962                                      Crysis 2  purchase   1.0
1835  112845094                  Grand Theft Auto San Andreas  purchase   1.0
1837  112845094                    Grand Theft Auto Vice City  purchase   1.0
1838  112845094                          Grand Theft Auto III  purchase   1.0

[707 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we can see the duplicates, no particular pattern seems to be present, we could just for curiosity count the games that are duplicated

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{game}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Grand Theft Auto San Andreas                    172
Grand Theft Auto Vice City                      103
Sid Meier\PYGZsq{}s Civilization IV                      98
Grand Theft Auto III                             90
Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword     80
Sid Meier\PYGZsq{}s Civilization IV Warlords             79
Sid Meier\PYGZsq{}s Civilization IV Colonization         75
Crysis 2                                          7
Arma 2                                            1
Tom Clancy\PYGZsq{}s H.A.W.X. 2                           1
TERA                                              1
Name: game, dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems there are some games which are very prone to being duplicated, at this point we could go and ask the IT department why these games are acting weird.

\sphinxAtStartPar
Another thing im interested about is the perspective of a single gamer, here we took a single user\_id and printed all his games

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{user\PYGZus{}id} \PYG{o}{==} \PYG{l+m+mi}{11373749}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    user\PYGZus{}id                                          game    action  freq
0  11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
1  11373749                   Sid Meier\PYGZsq{}s Civilization IV      play   0.1
2  11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
3  11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
4  11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
5  11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
6  11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
\end{sphinxVerbatim}

\sphinxAtStartPar
Ah, you can see all of his three games are somehow duplicated in purchase, also it seems he only played one of them for only 0.1 hours.
Looks like he fell to the bait of a tempting summer sale but didn’t realise he had no time to actually play it.

\sphinxAtStartPar
Another thing I would like to mention here is that this dataset would make a fine recommender system as it contains user ids and hours played.
Add game metadata (description) and reviews to the mix and your data preparation is done!

\sphinxAtStartPar
We can remove all duplicates now by overwriting our dataframe

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
One thing still bothers me, as hours played can change over time it might be that different snapshots have produced different values, therefore more duplicates might be present with different hours\_played.

\sphinxAtStartPar
Time to investigate this by using a subset of columns in the drop\_duplicates algorithm

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{game}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{action}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game    action  freq
0      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
1      11373749                   Sid Meier\PYGZsq{}s Civilization IV      play   0.1
3      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
5      11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
7      56038151                       Tom Clancy\PYGZsq{}s H.A.W.X. 2  purchase   1.0
...         ...                                           ...       ...   ...
1831  112845094                  Grand Theft Auto San Andreas  purchase   1.0
1832  112845094                  Grand Theft Auto San Andreas      play   0.2
1833  112845094                          Grand Theft Auto III  purchase   1.0
1834  112845094                                        Arma 2  purchase   1.0
1836  112845094                    Grand Theft Auto Vice City  purchase   1.0

[1120 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Seems we have shaved off another 12 records, so our intuition was right, again lets see which the duplicates are:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{game}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{action}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game action  freq
118   118664413                  Grand Theft Auto San Andreas   play   0.2
458    50769696                  Grand Theft Auto San Andreas   play   3.1
521    71411882                          Grand Theft Auto III   play   0.2
607    33865373                   Sid Meier\PYGZsq{}s Civilization IV   play   2.0
898    71510748                  Grand Theft Auto San Andreas   play   0.2
908    28472068                    Grand Theft Auto Vice City   play   0.4
910    28472068                  Grand Theft Auto San Andreas   play   0.2
912    28472068                          Grand Theft Auto III   play   0.1
1506   59925638                       Tom Clancy\PYGZsq{}s H.A.W.X. 2   play   0.3
1553  148362155                  Grand Theft Auto San Andreas   play  12.5
1709  176261926  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword   play   0.4
1711  176261926                   Sid Meier\PYGZsq{}s Civilization IV   play   0.2
\end{sphinxVerbatim}

\sphinxAtStartPar
As expected the duplicates are all in the ‘play’ action, to complete our view we extract the data of a single user

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{user\PYGZus{}id}\PYG{o}{==}\PYG{l+m+mi}{118664413}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       user\PYGZus{}id                          game    action  freq
115  118664413  Grand Theft Auto San Andreas  purchase   1.0
116  118664413  Grand Theft Auto San Andreas      play   1.9
118  118664413  Grand Theft Auto San Andreas      play   0.2
\end{sphinxVerbatim}

\sphinxAtStartPar
It looks like we have a problem now, we know these are duplicates and should be removed, but which one?
Personally I would argue here that we keep the highest value, as it is impossible to ‘unplay’ hours on the game.
I will leave this as an exercise for you, but the solution is pretty tricky so i’ll give a hint:

\sphinxAtStartPar
The algorithm always keeps the first record in case of duplicates, so you could sort the rows making sure the higher value is always encountered first, good luck!


\chapter{Outliers and validity}
\label{\detokenize{c2_data_preparation/outliers:outliers-and-validity}}\label{\detokenize{c2_data_preparation/outliers::doc}}
\sphinxAtStartPar
When preparing data we have to be cautious with the accuracy of our set.
Outliers and invalid data points are difficult to detect but should be handled with caution.

\sphinxAtStartPar
we start out by importing our most important library.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}


\section{Silicon wafer thickness}
\label{\detokenize{c2_data_preparation/outliers:silicon-wafer-thickness}}
\sphinxAtStartPar
Our first dataset contains information about the production of silicon wafers, each wafers thickness is measure on 9 different spots.
More information on the dataset can be found \sphinxhref{https://openmv.net/info/silicon-wafer-thickness}{here}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{wafer\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/silicon\PYGZhy{}wafer\PYGZhy{}thickness.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{wafer\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
      G1     G2     G3     G4     G5     G6     G7     G8     G9
0  0.175  0.188 \PYGZhy{}0.159  0.095  0.374 \PYGZhy{}0.238 \PYGZhy{}0.800  0.158 \PYGZhy{}0.211
1  0.102  0.075  0.141  0.180  0.138 \PYGZhy{}0.057 \PYGZhy{}0.075  0.072  0.072
2  0.607  0.711  0.879  0.765  0.592  0.187  0.431  0.345  0.187
3  0.774  0.823  0.619  0.370  0.725  0.439 \PYGZhy{}0.025 \PYGZhy{}0.259  0.496
4  0.504  0.644  0.845  0.681  0.502  0.151  0.404  0.296  0.260
\end{sphinxVerbatim}

\sphinxAtStartPar
we would like to investigate the distribution of measurements here, as we are early in this course using visualisation techniques would be too soon.
This does not mean we can’t use simple mathematics, introducing the InterQuartile Range.
A reason for using IQR over standard deviation is that with IQR we do not assume a normal distribution.
The IQR calculates the range between the bottom ‘quart’ or 25\% and the top 25\%, giving us an indication of the spread of our results, we calculate this IQR for each of the 9 measurements independently.
For more info about IQR you can visit \sphinxhref{https://en.wikipedia.org/wiki/Interquartile\_range}{wikipedia}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{iqr} \PYG{o}{=} \PYG{n}{wafer\PYGZus{}df}\PYG{o}{.}\PYG{n}{quantile}\PYG{p}{(}\PYG{l+m+mf}{0.75}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{wafer\PYGZus{}df}\PYG{o}{.}\PYG{n}{quantile}\PYG{p}{(}\PYG{l+m+mf}{0.25}\PYG{p}{)}
\PYG{n}{iqr}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
G1    0.54425
G2    0.61000
G3    0.54075
G4    0.52475
G5    0.61175
G6    0.86750
G7    0.76175
G8    0.87225
G9    0.86300
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
you can see that the IQR spread for each measurement lays between 0.5 and 1 unit indicating that the 9 measurements of the wafer have a similar spread.
With these IQR’s we could calculate for each point relative to the spread of the measurement how far it is from the median.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{relative\PYGZus{}spread\PYGZus{}df} \PYG{o}{=} \PYG{p}{(}\PYG{n}{wafer\PYGZus{}df}\PYG{o}{\PYGZhy{}}\PYG{n}{wafer\PYGZus{}df}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{iqr}
\PYG{n}{relative\PYGZus{}spread\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
         G1        G2        G3        G4        G5        G6        G7  \PYGZbs{}
0 \PYGZhy{}0.011024 \PYGZhy{}0.077869 \PYGZhy{}0.819233 \PYGZhy{}0.367794  0.176543 \PYGZhy{}0.352738 \PYGZhy{}1.029865   
1 \PYGZhy{}0.145154 \PYGZhy{}0.263115 \PYGZhy{}0.264448 \PYGZhy{}0.205812 \PYGZhy{}0.209236 \PYGZhy{}0.144092 \PYGZhy{}0.078110   
2  0.782729  0.779508  1.100324  0.909004  0.532897  0.137176  0.586150   
3  1.089573  0.963115  0.619510  0.156265  0.750306  0.427666 \PYGZhy{}0.012471   
4  0.593477  0.669672  1.037448  0.748928  0.385779  0.095677  0.550706   

         G8        G9  
0 \PYGZhy{}0.130696 \PYGZhy{}0.254925  
1 \PYGZhy{}0.229292  0.073001  
2  0.083692  0.206257  
3 \PYGZhy{}0.608770  0.564311  
4  0.027515  0.290846  
\end{sphinxVerbatim}

\sphinxAtStartPar
You can now see that some points are close to the median, whilst others are much higher, both positive as negative.
By defining a threshold, we quantify what deviation has to be there to flag a reading as an outlier.
The high outliers are seperated, note that only a single measurement of the 9 can trigger and render the total measurement as an outlier.
Yet judging from the setup where we would want to find wafers with varying thickness that approach is desirable.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{relative\PYGZus{}spread\PYGZus{}df}\PYG{p}{[}\PYG{p}{(}\PYG{n}{relative\PYGZus{}spread\PYGZus{}df}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{columns}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            G1         G2         G3         G4         G5        G6  \PYGZbs{}
8     2.232430   2.009016   1.956542   1.589328   1.843890  1.544669   
38   12.891135  12.827049  12.832178  13.913292  11.429506  9.500865   
39    3.691318   3.981148   3.774387   4.081944   3.248059  3.729107   
61    2.010106   2.153279   1.987980   1.863745   1.858602  1.274928   
110   3.678457   2.841803   3.204808   3.180562   2.669391  0.518732   
112   2.361047   2.086066   2.363384   2.107670   1.925623  1.238040   
117   1.475425   1.043443   2.154415   2.582182   0.653862  1.823631   
120   1.791456   1.484426   2.583449   1.440686   2.085819  0.990202   
121   1.791456   1.484426   2.583449   1.440686   2.085819  0.990202   
152   2.610932   2.102459   2.387425   2.549786   2.169187  1.730259   
154  \PYGZhy{}0.529169  \PYGZhy{}0.538525  \PYGZhy{}0.404993  \PYGZhy{}0.331586  \PYGZhy{}0.552513  4.565994   

            G7        G8        G9  
8     1.233344  0.419604  1.582851  
38   10.305875  9.927200  9.055620  
39    3.304890  3.846374  3.149479  
61    1.237283  0.825451  0.955968  
110   0.700361  0.176555  0.727694  
112   1.766328  0.890800  1.377752  
117   1.581227  0.857552  1.188876  
120   1.782081  1.034107  1.822711  
121   1.782081  1.034107  1.822711  
152   2.241549  1.713958  1.592121  
154  \PYGZhy{}0.051854 \PYGZhy{}0.382918 \PYGZhy{}0.536501  
\end{sphinxVerbatim}

\sphinxAtStartPar
seems we have a few high outliers, you can clearly see the measurements are mostly all across the board high, but in some cases (e.g. id 154) only one measurement was an outlier.
We can do the same for the low outliers.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{relative\PYGZus{}spread\PYGZus{}df}\PYG{p}{[}\PYG{p}{(}\PYG{n}{relative\PYGZus{}spread\PYGZus{}df}\PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{columns}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           G1        G2        G3        G4        G5        G6        G7  \PYGZbs{}
54  \PYGZhy{}1.550758 \PYGZhy{}1.525410 \PYGZhy{}1.843736 \PYGZhy{}2.082897 \PYGZhy{}1.659174 \PYGZhy{}1.203458 \PYGZhy{}1.184772   
56  \PYGZhy{}1.732660 \PYGZhy{}1.510656 \PYGZhy{}2.121128 \PYGZhy{}2.122916 \PYGZhy{}1.781774 \PYGZhy{}1.521614 \PYGZhy{}1.909419   
59  \PYGZhy{}1.971520 \PYGZhy{}1.310656 \PYGZhy{}2.328248 \PYGZhy{}1.175798 \PYGZhy{}2.067838 \PYGZhy{}0.915274 \PYGZhy{}1.783394   
64  \PYGZhy{}1.234727 \PYGZhy{}1.361475 \PYGZhy{}0.736015 \PYGZhy{}1.055741 \PYGZhy{}2.224765 \PYGZhy{}0.839193 \PYGZhy{}0.679357   
65  \PYGZhy{}2.226918 \PYGZhy{}1.194262 \PYGZhy{}2.117429 \PYGZhy{}2.161029 \PYGZhy{}2.043318 \PYGZhy{}0.190202 \PYGZhy{}1.004923   
102 \PYGZhy{}2.484153 \PYGZhy{}2.330328 \PYGZhy{}1.568192 \PYGZhy{}2.808957 \PYGZhy{}1.945239 \PYGZhy{}1.340634 \PYGZhy{}0.846078   

           G8        G9  
54  \PYGZhy{}1.650903 \PYGZhy{}1.245655  
56  \PYGZhy{}1.782746 \PYGZhy{}1.159907  
59  \PYGZhy{}1.304672 \PYGZhy{}1.514484  
64  \PYGZhy{}0.865578 \PYGZhy{}0.663963  
65  \PYGZhy{}0.270565 \PYGZhy{}0.794902  
102 \PYGZhy{}1.691029 \PYGZhy{}0.887601  
\end{sphinxVerbatim}

\sphinxAtStartPar
For a simple mathematical equation these result look promising, yet it can always be more sophisticated.
Not going to deep into the subject we could perform some Machine Learning, using a unsupervised method.
Here we use the sklearn library which contains the Isolation forest algorithm.
More info about the algorithm \sphinxhref{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html}{here}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{ensemble} \PYG{k+kn}{import} \PYG{n}{IsolationForest}
\end{sphinxVerbatim}

\sphinxAtStartPar
We first create the classifier and train (fit) it with the generic wafer data.
Then for each record of the wafer data we make a prediction, if it thinks its an outlier, we keep them

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{clf} \PYG{o}{=} \PYG{n}{IsolationForest}\PYG{p}{(}\PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{wafer\PYGZus{}df}\PYG{p}{)}
\PYG{n}{wafer\PYGZus{}df}\PYG{p}{[}\PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{wafer\PYGZus{}df}\PYG{p}{)}\PYG{o}{==}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        G1     G2     G3     G4     G5     G6     G7     G8     G9
8    1.396  1.461  1.342  1.122  1.394  1.408  0.924  0.638  1.375
20  \PYGZhy{}0.558 \PYGZhy{}0.705 \PYGZhy{}0.526 \PYGZhy{}0.412 \PYGZhy{}0.753 \PYGZhy{}0.998 \PYGZhy{}0.270  0.598 \PYGZhy{}1.416
38   7.197  8.060  7.223  7.589  7.258  8.310  7.835  8.931  7.824
39   2.190  2.664  2.325  2.430  2.253  3.303  2.502  3.627  2.727
54  \PYGZhy{}0.663 \PYGZhy{}0.695 \PYGZhy{}0.713 \PYGZhy{}0.805 \PYGZhy{}0.749 \PYGZhy{}0.976 \PYGZhy{}0.918 \PYGZhy{}1.168 \PYGZhy{}1.066
56  \PYGZhy{}0.762 \PYGZhy{}0.686 \PYGZhy{}0.863 \PYGZhy{}0.826 \PYGZhy{}0.824 \PYGZhy{}1.252 \PYGZhy{}1.470 \PYGZhy{}1.283 \PYGZhy{}0.992
59  \PYGZhy{}0.892 \PYGZhy{}0.564 \PYGZhy{}0.975 \PYGZhy{}0.329 \PYGZhy{}0.999 \PYGZhy{}0.726 \PYGZhy{}1.374 \PYGZhy{}0.866 \PYGZhy{}1.298
61   1.275  1.549  1.359  1.266  1.403  1.174  0.927  0.992  0.834
65  \PYGZhy{}1.031 \PYGZhy{}0.493 \PYGZhy{}0.861 \PYGZhy{}0.846 \PYGZhy{}0.984 \PYGZhy{}0.097 \PYGZhy{}0.781  0.036 \PYGZhy{}0.677
102 \PYGZhy{}1.171 \PYGZhy{}1.186 \PYGZhy{}0.564 \PYGZhy{}1.186 \PYGZhy{}0.924 \PYGZhy{}1.095 \PYGZhy{}0.660 \PYGZhy{}1.203 \PYGZhy{}0.757
106 \PYGZhy{}0.659 \PYGZhy{}0.451 \PYGZhy{}0.692 \PYGZhy{}0.708 \PYGZhy{}0.595 \PYGZhy{}0.726 \PYGZhy{}1.031 \PYGZhy{}0.877 \PYGZhy{}1.080
110  2.183  1.969  2.017  1.957  1.899  0.518  0.518  0.426  0.637
112  1.466  1.508  1.562  1.394  1.444  1.142  1.330  1.049  1.198
117  0.984  0.872  1.449  1.643  0.666  1.650  1.189  1.020  1.035
120  1.156  1.141  1.681  1.044  1.542  0.927  1.342  1.174  1.582
121  1.156  1.141  1.681  1.044  1.542  0.927  1.342  1.174  1.582
152  1.602  1.518  1.575  1.626  1.593  1.569  1.692  1.767  1.383
\end{sphinxVerbatim}

\sphinxAtStartPar
Comparing the results with our IQR approach we see a lot of similarities, here the id 154 record did not show up as we already realised this was perhaps not a strong enough outlier.
You could enhance our IQR technique by checking the amount of measurements that are above the threshold and respond accordingly, I will leave you a little hint.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{relative\PYGZus{}spread\PYGZus{}df}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
G1    7
G2    7
G3    8
G4    6
G5    6
G6    3
G7    3
G8    2
G9    2
dtype: int64
\end{sphinxVerbatim}


\section{Distillation column}
\label{\detokenize{c2_data_preparation/outliers:distillation-column}}
\sphinxAtStartPar
As an exercise you can try the same technique to this dataset and see what you would find, good luck!
Be mindful that you do not incorporate the date as a variable in your outlier algorithm.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{distil\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/distillation\PYGZhy{}tower.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{distil\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date     Temp1    FlowC1     Temp2    TempC1     Temp3    TempC2  \PYGZbs{}
0    2000\PYGZhy{}08\PYGZhy{}21  139.9857  432.0636  377.8119  100.2204  492.1353  490.1459   
1    2000\PYGZhy{}08\PYGZhy{}23  131.0470  487.4029  371.3060  100.2297  482.2100  480.3128   
2    2000\PYGZhy{}08\PYGZhy{}26  118.2666  437.3516  378.4483  100.3084  488.7266  487.0040   
3    2000\PYGZhy{}08\PYGZhy{}29  118.1769  481.8314  378.0028   95.5766  493.1481  491.1137   
4    2000\PYGZhy{}08\PYGZhy{}30  120.7891  412.6471  377.8871   92.9052  490.2486  488.6641   
..          ...       ...       ...       ...       ...       ...       ...   
248  2003\PYGZhy{}01\PYGZhy{}26  130.8138  212.6385  341.5964  121.4354  468.3401  467.0299   
249  2003\PYGZhy{}01\PYGZhy{}28  128.9673  225.1412  349.8965  118.8604  479.7665  478.4652   
250  2003\PYGZhy{}01\PYGZhy{}31  130.5328  223.5965  345.9366  120.4027  474.5378  473.1145   
251  2003\PYGZhy{}02\PYGZhy{}03  128.5248  213.5613  343.4950  119.6989  469.3802  467.9954   
252  2003\PYGZhy{}02\PYGZhy{}04  131.0491  217.4117  346.1960  119.0825  474.6599  473.0381   

       TempC3     Temp4  PressureC1  ...    Temp10  FlowC3   FlowC4   Temp11  \PYGZbs{}
0    180.5578  187.4331    215.0627  ...  513.9653  8.6279  10.5988  30.8983   
1    172.6575  179.5089    205.0999  ...  504.5145  8.7662  10.7560  31.9099   
2    165.9400  172.9262    205.0304  ...  508.9997  8.5319  10.5737  29.9165   
3    167.2085  174.2338    205.2561  ...  514.1794  8.6260  10.6695  30.6229   
4    167.0326  173.9681    205.0883  ...  511.0948  8.5939  10.4922  29.4977   
..        ...       ...         ...  ...       ...     ...      ...      ...   
248  174.7639  180.7649    229.7393  ...  479.0290  5.5590   6.4470  16.4131   
249  176.2176  182.3646    230.5049  ...  491.2362  5.6342   6.4360  17.2385   
250  176.3310  182.2578    230.6638  ...  485.8786  5.4810   6.3575  16.9866   
251  174.6435  180.5093    230.5226  ...  480.2879  5.4727   6.4175  16.6778   
252  177.1088  183.1810    225.6420  ...  486.0253  5.4597   6.3291  16.8766   

       Temp12  InvTemp1  InvTemp2  InvTemp3  InvPressure1  VapourPressure  
0    489.9900    2.0409    2.6468    2.1681        4.3524         32.5026  
1    480.2888    2.0821    2.6932    2.2207        4.5497         34.8598  
2    486.6190    2.0550    2.6424    2.1796        4.5511         32.1666  
3    491.1304    2.0361    2.6455    2.1620        4.5464         30.4064  
4    487.6475    2.0507    2.6463    2.1704        4.5499         30.9238  
..        ...       ...       ...       ...           ...             ...  
248  466.3347    2.1444    2.9274    2.2127        4.0911         38.8507  
249  477.8816    2.0926    2.8580    2.1620        4.0783         34.2653  
250  472.3176    2.1172    2.8907    2.1855        4.0756         36.5717  
251  467.0001    2.1413    2.9113    2.2090        4.0780         38.1054  
252  472.2701    2.1174    2.8885    2.1844        4.1608         35.6298  

[253 rows x 28 columns]
\end{sphinxVerbatim}


\chapter{String operations}
\label{\detokenize{c2_data_preparation/string_operations:string-operations}}\label{\detokenize{c2_data_preparation/string_operations::doc}}

\chapter{Datetime operations}
\label{\detokenize{c2_data_preparation/datetime_operations:datetime-operations}}\label{\detokenize{c2_data_preparation/datetime_operations::doc}}
\sphinxAtStartPar
When our dataset contains time\sphinxhyphen{}related data, datetime operations are a great asset to our data science toolkit.
For this exercise we obtain a public covid dataset containing A LOT of information on infection cases, deaths, tests and vaccinations.

\sphinxAtStartPar
Let’s start by importing the data, as the dataset is about 60MB at the time of writing, this might take some time.
Perhaps you could think of a method to make this more efficient, do we always need all of the data?

\sphinxAtStartPar
More info about the data can be found \sphinxhref{https://github.com/owid/covid-19-data/tree/master/public/data}{here}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/owid/covid\PYGZhy{}19\PYGZhy{}data/master/public/data/owid\PYGZhy{}covid\PYGZhy{}data.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{on\PYGZus{}bad\PYGZus{}lines}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{skip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  iso\PYGZus{}code continent     location        date  total\PYGZus{}cases  new\PYGZus{}cases  \PYGZbs{}
0      AFG      Asia  Afghanistan  2020\PYGZhy{}02\PYGZhy{}24          5.0        5.0   
1      AFG      Asia  Afghanistan  2020\PYGZhy{}02\PYGZhy{}25          5.0        0.0   
2      AFG      Asia  Afghanistan  2020\PYGZhy{}02\PYGZhy{}26          5.0        0.0   
3      AFG      Asia  Afghanistan  2020\PYGZhy{}02\PYGZhy{}27          5.0        0.0   
4      AFG      Asia  Afghanistan  2020\PYGZhy{}02\PYGZhy{}28          5.0        0.0   

   new\PYGZus{}cases\PYGZus{}smoothed  total\PYGZus{}deaths  new\PYGZus{}deaths  new\PYGZus{}deaths\PYGZus{}smoothed  ...  \PYGZbs{}
0                 NaN           NaN         NaN                  NaN  ...   
1                 NaN           NaN         NaN                  NaN  ...   
2                 NaN           NaN         NaN                  NaN  ...   
3                 NaN           NaN         NaN                  NaN  ...   
4                 NaN           NaN         NaN                  NaN  ...   

   female\PYGZus{}smokers  male\PYGZus{}smokers  handwashing\PYGZus{}facilities  \PYGZbs{}
0             NaN           NaN                  37.746   
1             NaN           NaN                  37.746   
2             NaN           NaN                  37.746   
3             NaN           NaN                  37.746   
4             NaN           NaN                  37.746   

   hospital\PYGZus{}beds\PYGZus{}per\PYGZus{}thousand  life\PYGZus{}expectancy  human\PYGZus{}development\PYGZus{}index  \PYGZbs{}
0                         0.5            64.83                    0.511   
1                         0.5            64.83                    0.511   
2                         0.5            64.83                    0.511   
3                         0.5            64.83                    0.511   
4                         0.5            64.83                    0.511   

   excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}absolute  excess\PYGZus{}mortality\PYGZus{}cumulative  \PYGZbs{}
0                                   NaN                          NaN   
1                                   NaN                          NaN   
2                                   NaN                          NaN   
3                                   NaN                          NaN   
4                                   NaN                          NaN   

   excess\PYGZus{}mortality  excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}per\PYGZus{}million  
0               NaN                                      NaN  
1               NaN                                      NaN  
2               NaN                                      NaN  
3               NaN                                      NaN  
4               NaN                                      NaN  

[5 rows x 65 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
As mentioned a lot of information is present here, about 65 columns. yet for this exercise my main objective is the ‘date’ column.
If we would print out the data types using the info method, we can see that the date is recognized as an ‘object’ stating that it is an ordinary string, not a datetime.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 121744 entries, 0 to 121743
Data columns (total 65 columns):
 \PYGZsh{}   Column                                   Non\PYGZhy{}Null Count   Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}                                   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   iso\PYGZus{}code                                 121744 non\PYGZhy{}null  object 
 1   continent                                116202 non\PYGZhy{}null  object 
 2   location                                 121744 non\PYGZhy{}null  object 
 3   date                                     121744 non\PYGZhy{}null  object 
 4   total\PYGZus{}cases                              115518 non\PYGZhy{}null  float64
 5   new\PYGZus{}cases                                115515 non\PYGZhy{}null  float64
 6   new\PYGZus{}cases\PYGZus{}smoothed                       114500 non\PYGZhy{}null  float64
 7   total\PYGZus{}deaths                             104708 non\PYGZhy{}null  float64
 8   new\PYGZus{}deaths                               104863 non\PYGZhy{}null  float64
 9   new\PYGZus{}deaths\PYGZus{}smoothed                      114500 non\PYGZhy{}null  float64
 10  total\PYGZus{}cases\PYGZus{}per\PYGZus{}million                  114910 non\PYGZhy{}null  float64
 11  new\PYGZus{}cases\PYGZus{}per\PYGZus{}million                    114907 non\PYGZhy{}null  float64
 12  new\PYGZus{}cases\PYGZus{}smoothed\PYGZus{}per\PYGZus{}million           113897 non\PYGZhy{}null  float64
 13  total\PYGZus{}deaths\PYGZus{}per\PYGZus{}million                 104113 non\PYGZhy{}null  float64
 14  new\PYGZus{}deaths\PYGZus{}per\PYGZus{}million                   104268 non\PYGZhy{}null  float64
 15  new\PYGZus{}deaths\PYGZus{}smoothed\PYGZus{}per\PYGZus{}million          113897 non\PYGZhy{}null  float64
 16  reproduction\PYGZus{}rate                        98318 non\PYGZhy{}null   float64
 17  icu\PYGZus{}patients                             14443 non\PYGZhy{}null   float64
 18  icu\PYGZus{}patients\PYGZus{}per\PYGZus{}million                 14443 non\PYGZhy{}null   float64
 19  hosp\PYGZus{}patients                            16504 non\PYGZhy{}null   float64
 20  hosp\PYGZus{}patients\PYGZus{}per\PYGZus{}million                16504 non\PYGZhy{}null   float64
 21  weekly\PYGZus{}icu\PYGZus{}admissions                    1268 non\PYGZhy{}null    float64
 22  weekly\PYGZus{}icu\PYGZus{}admissions\PYGZus{}per\PYGZus{}million        1268 non\PYGZhy{}null    float64
 23  weekly\PYGZus{}hosp\PYGZus{}admissions                   2088 non\PYGZhy{}null    float64
 24  weekly\PYGZus{}hosp\PYGZus{}admissions\PYGZus{}per\PYGZus{}million       2088 non\PYGZhy{}null    float64
 25  new\PYGZus{}tests                                52248 non\PYGZhy{}null   float64
 26  total\PYGZus{}tests                              52352 non\PYGZhy{}null   float64
 27  total\PYGZus{}tests\PYGZus{}per\PYGZus{}thousand                 52352 non\PYGZhy{}null   float64
 28  new\PYGZus{}tests\PYGZus{}per\PYGZus{}thousand                   52248 non\PYGZhy{}null   float64
 29  new\PYGZus{}tests\PYGZus{}smoothed                       62816 non\PYGZhy{}null   float64
 30  new\PYGZus{}tests\PYGZus{}smoothed\PYGZus{}per\PYGZus{}thousand          62816 non\PYGZhy{}null   float64
 31  positive\PYGZus{}rate                            58959 non\PYGZhy{}null   float64
 32  tests\PYGZus{}per\PYGZus{}case                           58319 non\PYGZhy{}null   float64
 33  tests\PYGZus{}units                              64746 non\PYGZhy{}null   object 
 34  total\PYGZus{}vaccinations                       28115 non\PYGZhy{}null   float64
 35  people\PYGZus{}vaccinated                        26746 non\PYGZhy{}null   float64
 36  people\PYGZus{}fully\PYGZus{}vaccinated                  23714 non\PYGZhy{}null   float64
 37  total\PYGZus{}boosters                           3057 non\PYGZhy{}null    float64
 38  new\PYGZus{}vaccinations                         23298 non\PYGZhy{}null   float64
 39  new\PYGZus{}vaccinations\PYGZus{}smoothed                50221 non\PYGZhy{}null   float64
 40  total\PYGZus{}vaccinations\PYGZus{}per\PYGZus{}hundred           28115 non\PYGZhy{}null   float64
 41  people\PYGZus{}vaccinated\PYGZus{}per\PYGZus{}hundred            26746 non\PYGZhy{}null   float64
 42  people\PYGZus{}fully\PYGZus{}vaccinated\PYGZus{}per\PYGZus{}hundred      23714 non\PYGZhy{}null   float64
 43  total\PYGZus{}boosters\PYGZus{}per\PYGZus{}hundred               3057 non\PYGZhy{}null    float64
 44  new\PYGZus{}vaccinations\PYGZus{}smoothed\PYGZus{}per\PYGZus{}million    50221 non\PYGZhy{}null   float64
 45  stringency\PYGZus{}index                         101767 non\PYGZhy{}null  float64
 46  population                               120880 non\PYGZhy{}null  float64
 47  population\PYGZus{}density                       112501 non\PYGZhy{}null  float64
 48  median\PYGZus{}age                               107423 non\PYGZhy{}null  float64
 49  aged\PYGZus{}65\PYGZus{}older                            106229 non\PYGZhy{}null  float64
 50  aged\PYGZus{}70\PYGZus{}older                            106834 non\PYGZhy{}null  float64
 51  gdp\PYGZus{}per\PYGZus{}capita                           108055 non\PYGZhy{}null  float64
 52  extreme\PYGZus{}poverty                          72482 non\PYGZhy{}null   float64
 53  cardiovasc\PYGZus{}death\PYGZus{}rate                    107695 non\PYGZhy{}null  float64
 54  diabetes\PYGZus{}prevalence                      111063 non\PYGZhy{}null  float64
 55  female\PYGZus{}smokers                           84078 non\PYGZhy{}null   float64
 56  male\PYGZus{}smokers                             82858 non\PYGZhy{}null   float64
 57  handwashing\PYGZus{}facilities                   54111 non\PYGZhy{}null   float64
 58  hospital\PYGZus{}beds\PYGZus{}per\PYGZus{}thousand               97911 non\PYGZhy{}null   float64
 59  life\PYGZus{}expectancy                          115458 non\PYGZhy{}null  float64
 60  human\PYGZus{}development\PYGZus{}index                  107790 non\PYGZhy{}null  float64
 61  excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}absolute     4317 non\PYGZhy{}null    float64
 62  excess\PYGZus{}mortality\PYGZus{}cumulative              4317 non\PYGZhy{}null    float64
 63  excess\PYGZus{}mortality                         4317 non\PYGZhy{}null    float64
 64  excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}per\PYGZus{}million  4317 non\PYGZhy{}null    float64
dtypes: float64(60), object(5)
memory usage: 60.4+ MB
\end{sphinxVerbatim}

\sphinxAtStartPar
We would like to change that, as we can only perform datetime operations if pandas recognises the datetime format used.
Good for us, pandas has a method to automatically infer the date format, we do that now.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{date} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{date}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 121744 entries, 0 to 121743
Data columns (total 65 columns):
 \PYGZsh{}   Column                                   Non\PYGZhy{}Null Count   Dtype         
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}                                   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}         
 0   iso\PYGZus{}code                                 121744 non\PYGZhy{}null  object        
 1   continent                                116202 non\PYGZhy{}null  object        
 2   location                                 121744 non\PYGZhy{}null  object        
 3   date                                     121744 non\PYGZhy{}null  datetime64[ns]
 4   total\PYGZus{}cases                              115518 non\PYGZhy{}null  float64       
 5   new\PYGZus{}cases                                115515 non\PYGZhy{}null  float64       
 6   new\PYGZus{}cases\PYGZus{}smoothed                       114500 non\PYGZhy{}null  float64       
 7   total\PYGZus{}deaths                             104708 non\PYGZhy{}null  float64       
 8   new\PYGZus{}deaths                               104863 non\PYGZhy{}null  float64       
 9   new\PYGZus{}deaths\PYGZus{}smoothed                      114500 non\PYGZhy{}null  float64       
 10  total\PYGZus{}cases\PYGZus{}per\PYGZus{}million                  114910 non\PYGZhy{}null  float64       
 11  new\PYGZus{}cases\PYGZus{}per\PYGZus{}million                    114907 non\PYGZhy{}null  float64       
 12  new\PYGZus{}cases\PYGZus{}smoothed\PYGZus{}per\PYGZus{}million           113897 non\PYGZhy{}null  float64       
 13  total\PYGZus{}deaths\PYGZus{}per\PYGZus{}million                 104113 non\PYGZhy{}null  float64       
 14  new\PYGZus{}deaths\PYGZus{}per\PYGZus{}million                   104268 non\PYGZhy{}null  float64       
 15  new\PYGZus{}deaths\PYGZus{}smoothed\PYGZus{}per\PYGZus{}million          113897 non\PYGZhy{}null  float64       
 16  reproduction\PYGZus{}rate                        98318 non\PYGZhy{}null   float64       
 17  icu\PYGZus{}patients                             14443 non\PYGZhy{}null   float64       
 18  icu\PYGZus{}patients\PYGZus{}per\PYGZus{}million                 14443 non\PYGZhy{}null   float64       
 19  hosp\PYGZus{}patients                            16504 non\PYGZhy{}null   float64       
 20  hosp\PYGZus{}patients\PYGZus{}per\PYGZus{}million                16504 non\PYGZhy{}null   float64       
 21  weekly\PYGZus{}icu\PYGZus{}admissions                    1268 non\PYGZhy{}null    float64       
 22  weekly\PYGZus{}icu\PYGZus{}admissions\PYGZus{}per\PYGZus{}million        1268 non\PYGZhy{}null    float64       
 23  weekly\PYGZus{}hosp\PYGZus{}admissions                   2088 non\PYGZhy{}null    float64       
 24  weekly\PYGZus{}hosp\PYGZus{}admissions\PYGZus{}per\PYGZus{}million       2088 non\PYGZhy{}null    float64       
 25  new\PYGZus{}tests                                52248 non\PYGZhy{}null   float64       
 26  total\PYGZus{}tests                              52352 non\PYGZhy{}null   float64       
 27  total\PYGZus{}tests\PYGZus{}per\PYGZus{}thousand                 52352 non\PYGZhy{}null   float64       
 28  new\PYGZus{}tests\PYGZus{}per\PYGZus{}thousand                   52248 non\PYGZhy{}null   float64       
 29  new\PYGZus{}tests\PYGZus{}smoothed                       62816 non\PYGZhy{}null   float64       
 30  new\PYGZus{}tests\PYGZus{}smoothed\PYGZus{}per\PYGZus{}thousand          62816 non\PYGZhy{}null   float64       
 31  positive\PYGZus{}rate                            58959 non\PYGZhy{}null   float64       
 32  tests\PYGZus{}per\PYGZus{}case                           58319 non\PYGZhy{}null   float64       
 33  tests\PYGZus{}units                              64746 non\PYGZhy{}null   object        
 34  total\PYGZus{}vaccinations                       28115 non\PYGZhy{}null   float64       
 35  people\PYGZus{}vaccinated                        26746 non\PYGZhy{}null   float64       
 36  people\PYGZus{}fully\PYGZus{}vaccinated                  23714 non\PYGZhy{}null   float64       
 37  total\PYGZus{}boosters                           3057 non\PYGZhy{}null    float64       
 38  new\PYGZus{}vaccinations                         23298 non\PYGZhy{}null   float64       
 39  new\PYGZus{}vaccinations\PYGZus{}smoothed                50221 non\PYGZhy{}null   float64       
 40  total\PYGZus{}vaccinations\PYGZus{}per\PYGZus{}hundred           28115 non\PYGZhy{}null   float64       
 41  people\PYGZus{}vaccinated\PYGZus{}per\PYGZus{}hundred            26746 non\PYGZhy{}null   float64       
 42  people\PYGZus{}fully\PYGZus{}vaccinated\PYGZus{}per\PYGZus{}hundred      23714 non\PYGZhy{}null   float64       
 43  total\PYGZus{}boosters\PYGZus{}per\PYGZus{}hundred               3057 non\PYGZhy{}null    float64       
 44  new\PYGZus{}vaccinations\PYGZus{}smoothed\PYGZus{}per\PYGZus{}million    50221 non\PYGZhy{}null   float64       
 45  stringency\PYGZus{}index                         101767 non\PYGZhy{}null  float64       
 46  population                               120880 non\PYGZhy{}null  float64       
 47  population\PYGZus{}density                       112501 non\PYGZhy{}null  float64       
 48  median\PYGZus{}age                               107423 non\PYGZhy{}null  float64       
 49  aged\PYGZus{}65\PYGZus{}older                            106229 non\PYGZhy{}null  float64       
 50  aged\PYGZus{}70\PYGZus{}older                            106834 non\PYGZhy{}null  float64       
 51  gdp\PYGZus{}per\PYGZus{}capita                           108055 non\PYGZhy{}null  float64       
 52  extreme\PYGZus{}poverty                          72482 non\PYGZhy{}null   float64       
 53  cardiovasc\PYGZus{}death\PYGZus{}rate                    107695 non\PYGZhy{}null  float64       
 54  diabetes\PYGZus{}prevalence                      111063 non\PYGZhy{}null  float64       
 55  female\PYGZus{}smokers                           84078 non\PYGZhy{}null   float64       
 56  male\PYGZus{}smokers                             82858 non\PYGZhy{}null   float64       
 57  handwashing\PYGZus{}facilities                   54111 non\PYGZhy{}null   float64       
 58  hospital\PYGZus{}beds\PYGZus{}per\PYGZus{}thousand               97911 non\PYGZhy{}null   float64       
 59  life\PYGZus{}expectancy                          115458 non\PYGZhy{}null  float64       
 60  human\PYGZus{}development\PYGZus{}index                  107790 non\PYGZhy{}null  float64       
 61  excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}absolute     4317 non\PYGZhy{}null    float64       
 62  excess\PYGZus{}mortality\PYGZus{}cumulative              4317 non\PYGZhy{}null    float64       
 63  excess\PYGZus{}mortality                         4317 non\PYGZhy{}null    float64       
 64  excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}per\PYGZus{}million  4317 non\PYGZhy{}null    float64       
dtypes: datetime64[ns](1), float64(60), object(4)
memory usage: 60.4+ MB
\end{sphinxVerbatim}

\sphinxAtStartPar
now we are ready to perform datetime operations, however we can see that dates are appearing multiple times, this because we have records for multiple countries.
I live in Belgium, so decided to isolate that subsection of the data.
If they had used a data lake and partitioned into countries, reading out the data would have been much more efficient, but efficiency is not something I would expect from government as a Belgian.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df} \PYG{o}{=} \PYG{n}{covid\PYGZus{}df}\PYG{p}{[}\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{location}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Belgium}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           iso\PYGZus{}code continent location  total\PYGZus{}cases  new\PYGZus{}cases  \PYGZbs{}
date                                                             
2020\PYGZhy{}02\PYGZhy{}04      BEL    Europe  Belgium          1.0        1.0   
2020\PYGZhy{}02\PYGZhy{}05      BEL    Europe  Belgium          1.0        0.0   
2020\PYGZhy{}02\PYGZhy{}06      BEL    Europe  Belgium          1.0        0.0   
2020\PYGZhy{}02\PYGZhy{}07      BEL    Europe  Belgium          1.0        0.0   
2020\PYGZhy{}02\PYGZhy{}08      BEL    Europe  Belgium          1.0        0.0   

            new\PYGZus{}cases\PYGZus{}smoothed  total\PYGZus{}deaths  new\PYGZus{}deaths  new\PYGZus{}deaths\PYGZus{}smoothed  \PYGZbs{}
date                                                                            
2020\PYGZhy{}02\PYGZhy{}04                 NaN           NaN         NaN                  NaN   
2020\PYGZhy{}02\PYGZhy{}05                 NaN           NaN         NaN                  NaN   
2020\PYGZhy{}02\PYGZhy{}06                 NaN           NaN         NaN                  NaN   
2020\PYGZhy{}02\PYGZhy{}07                 NaN           NaN         NaN                  NaN   
2020\PYGZhy{}02\PYGZhy{}08                 NaN           NaN         NaN                  NaN   

            total\PYGZus{}cases\PYGZus{}per\PYGZus{}million  ...  female\PYGZus{}smokers  male\PYGZus{}smokers  \PYGZbs{}
date                                 ...                                 
2020\PYGZhy{}02\PYGZhy{}04                    0.086  ...            25.1          31.4   
2020\PYGZhy{}02\PYGZhy{}05                    0.086  ...            25.1          31.4   
2020\PYGZhy{}02\PYGZhy{}06                    0.086  ...            25.1          31.4   
2020\PYGZhy{}02\PYGZhy{}07                    0.086  ...            25.1          31.4   
2020\PYGZhy{}02\PYGZhy{}08                    0.086  ...            25.1          31.4   

            handwashing\PYGZus{}facilities  hospital\PYGZus{}beds\PYGZus{}per\PYGZus{}thousand  \PYGZbs{}
date                                                             
2020\PYGZhy{}02\PYGZhy{}04                     NaN                        5.64   
2020\PYGZhy{}02\PYGZhy{}05                     NaN                        5.64   
2020\PYGZhy{}02\PYGZhy{}06                     NaN                        5.64   
2020\PYGZhy{}02\PYGZhy{}07                     NaN                        5.64   
2020\PYGZhy{}02\PYGZhy{}08                     NaN                        5.64   

            life\PYGZus{}expectancy  human\PYGZus{}development\PYGZus{}index  \PYGZbs{}
date                                                   
2020\PYGZhy{}02\PYGZhy{}04            81.63                    0.931   
2020\PYGZhy{}02\PYGZhy{}05            81.63                    0.931   
2020\PYGZhy{}02\PYGZhy{}06            81.63                    0.931   
2020\PYGZhy{}02\PYGZhy{}07            81.63                    0.931   
2020\PYGZhy{}02\PYGZhy{}08            81.63                    0.931   

            excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}absolute  excess\PYGZus{}mortality\PYGZus{}cumulative  \PYGZbs{}
date                                                                            
2020\PYGZhy{}02\PYGZhy{}04                                   NaN                          NaN   
2020\PYGZhy{}02\PYGZhy{}05                                   NaN                          NaN   
2020\PYGZhy{}02\PYGZhy{}06                                   NaN                          NaN   
2020\PYGZhy{}02\PYGZhy{}07                                   NaN                          NaN   
2020\PYGZhy{}02\PYGZhy{}08                                   NaN                          NaN   

            excess\PYGZus{}mortality  excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}per\PYGZus{}million  
date                                                                   
2020\PYGZhy{}02\PYGZhy{}04               NaN                                      NaN  
2020\PYGZhy{}02\PYGZhy{}05               NaN                                      NaN  
2020\PYGZhy{}02\PYGZhy{}06               NaN                                      NaN  
2020\PYGZhy{}02\PYGZhy{}07               NaN                                      NaN  
2020\PYGZhy{}02\PYGZhy{}08               NaN                                      NaN  

[5 rows x 64 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Now that we have our dataset containing only Belgium I would like to emphasize another aspect, for features such as population density we would not expect a ‘head count’ to differ each day, and as we can see this number is steady over the whole line (results may vary for those who execute this in the future).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{population}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
11632334.0    611
Name: population, dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
we only have a single value (in my case 11.6M) that is repeated over the whole dataset, would this look optimal to you? How would you perhaps approach this to improve data management? If you would like to go hands\sphinxhyphen{}on I left you a blank cell to experiment.

\sphinxAtStartPar
Optimalizations aside, we can not do that which we came for! Datetime operations, the first thing that I have in mind is that due to weekends, the cases might fluctuate a lot per day, so it is not optimal to view it on a daily basis.

\sphinxAtStartPar
First we create a simple line plot with the raw daily cases, then we perform a weekly sum to create a more smooth version of the new cases.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{new\PYGZus{}cases}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{daily cases are fluctuating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}daily cases are fluctuating\PYGZsq{}\PYGZcb{}, xlabel=\PYGZsq{}date\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{datetime_operations_14_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weekly\PYGZus{}cases\PYGZus{}df} \PYG{o}{=} \PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{new\PYGZus{}cases}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{resample}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{weekly\PYGZus{}cases\PYGZus{}df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weekly cases are smoother}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}weekly cases are smoother\PYGZsq{}\PYGZcb{}, xlabel=\PYGZsq{}date\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{datetime_operations_15_1}.png}

\sphinxAtStartPar
That looks great! Those who inspected carefully saw that the x\sphinxhyphen{}axis was correclty identified as datetimes and that the y\sphinxhyphen{}axis for weekly sums have a much higher range.

\sphinxAtStartPar
In a next example we would like to have the relative changes from week to week, this can be done using the shift operation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weekly\PYGZus{}cases\PYGZus{}df}\PYG{o}{.}\PYG{n}{shift}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
date
2020\PYGZhy{}02\PYGZhy{}09        NaN
2020\PYGZhy{}02\PYGZhy{}16        1.0
2020\PYGZhy{}02\PYGZhy{}23        0.0
2020\PYGZhy{}03\PYGZhy{}01        0.0
2020\PYGZhy{}03\PYGZhy{}08        1.0
               ...   
2021\PYGZhy{}09\PYGZhy{}12    14099.0
2021\PYGZhy{}09\PYGZhy{}19    13508.0
2021\PYGZhy{}09\PYGZhy{}26    14298.0
2021\PYGZhy{}10\PYGZhy{}03    13909.0
2021\PYGZhy{}10\PYGZhy{}10    13474.0
Freq: W\PYGZhy{}SUN, Name: new\PYGZus{}cases, Length: 88, dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
This method shifted our data by 1 week forwards, this way we can subtract these results from our original data creating a relative increase (this\_week\_cases \sphinxhyphen{} last\_week\_cases).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{weekly\PYGZus{}cases\PYGZus{}df}\PYG{o}{\PYGZhy{}}\PYG{n}{weekly\PYGZus{}cases\PYGZus{}df}\PYG{o}{.}\PYG{n}{shift}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relative increase p week}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}relative increase p week\PYGZsq{}\PYGZcb{}, xlabel=\PYGZsq{}date\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{datetime_operations_19_1}.png}

\sphinxAtStartPar
Another powerfull asset of datetimes is that we can utilize the concepts of days, weeks, months and years.
In Belgium they speak about a phenomenon called ‘the weekend effect’ where a lot of reports are delayed and therefore Sundays have less cases whereas Mondays have more.

\sphinxAtStartPar
Do we see that in our data? let us seperate the Sundays and Mondays and take a mean!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean deaths on Monday}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{dayofweek}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{new\PYGZus{}deaths}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mean deaths on Monday
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
39.02439024390244
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean deaths on Sunday}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{dayofweek}\PYG{o}{==}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{new\PYGZus{}deaths}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mean deaths on Sunday
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
36.646341463414636
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems indeed that more people are reported to pass away no a Monday than on a Sunday, it would be optimal to verify this with statistics, but for now we keep it simple.

\sphinxAtStartPar
As a last example I would like to use slicing of our dataset to demonstrate we can also take a subset of our data and handle this, here we took the months of dec2020\sphinxhyphen{}jan2021 for belgium and calculated the total deaths.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2020\PYGZhy{}12\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2021\PYGZhy{}01\PYGZhy{}31}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{new\PYGZus{}deaths}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
4447.0
\end{sphinxVerbatim}

\sphinxAtStartPar
Now let’s compare this to our neighbours, the Netherlands and France, we do exactly the same operations by selecting exaclty the same time window.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}netherlands\PYGZus{}df} \PYG{o}{=} \PYG{n}{covid\PYGZus{}df}\PYG{p}{[}\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{location}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Netherlands}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}netherlands\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2020\PYGZhy{}12\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2021\PYGZhy{}01\PYGZhy{}31}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{new\PYGZus{}deaths}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
4655.0
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}france\PYGZus{}df} \PYG{o}{=} \PYG{n}{covid\PYGZus{}df}\PYG{p}{[}\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{location}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{France}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}france\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2020\PYGZhy{}12\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2021\PYGZhy{}01\PYGZhy{}31}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{new\PYGZus{}deaths}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
23382.0
\end{sphinxVerbatim}

\sphinxAtStartPar
You can see that Belgium has the lowest of total deaths in that time interval, so you could assume we performed the best!
However this approach is a bit simplified as there are not as many Belgians as French and Dutch. Could you perhaps think if an improvement to create a better understanding?


\chapter{Categorical encoding}
\label{\detokenize{c2_data_preparation/categorical_encoding:categorical-encoding}}\label{\detokenize{c2_data_preparation/categorical_encoding::doc}}
\sphinxAtStartPar
Often we deal with categorical data and this kind of data is something computer algorithms are not able to understand.
On the other hand long categorical features might take up unnecessary memory in our dataset, so converting to a categorical feature is optimal.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}


\section{Raw Material Charaterization}
\label{\detokenize{c2_data_preparation/categorical_encoding:raw-material-charaterization}}
\sphinxAtStartPar
In this dataset, we have a few numerical features describing characteristics of our material, next to that we also have an Outcome feature describing the state of our material in a category.

\sphinxAtStartPar
Let’s have a look at the data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/raw\PYGZhy{}material\PYGZhy{}characterization.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  Lot number   Outcome  Size5  Size10  Size15    TGA   DSC   TMA
0       B370  Adequate   13.8     9.2    41.2  787.3  18.0  65.0
1       B880  Adequate   11.2     5.8    27.6  772.2  17.7  68.8
2       B452  Adequate    9.9     5.8    28.3  602.3  18.3  50.7
3       B287  Adequate   10.4     4.0    24.7  677.9  17.7  56.5
4       B576  Adequate   12.3     9.3    22.0  593.5  19.5  52.0
\end{sphinxVerbatim}

\sphinxAtStartPar
So we can see that the outcome is indeed a text field with a human interpretable value.
The different values are:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([\PYGZsq{}Adequate\PYGZsq{}, \PYGZsq{}Poor\PYGZsq{}], dtype=object)
\end{sphinxVerbatim}

\sphinxAtStartPar
Image that we would like to get all records where the Outcome is less than adequate, using strings this is not possible as the computer does not understand relations of Adequate and Poor when they are denoted as text.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{p}{[}\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{o}{\PYGZlt{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Adequate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Empty DataFrame
Columns: [Lot number, Outcome, Size5, Size10, Size15, TGA, DSC, TMA]
Index: []
\end{sphinxVerbatim}

\sphinxAtStartPar
To overcome this we can change the type of the feature from ‘object’ (string) to ‘category’ let us look at the data types of our data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 24 entries, 0 to 23
Data columns (total 8 columns):
 \PYGZsh{}   Column      Non\PYGZhy{}Null Count  Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   Lot number  24 non\PYGZhy{}null     object 
 1   Outcome     24 non\PYGZhy{}null     object 
 2   Size5       24 non\PYGZhy{}null     float64
 3   Size10      24 non\PYGZhy{}null     float64
 4   Size15      24 non\PYGZhy{}null     float64
 5   TGA         24 non\PYGZhy{}null     float64
 6   DSC         24 non\PYGZhy{}null     float64
 7   TMA         24 non\PYGZhy{}null     float64
dtypes: float64(6), object(2)
memory usage: 1.6+ KB
\end{sphinxVerbatim}

\sphinxAtStartPar
Now we can change that of Outcome to category using the astype method

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome} \PYG{o}{=} \PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 24 entries, 0 to 23
Data columns (total 8 columns):
 \PYGZsh{}   Column      Non\PYGZhy{}Null Count  Dtype   
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   
 0   Lot number  24 non\PYGZhy{}null     object  
 1   Outcome     24 non\PYGZhy{}null     category
 2   Size5       24 non\PYGZhy{}null     float64 
 3   Size10      24 non\PYGZhy{}null     float64 
 4   Size15      24 non\PYGZhy{}null     float64 
 5   TGA         24 non\PYGZhy{}null     float64 
 6   DSC         24 non\PYGZhy{}null     float64 
 7   TMA         24 non\PYGZhy{}null     float64 
dtypes: category(1), float64(6), object(1)
memory usage: 1.6+ KB
\end{sphinxVerbatim}

\sphinxAtStartPar
Our feature might be of categorical nature now, however we still have to define it is an ordinal category and has an order.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome} \PYG{o}{=} \PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{o}{.}\PYG{n}{cat}\PYG{o}{.}\PYG{n}{as\PYGZus{}ordered}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cat}\PYG{o}{.}\PYG{n}{reorder\PYGZus{}categories}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Poor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Adequate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
If we retry to effort to only take the records where the Outcome is less than Adequate, we now get an outcome!
Since we only have 2 categories this is a bit unfortunate, but you should get the idea behind it.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{p}{[}\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{o}{\PYGZlt{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Adequate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Lot number Outcome  Size5  Size10  Size15    TGA   DSC   TMA
5        B914    Poor   13.7     7.8    27.0  597.9  18.1  49.8
6        B404    Poor   15.5    10.7    34.3  668.5  19.6  55.7
7        B694    Poor   15.4    10.7    35.9  602.8  19.2  53.6
8        B875    Poor   14.9    11.3    41.0  614.6  18.5  50.0
10       B517    Poor   16.1    11.6    39.2  682.8  17.5  56.4
13       B430    Poor   12.9     9.7    36.3  642.4  19.1  55.0
21       B745    Poor   10.2     5.8    24.7  575.9  18.5  46.2
\end{sphinxVerbatim}

\sphinxAtStartPar
Let’s take this a step further, since computer algorithms still have no idea what the numerical relation is between Adequate and Poor, we could use a Label Encoder for that.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{LabelEncoder}
\end{sphinxVerbatim}

\sphinxAtStartPar
the label encoder is inputted with the Outcome feature and recognises 2 types, it chooses a numerical value for each while fitting.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{le} \PYG{o}{=} \PYG{n}{LabelEncoder}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{le}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
LabelEncoder()
\end{sphinxVerbatim}

\sphinxAtStartPar
After fitting we can use this encoder to transform our dataset!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{outcome\PYGZus{}label}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{le}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{p}{)}
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  Lot number   Outcome  Size5  Size10  Size15    TGA   DSC   TMA  \PYGZbs{}
0       B370  Adequate   13.8     9.2    41.2  787.3  18.0  65.0   
1       B880  Adequate   11.2     5.8    27.6  772.2  17.7  68.8   
2       B452  Adequate    9.9     5.8    28.3  602.3  18.3  50.7   
3       B287  Adequate   10.4     4.0    24.7  677.9  17.7  56.5   
4       B576  Adequate   12.3     9.3    22.0  593.5  19.5  52.0   

   outcome\PYGZus{}label  
0              0  
1              0  
2              0  
3              0  
4              0  
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems something unfortunate has happened, the encoder gave the Adequate an outcome label of 0, which is lower than the label of Poor (1), this might be bad if we would like to give a score as our outcome.

\sphinxAtStartPar
There is in pandas another method of mapping a label to a category albeit less automated, as you would have to know the categories in your feature.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{outcome\PYGZus{}label} \PYG{o}{=} \PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Poor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Adequate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  Lot number   Outcome  Size5  Size10  Size15    TGA   DSC   TMA outcome\PYGZus{}label
0       B370  Adequate   13.8     9.2    41.2  787.3  18.0  65.0             1
1       B880  Adequate   11.2     5.8    27.6  772.2  17.7  68.8             1
2       B452  Adequate    9.9     5.8    28.3  602.3  18.3  50.7             1
3       B287  Adequate   10.4     4.0    24.7  677.9  17.7  56.5             1
4       B576  Adequate   12.3     9.3    22.0  593.5  19.5  52.0             1
\end{sphinxVerbatim}

\sphinxAtStartPar
Yes! This did the trick, now we can use that outcome label to predict an outcome for future samples.


\chapter{Restaurant tips}
\label{\detokenize{c2_data_preparation/categorical_encoding:restaurant-tips}}
\sphinxAtStartPar
Now we are going to look at a dataset of tips, here a restaurant tracked the table bills and tips for a few days in the week whilst also noting the gender, smoking habit and time of day.
This led to a small yet very interesting dataset, let’s have a look!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tips\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/mwaskom/seaborn\PYGZhy{}data/master/tips.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{tips\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     total\PYGZus{}bill   tip     sex smoker   day    time  size
0         16.99  1.01  Female     No   Sun  Dinner     2
1         10.34  1.66    Male     No   Sun  Dinner     3
2         21.01  3.50    Male     No   Sun  Dinner     3
3         23.68  3.31    Male     No   Sun  Dinner     2
4         24.59  3.61  Female     No   Sun  Dinner     4
..          ...   ...     ...    ...   ...     ...   ...
239       29.03  5.92    Male     No   Sat  Dinner     3
240       27.18  2.00  Female    Yes   Sat  Dinner     2
241       22.67  2.00    Male    Yes   Sat  Dinner     2
242       17.82  1.75    Male     No   Sat  Dinner     2
243       18.78  3.00  Female     No  Thur  Dinner     2

[244 rows x 7 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
We can see here that we have a lot of categorical variables: gender, smoker, the day and the time.
In later sections we will see how we can aggregate on these categorical variables.
Now however we would like to process them for a machine learning exercise, where we need numbers not text.
For the features smoker and day, you could argue there is a clear numbering between them, smoking is 1 if the person was smoking and e.g. Sun relates to 7 as it is the seventh day of the week.

\sphinxAtStartPar
But for the gender this is different, we can’t really say that women are 1 and Men are 0 or vice versa (although in this binary case it might work).
The same theory applies for time, if we would say that breakfast, lunch and dinner equal to 0, 1 and 2 this would give our algorithm a bad impression as it would think dinner is twice lunch…

\sphinxAtStartPar
We use One Hot Encoding for this, the idea is that for each value of the feature we create a new column.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{OneHotEncoder}
\end{sphinxVerbatim}

\sphinxAtStartPar
First we create our encoder, then we give it the day column to learn and see which values of categories there are.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ohe} \PYG{o}{=} \PYG{n}{OneHotEncoder}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{ohe}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{tips\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
OneHotEncoder()
\end{sphinxVerbatim}

\sphinxAtStartPar
Now we can perform an encoding, here we insert the day column and it returns a matrix with 4 columns corresponding to the 4 days in our feature.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ohe}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{tips\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{todense}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
matrix([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 0., 1.]])
\end{sphinxVerbatim}

\sphinxAtStartPar
As this is a rather mathematical approach for this simple problem I prefer to use the pandas approach for this, which is the get\_dummies method.
The outcome is much more pleasing yet completely the same.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{get\PYGZus{}dummies}\PYG{p}{(}\PYG{n}{tips\PYGZus{}df}\PYG{o}{.}\PYG{n}{day}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     Fri  Sat  Sun  Thur
0      0    0    1     0
1      0    0    1     0
2      0    0    1     0
3      0    0    1     0
4      0    0    1     0
..   ...  ...  ...   ...
239    0    1    0     0
240    0    1    0     0
241    0    1    0     0
242    0    1    0     0
243    0    0    0     1

[244 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
As an exercise you could create a script that given a specific feature (e.g. day):
\begin{itemize}
\item {} 
\sphinxAtStartPar
extracts that feature

\item {} 
\sphinxAtStartPar
creates dummies

\item {} 
\sphinxAtStartPar
concattenates it to the dataframe

\end{itemize}

\sphinxAtStartPar
Good luck!


\chapter{Scaling and Normalization}
\label{\detokenize{c2_data_preparation/scaling_normalization:scaling-and-normalization}}\label{\detokenize{c2_data_preparation/scaling_normalization::doc}}
\sphinxAtStartPar
In this notebook we are going to look into 2 rather mathematical concepts, Scaling and Normalization.
The idea is that we can scale the values and shape the distribution of feature in our dataset.

\sphinxAtStartPar
As an example we take a dataset containing samples from a low density polyethylene production process, containing several numerical features such as temperatures, Forces, Pressure,…

\sphinxAtStartPar
The idea is that by using Scaling and normalization, the ‘range of motion’ for these sensors is equal and we can compare the fluxtuations not only inbetween records, but also inbetween sensors.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ldpe\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://openmv.net/file/LDPE.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Unnamed: 0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ldpe\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
      Tin   Tmax1   Tout1   Tmax2   Tout2   Tcin1   Tcin2     z1     z2  \PYGZbs{}
0  208.17  296.35  233.81  283.41  239.05  117.14  117.20  0.029  0.581   
1  207.26  298.26  230.88  287.55  242.55  116.39  117.23  0.028  0.574   
2  205.30  296.57  235.38  284.35  245.19  117.33  118.42  0.031  0.578   
3  209.29  294.11  225.61  283.31  242.04  116.15  117.94  0.030  0.581   
4  206.76  295.13  230.26  283.74  244.92  116.75  118.49  0.030  0.579   

      Fi1     Fi2     Fs1     Fs2  Press    Conv     Mn      Mw    LCB    SCB  
0  0.4507  0.4518  666.42  248.95   3021  0.1322  27379  160326  0.781  26.11  
1  0.4765  0.5091  658.61  246.36   3033  0.1365  27043  165044  0.819  26.29  
2  0.4744  0.4505  666.51  244.65   3004  0.1335  27344  165621  0.801  26.13  
3  0.4429  0.4516  667.31  242.28   2980  0.1300  27502  160497  0.778  25.92  
4  0.4394  0.4414  670.83  244.31   2997  0.1316  27518  165713  0.786  26.02  
\end{sphinxVerbatim}

\sphinxAtStartPar
We can see that our features clearly have different ranges, but lets try to visualise these ranges using a density plot

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ldpe\PYGZus{}df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:ylabel=\PYGZsq{}Density\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{scaling_normalization_4_1}.png}

\sphinxAtStartPar
Ouch, this is clearly not working! Because the ‘Mw’ feature is in the range of 150k\sphinxhyphen{}175k our plot is so dilluted the rest are pinned to 0.
We can use the sklearn library to perform a min max scaling, this scaling will shift the distribution of each feature between 0 and 1, but that can also be adjusted.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{MinMaxScaler}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{scaler} \PYG{o}{=} \PYG{n}{MinMaxScaler}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{scaler}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{ldpe\PYGZus{}df}\PYG{p}{)}
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{scaler}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{ldpe\PYGZus{}df}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{n}{ldpe\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:ylabel=\PYGZsq{}Density\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{scaling_normalization_7_1}.png}

\sphinxAtStartPar
That makes a lot more sense, you can now see all of the distribution at once.
Also there seems to be one (yellow) feature that has some outliers perhaps something weird is going on there…

\sphinxAtStartPar
Taking it a step further we could also alter the distributions by using a standard scaler instead of a min max scaler, redistributing the values mathematically into a normal distribution.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{StandardScaler}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{scaler} \PYG{o}{=} \PYG{n}{StandardScaler}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{scaler}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{ldpe\PYGZus{}df}\PYG{p}{)}
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{scaler}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{ldpe\PYGZus{}df}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{n}{ldpe\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:ylabel=\PYGZsq{}Density\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{scaling_normalization_10_1}.png}

\sphinxAtStartPar
You can see it had some trouble fitting our special feature into the normal distribution but it did work out in the end.
With this we are ready to perform machine learning algorithms on this data, but first why not try and figure out where those outliers are I mentioned earlier?


\chapter{Binning and ranking}
\label{\detokenize{c2_data_preparation/binning_ranking:binning-and-ranking}}\label{\detokenize{c2_data_preparation/binning_ranking::doc}}
\sphinxAtStartPar
When dealing with numerical data the trouble can sometimes be that numbers can have a wide variety.

\sphinxAtStartPar
Here we apply 2 methods to deal with that, binning and ranking.
With binning we change the numerical feature into a categorical/ordinal feature.
Ranking is used when our numerical feature contains a non normal distribution that fails to be normalized.

\sphinxAtStartPar
For this example we use a food consumption dataset, where european countries are listed and the relative percentage of each country is given that consumes the type of food, e.g. a value of 67 means that 67\% of that country eats that type of food.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{set\PYGZus{}option}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{display.max\PYGZus{}columns}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://openmv.net/file/food\PYGZhy{}consumption.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{food\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        Country  Real coffee  Instant coffee  Tea  Sweetener  Biscuits  \PYGZbs{}
0       Germany           90              49   88       19.0      57.0   
1         Italy           82              10   60        2.0      55.0   
2        France           88              42   63        4.0      76.0   
3       Holland           96              62   98       32.0      62.0   
4       Belgium           94              38   48       11.0      74.0   
5    Luxembourg           97              61   86       28.0      79.0   
6       England           27              86   99       22.0      91.0   
7      Portugal           72              26   77        2.0      22.0   
8       Austria           55              31   61       15.0      29.0   
9   Switzerland           73              72   85       25.0      31.0   
10       Sweden           97              13   93       31.0       NaN   
11      Denmark           96              17   92       35.0      66.0   
12       Norway           92              17   83       13.0      62.0   
13      Finland           98              12   84       20.0      64.0   
14        Spain           70              40   40        NaN      62.0   
15      Ireland           30              52   99       11.0      80.0   

    Powder soup  Tin soup  Potatoes  Frozen fish  Frozen veggies  Apples  \PYGZbs{}
0            51        19        21           27              21      81   
1            41         3         2            4               2      67   
2            53        11        23           11               5      87   
3            67        43         7           14              14      83   
4            37        23         9           13              12      76   
5            73        12         7           26              23      85   
6            55        76        17           20              24      76   
7            34         1         5           20               3      22   
8            33         1         5           15              11      49   
9            69        10        17           19              15      79   
10           43        43        39           54              45      56   
11           32        17        11           51              42      81   
12           51         4        17           30              15      61   
13           27        10         8           18              12      50   
14           43         2        14           23               7      59   
15           75        18         2            5               3      57   

    Oranges  Tinned fruit  Jam  Garlic  Butter  Margarine  Olive oil  Yoghurt  \PYGZbs{}
0        75            44   71      22      91         85         74     30.0   
1        71             9   46      80      66         24         94      5.0   
2        84            40   45      88      94         47         36     57.0   
3        89            61   81      15      31         97         13     53.0   
4        76            42   57      29      84         80         83     20.0   
5        94            83   20      91      94         94         84     31.0   
6        68            89   91      11      95         94         57     11.0   
7        51             8   16      89      65         78         92      6.0   
8        42            14   41      51      51         72         28     13.0   
9        70            46   61      64      82         48         61     48.0   
10       78            53   75       9      68         32         48      2.0   
11       72            50   64      11      92         91         30     11.0   
12       72            34   51      11      63         94         28      2.0   
13       57            22   37      15      96         94         17      NaN   
14       77            30   38      86      44         51         91     16.0   
15       52            46   89       5      97         25         31      3.0   

    Crisp bread  
0            26  
1            18  
2             3  
3            15  
4             5  
5            24  
6            28  
7             9  
8            11  
9            30  
10           93  
11           34  
12           62  
13           64  
14           13  
15            9  
\end{sphinxVerbatim}

\sphinxAtStartPar
Here you could do some data validity, where we check if all values are between 0 and 100, or we check for missing values.
I will leave that up to you


\section{Binning}
\label{\detokenize{c2_data_preparation/binning_ranking:binning}}
\sphinxAtStartPar
the first thing we want to do is seperate the countries based on their coffee consumption, instead of creating arbitrary values we can perform a quantitative cut.
This means we create a number of equally sized groups using the qcut function, we give them the labels low, medium and high.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bin\PYGZus{}coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{qcut}\PYG{p}{(}\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{q}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{low}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{medium}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{high}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{food\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        Country  Real coffee  Instant coffee  Tea  Sweetener  Biscuits  \PYGZbs{}
0       Germany           90              49   88       19.0      57.0   
1         Italy           82              10   60        2.0      55.0   
2        France           88              42   63        4.0      76.0   
3       Holland           96              62   98       32.0      62.0   
4       Belgium           94              38   48       11.0      74.0   
5    Luxembourg           97              61   86       28.0      79.0   
6       England           27              86   99       22.0      91.0   
7      Portugal           72              26   77        2.0      22.0   
8       Austria           55              31   61       15.0      29.0   
9   Switzerland           73              72   85       25.0      31.0   
10       Sweden           97              13   93       31.0       NaN   
11      Denmark           96              17   92       35.0      66.0   
12       Norway           92              17   83       13.0      62.0   
13      Finland           98              12   84       20.0      64.0   
14        Spain           70              40   40        NaN      62.0   
15      Ireland           30              52   99       11.0      80.0   

    Powder soup  Tin soup  Potatoes  Frozen fish  Frozen veggies  Apples  \PYGZbs{}
0            51        19        21           27              21      81   
1            41         3         2            4               2      67   
2            53        11        23           11               5      87   
3            67        43         7           14              14      83   
4            37        23         9           13              12      76   
5            73        12         7           26              23      85   
6            55        76        17           20              24      76   
7            34         1         5           20               3      22   
8            33         1         5           15              11      49   
9            69        10        17           19              15      79   
10           43        43        39           54              45      56   
11           32        17        11           51              42      81   
12           51         4        17           30              15      61   
13           27        10         8           18              12      50   
14           43         2        14           23               7      59   
15           75        18         2            5               3      57   

    Oranges  Tinned fruit  Jam  Garlic  Butter  Margarine  Olive oil  Yoghurt  \PYGZbs{}
0        75            44   71      22      91         85         74     30.0   
1        71             9   46      80      66         24         94      5.0   
2        84            40   45      88      94         47         36     57.0   
3        89            61   81      15      31         97         13     53.0   
4        76            42   57      29      84         80         83     20.0   
5        94            83   20      91      94         94         84     31.0   
6        68            89   91      11      95         94         57     11.0   
7        51             8   16      89      65         78         92      6.0   
8        42            14   41      51      51         72         28     13.0   
9        70            46   61      64      82         48         61     48.0   
10       78            53   75       9      68         32         48      2.0   
11       72            50   64      11      92         91         30     11.0   
12       72            34   51      11      63         94         28      2.0   
13       57            22   37      15      96         94         17      NaN   
14       77            30   38      86      44         51         91     16.0   
15       52            46   89       5      97         25         31      3.0   

    Crisp bread bin\PYGZus{}coffee  
0            26     medium  
1            18     medium  
2             3     medium  
3            15       high  
4             5     medium  
5            24       high  
6            28        low  
7             9        low  
8            11        low  
9            30        low  
10           93       high  
11           34       high  
12           62     medium  
13           64       high  
14           13        low  
15            9        low  
\end{sphinxVerbatim}

\sphinxAtStartPar
a new column has appeared at the end of our dataframe, containing the labels of our binning, countries with low coffee consumption are put in the low category and vice versa.
Now we can seperate the countries with low coffee consumption from the rest

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{n}{food\PYGZus{}df}\PYG{o}{.}\PYG{n}{bin\PYGZus{}coffee} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{low}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        Country  Real coffee  Instant coffee  Tea  Sweetener  Biscuits  \PYGZbs{}
6       England           27              86   99       22.0      91.0   
7      Portugal           72              26   77        2.0      22.0   
8       Austria           55              31   61       15.0      29.0   
9   Switzerland           73              72   85       25.0      31.0   
14        Spain           70              40   40        NaN      62.0   
15      Ireland           30              52   99       11.0      80.0   

    Powder soup  Tin soup  Potatoes  Frozen fish  Frozen veggies  Apples  \PYGZbs{}
6            55        76        17           20              24      76   
7            34         1         5           20               3      22   
8            33         1         5           15              11      49   
9            69        10        17           19              15      79   
14           43         2        14           23               7      59   
15           75        18         2            5               3      57   

    Oranges  Tinned fruit  Jam  Garlic  Butter  Margarine  Olive oil  Yoghurt  \PYGZbs{}
6        68            89   91      11      95         94         57     11.0   
7        51             8   16      89      65         78         92      6.0   
8        42            14   41      51      51         72         28     13.0   
9        70            46   61      64      82         48         61     48.0   
14       77            30   38      86      44         51         91     16.0   
15       52            46   89       5      97         25         31      3.0   

    Crisp bread bin\PYGZus{}coffee  
6            28        low  
7             9        low  
8            11        low  
9            30        low  
14           13        low  
15            9        low  
\end{sphinxVerbatim}

\sphinxAtStartPar
You can already see the England/Ireland stereotype here, note that those are the only 2 with really low coffee consumption, the others are only in this low binning because we requested equally spaced bins in our qcut function. using the cut function would result in a different outcome.
Perhaps you could try that out?

\sphinxAtStartPar
I tried to think of some metric to quantify the status of coffee drinkers, since we also have the instant coffee consumption we could create a metric where we subtract the amount of instant coffe drinkers from the amount of real coffee drinkers.
This way we can measure that difference between them, I already went ahead and made equal quantity bins for them with labels low, medium and high ‘quality coffee’.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bin\PYGZus{}qual\PYGZus{}coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{qcut}\PYG{p}{(}\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Instant coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{q}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{low}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{medium}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{high}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{n}{food\PYGZus{}df}\PYG{o}{.}\PYG{n}{bin\PYGZus{}qual\PYGZus{}coffee}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{high}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    Country  Real coffee  Instant coffee  Tea  Sweetener  Biscuits  \PYGZbs{}
1     Italy           82              10   60        2.0      55.0   
10   Sweden           97              13   93       31.0       NaN   
11  Denmark           96              17   92       35.0      66.0   
12   Norway           92              17   83       13.0      62.0   
13  Finland           98              12   84       20.0      64.0   

    Powder soup  Tin soup  Potatoes  Frozen fish  Frozen veggies  Apples  \PYGZbs{}
1            41         3         2            4               2      67   
10           43        43        39           54              45      56   
11           32        17        11           51              42      81   
12           51         4        17           30              15      61   
13           27        10         8           18              12      50   

    Oranges  Tinned fruit  Jam  Garlic  Butter  Margarine  Olive oil  Yoghurt  \PYGZbs{}
1        71             9   46      80      66         24         94      5.0   
10       78            53   75       9      68         32         48      2.0   
11       72            50   64      11      92         91         30     11.0   
12       72            34   51      11      63         94         28      2.0   
13       57            22   37      15      96         94         17      NaN   

    Crisp bread bin\PYGZus{}coffee bin\PYGZus{}qual\PYGZus{}coffee  
1            18     medium            high  
10           93       high            high  
11           34       high            high  
12           62     medium            high  
13           64       high            high  
\end{sphinxVerbatim}

\sphinxAtStartPar
Aha! you can see here which countries prefer the real coffee over the instant version.
It seems the scandinavian countries together with obviously Italy are the true Caffeine connoisseur of Europe.
Another intersting thing we can do now is take the mean for each product for both group high and low and take the difference for high \sphinxhyphen{} low.
We can see the result below

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{n}{food\PYGZus{}df}\PYG{o}{.}\PYG{n}{bin\PYGZus{}qual\PYGZus{}coffee}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{high}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{n}{food\PYGZus{}df}\PYG{o}{.}\PYG{n}{bin\PYGZus{}qual\PYGZus{}coffee}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{low}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/tmp/ipykernel\PYGZus{}16521/3908782487.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with \PYGZsq{}numeric\PYGZus{}only=None\PYGZsq{}) is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.
  food\PYGZus{}df[food\PYGZus{}df.bin\PYGZus{}qual\PYGZus{}coffee==\PYGZsq{}high\PYGZsq{}].mean()\PYGZhy{}food\PYGZus{}df[food\PYGZus{}df.bin\PYGZus{}qual\PYGZus{}coffee==\PYGZsq{}low\PYGZsq{}].mean()
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Real coffee       34.500000
Instant coffee   \PYGZhy{}43.366667
Tea                2.066667
Sweetener         \PYGZhy{}0.800000
Biscuits           2.583333
Powder soup      \PYGZhy{}18.200000
Tin soup          \PYGZhy{}9.600000
Potatoes           5.066667
Frozen fish       15.400000
Frozen veggies    10.866667
Apples            \PYGZhy{}4.166667
Oranges            3.666667
Tinned fruit     \PYGZhy{}14.066667
Jam              \PYGZhy{}12.233333
Garlic           \PYGZhy{}13.466667
Butter            10.333333
Margarine          2.500000
Olive oil         \PYGZhy{}3.433333
Yoghurt          \PYGZhy{}19.000000
Crisp bread       36.533333
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems a preference for quality coffee also pairs with crisp bread, who knew?
Do you think scaling/normalization might be interesting here? why (not)?


\section{Ranking}
\label{\detokenize{c2_data_preparation/binning_ranking:ranking}}
\sphinxAtStartPar
In case normalization fails us and we are for some reason not able to get a normal distribution out of a feature, we can still resort to ranking.
Note that non linear machine learning techniques often use a ranking functionality under the hood, therefore this technique is often not required, yet for educational purposes we are going to use it here anyway.
Let’s see how the distribution for Real coffee consumption looks like.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        Country  Real coffee  Instant coffee  Tea  Sweetener  Biscuits  \PYGZbs{}
6       England           27              86   99       22.0      91.0   
15      Ireland           30              52   99       11.0      80.0   
8       Austria           55              31   61       15.0      29.0   
14        Spain           70              40   40        NaN      62.0   
7      Portugal           72              26   77        2.0      22.0   
9   Switzerland           73              72   85       25.0      31.0   
1         Italy           82              10   60        2.0      55.0   
2        France           88              42   63        4.0      76.0   
0       Germany           90              49   88       19.0      57.0   
12       Norway           92              17   83       13.0      62.0   
4       Belgium           94              38   48       11.0      74.0   
3       Holland           96              62   98       32.0      62.0   
11      Denmark           96              17   92       35.0      66.0   
5    Luxembourg           97              61   86       28.0      79.0   
10       Sweden           97              13   93       31.0       NaN   
13      Finland           98              12   84       20.0      64.0   

    Powder soup  Tin soup  Potatoes  Frozen fish  Frozen veggies  Apples  \PYGZbs{}
6            55        76        17           20              24      76   
15           75        18         2            5               3      57   
8            33         1         5           15              11      49   
14           43         2        14           23               7      59   
7            34         1         5           20               3      22   
9            69        10        17           19              15      79   
1            41         3         2            4               2      67   
2            53        11        23           11               5      87   
0            51        19        21           27              21      81   
12           51         4        17           30              15      61   
4            37        23         9           13              12      76   
3            67        43         7           14              14      83   
11           32        17        11           51              42      81   
5            73        12         7           26              23      85   
10           43        43        39           54              45      56   
13           27        10         8           18              12      50   

    Oranges  Tinned fruit  Jam  Garlic  Butter  Margarine  Olive oil  Yoghurt  \PYGZbs{}
6        68            89   91      11      95         94         57     11.0   
15       52            46   89       5      97         25         31      3.0   
8        42            14   41      51      51         72         28     13.0   
14       77            30   38      86      44         51         91     16.0   
7        51             8   16      89      65         78         92      6.0   
9        70            46   61      64      82         48         61     48.0   
1        71             9   46      80      66         24         94      5.0   
2        84            40   45      88      94         47         36     57.0   
0        75            44   71      22      91         85         74     30.0   
12       72            34   51      11      63         94         28      2.0   
4        76            42   57      29      84         80         83     20.0   
3        89            61   81      15      31         97         13     53.0   
11       72            50   64      11      92         91         30     11.0   
5        94            83   20      91      94         94         84     31.0   
10       78            53   75       9      68         32         48      2.0   
13       57            22   37      15      96         94         17      NaN   

    Crisp bread bin\PYGZus{}coffee bin\PYGZus{}qual\PYGZus{}coffee  
6            28        low             low  
15            9        low             low  
8            11        low             low  
14           13        low             low  
7             9        low          medium  
9            30        low             low  
1            18     medium            high  
2             3     medium          medium  
0            26     medium          medium  
12           62     medium            high  
4             5     medium          medium  
3            15       high             low  
11           34       high            high  
5            24       high          medium  
10           93       high            high  
13           64       high            high  
\end{sphinxVerbatim}

\sphinxAtStartPar
Ah yes, about half of the values are 90 or higher, not really optimal as the range is between 0 and 100!
We can also view this in a visual way using a density plot.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee (raw)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}Real coffee (raw)\PYGZsq{}\PYGZcb{}, ylabel=\PYGZsq{}Density\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{binning_ranking_21_1}.png}

\sphinxAtStartPar
For larger datasets this would be more useful as we cannot see our whole dataset, it is clear we have to do something about this, now imagine we can not use regular normalization techniques.
The rank method now comes in handy!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rank\PYGZus{}coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{rank}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{food\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        Country  Real coffee  Instant coffee  Tea  Sweetener  Biscuits  \PYGZbs{}
0       Germany           90              49   88       19.0      57.0   
1         Italy           82              10   60        2.0      55.0   
2        France           88              42   63        4.0      76.0   
3       Holland           96              62   98       32.0      62.0   
4       Belgium           94              38   48       11.0      74.0   
5    Luxembourg           97              61   86       28.0      79.0   
6       England           27              86   99       22.0      91.0   
7      Portugal           72              26   77        2.0      22.0   
8       Austria           55              31   61       15.0      29.0   
9   Switzerland           73              72   85       25.0      31.0   
10       Sweden           97              13   93       31.0       NaN   
11      Denmark           96              17   92       35.0      66.0   
12       Norway           92              17   83       13.0      62.0   
13      Finland           98              12   84       20.0      64.0   
14        Spain           70              40   40        NaN      62.0   
15      Ireland           30              52   99       11.0      80.0   

    Powder soup  Tin soup  Potatoes  Frozen fish  Frozen veggies  Apples  \PYGZbs{}
0            51        19        21           27              21      81   
1            41         3         2            4               2      67   
2            53        11        23           11               5      87   
3            67        43         7           14              14      83   
4            37        23         9           13              12      76   
5            73        12         7           26              23      85   
6            55        76        17           20              24      76   
7            34         1         5           20               3      22   
8            33         1         5           15              11      49   
9            69        10        17           19              15      79   
10           43        43        39           54              45      56   
11           32        17        11           51              42      81   
12           51         4        17           30              15      61   
13           27        10         8           18              12      50   
14           43         2        14           23               7      59   
15           75        18         2            5               3      57   

    Oranges  Tinned fruit  Jam  Garlic  Butter  Margarine  Olive oil  Yoghurt  \PYGZbs{}
0        75            44   71      22      91         85         74     30.0   
1        71             9   46      80      66         24         94      5.0   
2        84            40   45      88      94         47         36     57.0   
3        89            61   81      15      31         97         13     53.0   
4        76            42   57      29      84         80         83     20.0   
5        94            83   20      91      94         94         84     31.0   
6        68            89   91      11      95         94         57     11.0   
7        51             8   16      89      65         78         92      6.0   
8        42            14   41      51      51         72         28     13.0   
9        70            46   61      64      82         48         61     48.0   
10       78            53   75       9      68         32         48      2.0   
11       72            50   64      11      92         91         30     11.0   
12       72            34   51      11      63         94         28      2.0   
13       57            22   37      15      96         94         17      NaN   
14       77            30   38      86      44         51         91     16.0   
15       52            46   89       5      97         25         31      3.0   

    Crisp bread bin\PYGZus{}coffee bin\PYGZus{}qual\PYGZus{}coffee  rank\PYGZus{}coffee  
0            26     medium          medium          9.0  
1            18     medium            high          7.0  
2             3     medium          medium          8.0  
3            15       high             low         12.5  
4             5     medium          medium         11.0  
5            24       high          medium         14.5  
6            28        low             low          1.0  
7             9        low          medium          5.0  
8            11        low             low          3.0  
9            30        low             low          6.0  
10           93       high            high         14.5  
11           34       high            high         12.5  
12           62     medium            high         10.0  
13           64       high            high         16.0  
14           13        low             low          4.0  
15            9        low             low          2.0  
\end{sphinxVerbatim}

\sphinxAtStartPar
At the end of our data a new column was appended, containing the ranking of each country with the lowest being 1 and the highest equal to the amount of countries.
When we visualise this distribution we get a uniform distribution, not normal but still better than before!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rank\PYGZus{}coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee (ranked)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}Real coffee (ranked)\PYGZsq{}\PYGZcb{}, ylabel=\PYGZsq{}Density\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{binning_ranking_25_1}.png}


\chapter{Some practice}
\label{\detokenize{c2_data_preparation/some_practice:some-practice}}\label{\detokenize{c2_data_preparation/some_practice::doc}}
\sphinxAtStartPar
Now that you have learned techniques in data preparation, why don’t you put them to use in this wonderfully horrifying dataset. Good luck!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{json}

\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kaggle\PYGZus{}dir} \PYG{o}{=} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{expanduser}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZti{}/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{n}{kaggle\PYGZus{}dir}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{mkdir}\PYG{p}{(}\PYG{n}{kaggle\PYGZus{}dir}\PYG{p}{)}

\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{kaggle\PYGZus{}dir}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/kaggle.json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}
        \PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{username}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lorenzf}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{7a44a9e99b27e796177d793a3d85b8cf}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{\PYGZcb{}}
        \PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kaggle}
\PYG{n}{kaggle}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{dataset\PYGZus{}download\PYGZus{}files}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PromptCloudHQ/us\PYGZhy{}jobs\PYGZhy{}on\PYGZhy{}monstercom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{unzip}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ModuleNotFoundError}\PYG{g+gWhitespace}{                       }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}25600}\PYG{o}{/}\PYG{l+m+mf}{39646943.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{k+kn}{import} \PYG{n+nn}{kaggle}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{2} \PYG{n}{kaggle}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{dataset\PYGZus{}download\PYGZus{}files}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PromptCloudHQ/us\PYGZhy{}jobs\PYGZhy{}on\PYGZhy{}monstercom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{unzip}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n+ne}{ModuleNotFoundError}: No module named \PYGZsq{}kaggle\PYGZsq{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/monster\PYGZus{}com\PYGZhy{}job\PYGZus{}sample.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                    country country\PYGZus{}code date\PYGZus{}added has\PYGZus{}expired  \PYGZbs{}
0  United States of America           US        NaN          No   
1  United States of America           US        NaN          No   
2  United States of America           US        NaN          No   
3  United States of America           US        NaN          No   
4  United States of America           US        NaN          No   

          job\PYGZus{}board                                    job\PYGZus{}description  \PYGZbs{}
0  jobs.monster.com  TeamSoft is seeing an IT Support Specialist to...   
1  jobs.monster.com  The Wisconsin State Journal is seeking a flexi...   
2  jobs.monster.com  Report this job About the Job DePuy Synthes Co...   
3  jobs.monster.com  Why Join Altec? If you’re considering a career...   
4  jobs.monster.com  Position ID\PYGZsh{}  76162 \PYGZsh{} Positions  1 State  CT C...   

                                           job\PYGZus{}title             job\PYGZus{}type  \PYGZbs{}
0               IT Support Technician Job in Madison   Full Time Employee   
1            Business Reporter/Editor Job in Madison            Full Time   
2  Johnson \PYGZam{} Johnson Family of Companies Job Appl...  Full Time, Employee   
3                    Engineer \PYGZhy{} Quality Job in Dixon            Full Time   
4       Shift Supervisor \PYGZhy{} Part\PYGZhy{}Time Job in Camphill   Full Time Employee   

                                            location  \PYGZbs{}
0                                  Madison, WI 53702   
1                                  Madison, WI 53708   
2  DePuy Synthes Companies is a member of Johnson...   
3                                          Dixon, CA   
4                                       Camphill, PA   

                      organization  \PYGZbs{}
0                              NaN   
1          Printing and Publishing   
2  Personal and Household Services   
3                 Altec Industries   
4                           Retail   

                                            page\PYGZus{}url salary  \PYGZbs{}
0  http://jobview.monster.com/it\PYGZhy{}support\PYGZhy{}technici...    NaN   
1  http://jobview.monster.com/business\PYGZhy{}reporter\PYGZhy{}e...    NaN   
2  http://jobview.monster.com/senior\PYGZhy{}training\PYGZhy{}lea...    NaN   
3  http://jobview.monster.com/engineer\PYGZhy{}quality\PYGZhy{}jo...    NaN   
4  http://jobview.monster.com/shift\PYGZhy{}supervisor\PYGZhy{}pa...    NaN   

                       sector                           uniq\PYGZus{}id  
0     IT/Software Development  11d599f229a80023d2f40e7c52cd941e  
1                         NaN  e4cbb126dabf22159aff90223243ff2a  
2                         NaN  839106b353877fa3d896ffb9c1fe01c0  
3   Experienced (Non\PYGZhy{}Manager)  58435fcab804439efdcaa7ecca0fd783  
4  Project/Program Management  64d0272dc8496abfd9523a8df63c184c  
\end{sphinxVerbatim}

\sphinxAtStartPar
Need some inspiration? perhaps \sphinxhref{https://www.kaggle.com/ankkur13/perfect-dataset-to-get-the-hands-dirty}{this} might help!


\part{3. Data Preprocessing}


\chapter{Data Preprocessing}
\label{\detokenize{c3_data_preprocessing/introduction:data-preprocessing}}\label{\detokenize{c3_data_preprocessing/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\chapter{Indexing and slicing}
\label{\detokenize{c3_data_preprocessing/indexing_slicing:indexing-and-slicing}}\label{\detokenize{c3_data_preprocessing/indexing_slicing::doc}}
\sphinxAtStartPar
In

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/temperatures/australia/melbourne/1981.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date  Temp
0    1981\PYGZhy{}01\PYGZhy{}01  20.7
1    1981\PYGZhy{}01\PYGZhy{}02  17.9
2    1981\PYGZhy{}01\PYGZhy{}03  18.8
3    1981\PYGZhy{}01\PYGZhy{}04  14.6
4    1981\PYGZhy{}01\PYGZhy{}05  15.8
..          ...   ...
360  1981\PYGZhy{}12\PYGZhy{}27  15.5
361  1981\PYGZhy{}12\PYGZhy{}28  13.3
362  1981\PYGZhy{}12\PYGZhy{}29  15.6
363  1981\PYGZhy{}12\PYGZhy{}30  15.2
364  1981\PYGZhy{}12\PYGZhy{}31  17.4

[365 rows x 2 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{Date} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{Date}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df} \PYG{o}{=} \PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1981\PYGZhy{}06\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1981\PYGZhy{}06\PYGZhy{}30}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            Temp
Date            
1981\PYGZhy{}06\PYGZhy{}01  11.6
1981\PYGZhy{}06\PYGZhy{}02  10.6
1981\PYGZhy{}06\PYGZhy{}03   9.8
1981\PYGZhy{}06\PYGZhy{}04  11.2
1981\PYGZhy{}06\PYGZhy{}05   5.7
1981\PYGZhy{}06\PYGZhy{}06   7.1
1981\PYGZhy{}06\PYGZhy{}07   2.5
1981\PYGZhy{}06\PYGZhy{}08   3.5
1981\PYGZhy{}06\PYGZhy{}09   4.6
1981\PYGZhy{}06\PYGZhy{}10  11.0
1981\PYGZhy{}06\PYGZhy{}11   5.7
1981\PYGZhy{}06\PYGZhy{}12   7.7
1981\PYGZhy{}06\PYGZhy{}13  10.4
1981\PYGZhy{}06\PYGZhy{}14  11.4
1981\PYGZhy{}06\PYGZhy{}15   9.2
1981\PYGZhy{}06\PYGZhy{}16   6.1
1981\PYGZhy{}06\PYGZhy{}17   2.7
1981\PYGZhy{}06\PYGZhy{}18   4.3
1981\PYGZhy{}06\PYGZhy{}19   6.3
1981\PYGZhy{}06\PYGZhy{}20   3.8
1981\PYGZhy{}06\PYGZhy{}21   4.4
1981\PYGZhy{}06\PYGZhy{}22   7.1
1981\PYGZhy{}06\PYGZhy{}23   4.8
1981\PYGZhy{}06\PYGZhy{}24   5.8
1981\PYGZhy{}06\PYGZhy{}25   6.2
1981\PYGZhy{}06\PYGZhy{}26   7.3
1981\PYGZhy{}06\PYGZhy{}27   9.2
1981\PYGZhy{}06\PYGZhy{}28  10.2
1981\PYGZhy{}06\PYGZhy{}29   9.5
1981\PYGZhy{}06\PYGZhy{}30   9.5
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1989\PYGZhy{}06\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1989\PYGZhy{}06\PYGZhy{}30}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Temp   NaN
dtype: float64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{resample}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MS}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                 Temp
Date                 
1981\PYGZhy{}01\PYGZhy{}01  17.712903
1981\PYGZhy{}02\PYGZhy{}01  17.678571
1981\PYGZhy{}03\PYGZhy{}01  13.500000
1981\PYGZhy{}04\PYGZhy{}01  12.356667
1981\PYGZhy{}05\PYGZhy{}01   9.490323
1981\PYGZhy{}06\PYGZhy{}01   7.306667
1981\PYGZhy{}07\PYGZhy{}01   7.577419
1981\PYGZhy{}08\PYGZhy{}01   7.238710
1981\PYGZhy{}09\PYGZhy{}01  10.143333
1981\PYGZhy{}10\PYGZhy{}01  10.087097
1981\PYGZhy{}11\PYGZhy{}01  11.890000
1981\PYGZhy{}12\PYGZhy{}01  13.680645
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}df} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tips}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{tip\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   total\PYGZus{}bill   tip     sex smoker  day    time  size
0       16.99  1.01  Female     No  Sun  Dinner     2
1       10.34  1.66    Male     No  Sun  Dinner     3
2       21.01  3.50    Male     No  Sun  Dinner     3
3       23.68  3.31    Male     No  Sun  Dinner     2
4       24.59  3.61  Female     No  Sun  Dinner     4
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}index\PYGZus{}df} \PYG{o}{=} \PYG{n}{tip\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}index\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     total\PYGZus{}bill   tip     sex smoker    time  size
day                                               
Sun       16.99  1.01  Female     No  Dinner     2
Sun       10.34  1.66    Male     No  Dinner     3
Sun       21.01  3.50    Male     No  Dinner     3
Sun       23.68  3.31    Male     No  Dinner     2
Sun       24.59  3.61  Female     No  Dinner     4
..          ...   ...     ...    ...     ...   ...
Sun       20.90  3.50  Female    Yes  Dinner     3
Sun       30.46  2.00    Male    Yes  Dinner     5
Sun       18.15  3.50  Female    Yes  Dinner     3
Sun       23.10  4.00    Male    Yes  Dinner     3
Sun       15.69  1.50    Male    Yes  Dinner     2

[76 rows x 6 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}index\PYGZus{}df} \PYG{o}{=} \PYG{n}{tip\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}index\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Thur}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Lunch}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tip}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/tmp/ipykernel\PYGZus{}25625/2537502835.py:1: PerformanceWarning: indexing past lexsort depth may impact performance.
  tip\PYGZus{}index\PYGZus{}df.loc[(\PYGZsq{}Thur\PYGZsq{},\PYGZsq{}Lunch\PYGZsq{})].tip.mean()
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2.767704918032786
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{pivot\PYGZus{}table}\PYG{p}{(}\PYG{n}{tip\PYGZus{}df}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{total\PYGZus{}bill}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{aggfunc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{median}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
time  Lunch  Dinner
day                
Thur  16.00  18.780
Fri   13.42  18.665
Sat     NaN  18.240
Sun     NaN  19.630
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoker}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Male}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dinner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/tmp/ipykernel\PYGZus{}25625/3467525553.py:1: PerformanceWarning: indexing past lexsort depth may impact performance.
  tip\PYGZus{}df.set\PYGZus{}index([\PYGZsq{}sex\PYGZsq{}, \PYGZsq{}time\PYGZsq{},\PYGZsq{}smoker\PYGZsq{}]).loc[(\PYGZsq{}Male\PYGZsq{}, \PYGZsq{}Dinner\PYGZsq{},\PYGZsq{}Yes\PYGZsq{})][\PYGZsq{}tip\PYGZsq{}].mean()
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3.123191489361702
\end{sphinxVerbatim}


\chapter{Merge}
\label{\detokenize{c3_data_preprocessing/merge:merge}}\label{\detokenize{c3_data_preprocessing/merge::doc}}
\sphinxAtStartPar
When data becomes multi\sphinxhyphen{}dimensional \sphinxhyphen{} covering multiple aspects of information \sphinxhyphen{} it usually happens that a lot of information is redundant.
Take for example the next dataset, we have collected ratings of restaurants from users, when a single user rates 2 restaurants the information of the user relates to both rows, yet it would be wasteful to keep this info twice.
The same can happen when we have a restaurant with 2 ratings, the location of the restaurant is kept twice in our data, which is not scalable.

\sphinxAtStartPar
We solve this problem using relational data, the idea is that we have a common key column in 2 of our tables which we can use to join the data for further processing.

\sphinxAtStartPar
In our example we use a dataset with consumers, restaurants and ratings between those, you can find more information \sphinxhref{https://www.kaggle.com/uciml/restaurant-data-with-consumer-ratings}{here}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rating\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c3\PYGZus{}data\PYGZus{}preprocessing/data/cuisine/rating\PYGZus{}final.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{rating\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     userID  placeID  rating  food\PYGZus{}rating  service\PYGZus{}rating
0     U1077   135085       2            2               2
1     U1077   135038       2            2               1
2     U1077   132825       2            2               2
3     U1077   135060       1            2               2
4     U1068   135104       1            1               2
...     ...      ...     ...          ...             ...
1156  U1043   132630       1            1               1
1157  U1011   132715       1            1               0
1158  U1068   132733       1            1               0
1159  U1068   132594       1            1               1
1160  U1068   132660       0            0               0

[1161 rows x 5 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
this first table we read contains the userID from whom the rating came, the placeID is the restaurant he/she rated and the numerical values of the 3 different ratings.

\sphinxAtStartPar
Perhaps you can find out what the min and max values for the ratings are?

\sphinxAtStartPar
to know the type of restaurant, we can not read another table

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cuisine\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c3\PYGZus{}data\PYGZus{}preprocessing/data/cuisine/chefmozcuisine.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{cuisine\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     placeID        Rcuisine
0     135110         Spanish
1     135109         Italian
2     135107  Latin\PYGZus{}American
3     135106         Mexican
4     135105       Fast\PYGZus{}Food
..       ...             ...
911   132005         Seafood
912   132004         Seafood
913   132003   International
914   132002         Seafood
915   132001   Dutch\PYGZhy{}Belgian

[916 rows x 2 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
This table also contains the placeID, so we should be able to merge/join these 2 tables and create a new table with info of both.
Notice how we specify the ‘on’ parameter where we denote placeID as our common key.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{rating\PYGZus{}df}\PYG{p}{,} \PYG{n}{cuisine\PYGZus{}df}\PYG{p}{,} \PYG{n}{on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{placeID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{inner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{merged\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     userID  placeID  rating  food\PYGZus{}rating  service\PYGZus{}rating   Rcuisine
0     U1077   135085       2            2               2  Fast\PYGZus{}Food
1     U1108   135085       1            2               1  Fast\PYGZus{}Food
2     U1081   135085       1            2               1  Fast\PYGZus{}Food
3     U1056   135085       2            2               2  Fast\PYGZus{}Food
4     U1134   135085       2            1               2  Fast\PYGZus{}Food
...     ...      ...     ...          ...             ...        ...
1038  U1061   132958       2            2               2   American
1039  U1025   132958       1            0               0   American
1040  U1097   132958       2            1               1   American
1041  U1096   132958       1            2               2   American
1042  U1136   132958       2            2               2   American

[1043 rows x 6 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Great! now we have more info about the rating that were given, being the type of cuisine that they rated.
We could figure out which cuisines are available in our dataset and do a comparison, let us count the occurences of each cuisine.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}df}\PYG{o}{.}\PYG{n}{Rcuisine}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Mexican             238
Bar                 140
Cafeteria           102
Fast\PYGZus{}Food            91
Seafood              62
Bar\PYGZus{}Pub\PYGZus{}Brewery      59
Pizzeria             51
Chinese              41
American             39
International        37
Contemporary         32
Burgers              31
Japanese             29
Italian              26
Family               14
Cafe\PYGZhy{}Coffee\PYGZus{}Shop     12
Breakfast\PYGZhy{}Brunch      9
Game                  7
Vietnamese            6
Bakery                5
Mediterranean         4
Armenian              4
Regional              4
Name: Rcuisine, dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
A lot of mexican, which is not surpising as this dataset comes from Mexico.
I wonder if there is a difference between ‘Bar’ and ‘Bar\_Pub\_Brewery’, we can see if the average rating for those 2 differ.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{cuisine} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bar}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bar\PYGZus{}Pub\PYGZus{}Brewery}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{cuisine}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{merged\PYGZus{}df}\PYG{p}{[}\PYG{n}{merged\PYGZus{}df}\PYG{o}{.}\PYG{n}{Rcuisine}\PYG{o}{==}\PYG{n}{cuisine}\PYG{p}{]}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Bar
rating            1.200000
food\PYGZus{}rating       1.135714
service\PYGZus{}rating    1.085714
dtype: float64

Bar\PYGZus{}Pub\PYGZus{}Brewery
rating            1.305085
food\PYGZus{}rating       1.169492
service\PYGZus{}rating    1.203390
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
just looking at the averages we can deduces that while food ratings do not change a lot, the service seems a lot better at the Brewery.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}df}\PYG{p}{[}\PYG{n}{merged\PYGZus{}df}\PYG{o}{.}\PYG{n}{Rcuisine}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cafeteria}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
rating            1.205882
food\PYGZus{}rating       1.127451
service\PYGZus{}rating    1.078431
dtype: float64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}df}\PYG{p}{[}\PYG{n}{merged\PYGZus{}df}\PYG{o}{.}\PYG{n}{Rcuisine}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cafe\PYGZhy{}Coffee\PYGZus{}Shop}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
rating            1.583333
food\PYGZus{}rating       1.333333
service\PYGZus{}rating    1.416667
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
As easy as it looks, we can now merge information of different tables in our dataset and perform some simple comparisons, in later sections we will see how we can improve on those.

\sphinxAtStartPar
As an exercise I already read in the table containing the info about which type of payment the user has opted for.
Could you find out if the type of payment could have an influence on the rating?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{user\PYGZus{}payment\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c3\PYGZus{}data\PYGZus{}preprocessing/data/cuisine/userpayment.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{user\PYGZus{}payment\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    userID          Upayment
0    U1001              cash
1    U1002              cash
2    U1003              cash
3    U1004              cash
4    U1004  bank\PYGZus{}debit\PYGZus{}cards
..     ...               ...
172  U1134              cash
173  U1135              cash
174  U1136              cash
175  U1137              cash
176  U1138              cash

[177 rows x 2 columns]
\end{sphinxVerbatim}


\chapter{Groupby}
\label{\detokenize{c3_data_preprocessing/groupby:groupby}}\label{\detokenize{c3_data_preprocessing/groupby::doc}}
\sphinxAtStartPar
In the previous section we saw how to combine information of multiple tables from our dataset.
Here we are going to build further on that by using the merged information to group on categorical variables.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rating\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c3\PYGZus{}data\PYGZus{}preprocessing/data/cuisine/rating\PYGZus{}final.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{rating\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     userID  placeID  rating  food\PYGZus{}rating  service\PYGZus{}rating
0     U1077   135085       2            2               2
1     U1077   135038       2            2               1
2     U1077   132825       2            2               2
3     U1077   135060       1            2               2
4     U1068   135104       1            1               2
...     ...      ...     ...          ...             ...
1156  U1043   132630       1            1               1
1157  U1011   132715       1            1               0
1158  U1068   132733       1            1               0
1159  U1068   132594       1            1               1
1160  U1068   132660       0            0               0

[1161 rows x 5 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Again we have our rating data containing the users, places and ratings they gave.
As a simple example we could just group by the placeID column and take the mean, this would give us the mean rating for each restaurant

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{grouped\PYGZus{}rating\PYGZus{}df} \PYG{o}{=} \PYG{n}{rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{placeID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{grouped\PYGZus{}rating\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           rating  food\PYGZus{}rating  service\PYGZus{}rating
placeID                                       
132654   0.250000         0.25        0.250000
135040   0.250000         0.25        0.250000
132560   0.500000         1.00        0.250000
132663   0.500000         0.50        0.666667
135069   0.500000         0.50        0.750000
...           ...          ...             ...
132755   1.800000         2.00        1.600000
132922   1.833333         1.50        1.833333
134986   2.000000         2.00        2.000000
135034   2.000000         2.00        1.600000
132955   2.000000         1.80        1.800000

[130 rows x 3 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Keep in mind that this might be tricky, as we do not always have as much records per group, we could count the amount per records using a groupby operation and count.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{placeID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{rating}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
placeID
132560     4
132561     4
132564     4
132572    15
132583     4
          ..
135088     6
135104     7
135106    10
135108    11
135109     4
Name: rating, Length: 130, dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
Taking an average of 4 ratings might not be ideal, so we should keep in mind that our groups have a good sample size.

\sphinxAtStartPar
Let’s make things more interesting and insert some location data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/cuisine/geoplaces2.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{placeID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{geo\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
          latitude   longitude  \PYGZbs{}
placeID                          
134999   18.915421  \PYGZhy{}99.184871   
132825   22.147392 \PYGZhy{}100.983092   
135106   22.149709 \PYGZhy{}100.976093   
132667   23.752697  \PYGZhy{}99.163359   
132613   23.752903  \PYGZhy{}99.165076   
...            ...         ...   
132866   22.141220 \PYGZhy{}100.931311   
135072   22.149192 \PYGZhy{}101.002936   
135109   18.921785  \PYGZhy{}99.235350   
135019   18.875011  \PYGZhy{}99.159422   
132877   22.135364 \PYGZhy{}100.934948   

                                            the\PYGZus{}geom\PYGZus{}meter  \PYGZbs{}
placeID                                                      
134999   0101000020957F000088568DE356715AC138C0A525FC46...   
132825   0101000020957F00001AD016568C4858C1243261274BA5...   
135106   0101000020957F0000649D6F21634858C119AE9BF528A3...   
132667   0101000020957F00005D67BCDDED8157C1222A2DC8D84D...   
132613   0101000020957F00008EBA2D06DC8157C194E03B7B504E...   
...                                                    ...   
132866   0101000020957F000013871838EC4A58C1B5DF74F8E396...   
135072   0101000020957F0000E7B79B1DB94758C1D29BC363D8AA...   
135109   0101000020957F0000A6BF695F136F5AC1DADF87B20556...   
135019   0101000020957F0000B49B2E5C6E785AC12F9D58435241...   
132877   0101000020957F000090735015B84B58C1AF0DC0414698...   

                                   name  \PYGZbs{}
placeID                                   
134999                  Kiku Cuernavaca   
132825                  puesto de tacos   
135106       El Rinc�n de San Francisco   
132667   little pizza Emilio Portes Gil   
132613                    carnitas\PYGZus{}mata   
...                                 ...   
132866                          Chaires   
135072                       Sushi Itto   
135109                        Paniroles   
135019      Restaurant Bar Coty y Pablo   
132877                 sirloin stockade   

                                                 address             city  \PYGZbs{}
placeID                                                                     
134999                                        Revolucion       Cuernavaca   
132825            esquina santos degollado y leon guzman           s.l.p.   
135106                                   Universidad 169  San Luis Potosi   
132667                           calle emilio portes gil        victoria    
132613                            lic. Emilio portes gil         victoria   
...                                                  ...              ...   
132866                                  Ricardo B. Anaya  San Luis Potosi   
135072                Venustiano Carranza 1809 C Polanco  San Luis Potosi   
135109                                                 ?                ?   
135019   Paseo de Las Fuentes 24 Pedregal de Las Fuentes         Jiutepec   
132877                                                 ?                ?   

                   state country fax    zip            alcohol   smoking\PYGZus{}area  \PYGZbs{}
placeID                                                                         
134999           Morelos  Mexico   ?      ?  No\PYGZus{}Alcohol\PYGZus{}Served           none   
132825            s.l.p.  mexico   ?  78280  No\PYGZus{}Alcohol\PYGZus{}Served           none   
135106   San Luis Potosi  Mexico   ?  78000          Wine\PYGZhy{}Beer    only at bar   
132667        tamaulipas       ?   ?      ?  No\PYGZus{}Alcohol\PYGZus{}Served           none   
132613        Tamaulipas  Mexico   ?      ?  No\PYGZus{}Alcohol\PYGZus{}Served      permitted   
...                  ...     ...  ..    ...                ...            ...   
132866   San Luis Potosi  Mexico   ?      ?  No\PYGZus{}Alcohol\PYGZus{}Served  not permitted   
135072               SLP  Mexico   ?  78220  No\PYGZus{}Alcohol\PYGZus{}Served           none   
135109                 ?       ?   ?      ?          Wine\PYGZhy{}Beer  not permitted   
135019           Morelos  Mexico   ?      ?  No\PYGZus{}Alcohol\PYGZus{}Served           none   
132877                 ?       ?   ?      ?  No\PYGZus{}Alcohol\PYGZus{}Served           none   

        dress\PYGZus{}code     accessibility   price                    url Rambience  \PYGZbs{}
placeID                                                                         
134999    informal  no\PYGZus{}accessibility  medium  kikucuernavaca.com.mx  familiar   
132825    informal        completely     low                      ?  familiar   
135106    informal         partially  medium                      ?  familiar   
132667    informal        completely     low                      ?  familiar   
132613    informal        completely  medium                      ?  familiar   
...            ...               ...     ...                    ...       ...   
132866    informal        completely  medium                      ?  familiar   
135072    informal  no\PYGZus{}accessibility  medium      sushi\PYGZhy{}itto.com.mx  familiar   
135109    informal  no\PYGZus{}accessibility  medium                      ?     quiet   
135019    informal        completely     low                      ?  familiar   
132877    informal        completely     low                      ?  familiar   

        franchise    area other\PYGZus{}services  
placeID                                   
134999          f  closed           none  
132825          f    open           none  
135106          f    open           none  
132667          t  closed           none  
132613          t  closed           none  
...           ...     ...            ...  
132866          f  closed           none  
135072          f  closed           none  
135109          f  closed       Internet  
135019          f  closed           none  
132877          f  closed           none  

[130 rows x 20 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we have for each restaurant information about its location, I mentioned earlier that grouping per restaurant might be dangerous as some restaurants have nearly no reviews.
By adding information such as city, state and country we have other categorical variables to group by.
Notice how we use the merge operation from previous section, but this time specify our common key is the index.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}rating\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{grouped\PYGZus{}rating\PYGZus{}df}\PYG{p}{,} \PYG{n}{geo\PYGZus{}df}\PYG{p}{,} \PYG{n}{left\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{right\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{geo\PYGZus{}rating\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           rating  food\PYGZus{}rating  service\PYGZus{}rating   latitude   longitude  \PYGZbs{}
placeID                                                                 
132654   0.250000         0.25        0.250000  23.735523  \PYGZhy{}99.129588   
135040   0.250000         0.25        0.250000  22.135617 \PYGZhy{}100.969709   
132560   0.500000         1.00        0.250000  23.752304  \PYGZhy{}99.166913   
132663   0.500000         0.50        0.666667  23.752511  \PYGZhy{}99.166954   
135069   0.500000         0.50        0.750000  22.140129 \PYGZhy{}100.944872   
...           ...          ...             ...        ...         ...   
132755   1.800000         2.00        1.600000  22.153324 \PYGZhy{}101.019546   
132922   1.833333         1.50        1.833333  22.151135 \PYGZhy{}100.982311   
134986   2.000000         2.00        2.000000  18.928798  \PYGZhy{}99.239513   
135034   2.000000         2.00        1.600000  22.140517 \PYGZhy{}101.021422   
132955   2.000000         1.80        1.800000  22.147622 \PYGZhy{}101.010275   

                                            the\PYGZus{}geom\PYGZus{}meter  \PYGZbs{}
placeID                                                      
132654   0101000020957F000040E8F628488557C18224E8B94845...   
135040   0101000020957F00001B552189B84A58C15A2AAEFD2CA2...   
132560   0101000020957F0000FC60BDA8E88157C1B2C357D6DA4E...   
132663   0101000020957F0000FDF8D26EE08157C1FEDB6A1FDB4E...   
135069   0101000020957F000038E5D546B74A58C18FD29AD0D29A...   
...                                                    ...   
132755   0101000020957F000026CADE45A14658C1F011EBCA55AF...   
132922   0101000020957F000060A98A38FF4758C146718E41D9A4...   
134986   0101000020957F00002A0D05E2D96D5AC1AB058CB1EC56...   
135034   0101000020957F000026D92BB4894858C161A7552DA2B0...   
132955   0101000020957F000068BE7C87C24758C1920A360A08AD...   

                                          name  \PYGZbs{}
placeID                                          
132654   Carnitas Mata  Calle 16 de Septiembre   
135040                Restaurant los Compadres   
132560                      puesto de gorditas   
132663                               tacos abi   
135069               Abondance Restaurante Bar   
...                                        ...   
132755                    La Estrella de Dimas   
132922                    cafe punta del cielo   
134986                Restaurant Las Mananitas   
135034              Michiko Restaurant Japones   
132955                               emilianos   

                                             address             city  \PYGZbs{}
placeID                                                                 
132654                              16 de Septiembre        victoria    
135040                Camino a Simon Diaz 155 Centro  San Luis Potosi   
132560                         frente al tecnologico         victoria   
132663                                             ?         victoria   
135069                   Industrias 908 Valle Dorado  San Luis Potosi   
...                                              ...              ...   
132755                           Av. de los Pintores  San Luis Potosi   
132922                                             ?                ?   
134986                           Ricardo Linares 107       Cuernavaca   
135034   Cordillera de Los Alpes 160 Lomas 2 Seccion  San Luis Potosi   
132955                           venustiano carranza   san luis potos   

              state  ...            alcohol smoking\PYGZus{}area dress\PYGZus{}code  \PYGZbs{}
placeID              ...                                              
132654   tamaulipas  ...  No\PYGZus{}Alcohol\PYGZus{}Served         none   informal   
135040          SLP  ...          Wine\PYGZhy{}Beer         none   informal   
132560   tamaulipas  ...  No\PYGZus{}Alcohol\PYGZus{}Served    permitted   informal   
132663   tamaulipas  ...  No\PYGZus{}Alcohol\PYGZus{}Served         none   informal   
135069          SLP  ...          Wine\PYGZhy{}Beer         none   informal   
...             ...  ...                ...          ...        ...   
132755       S.L.P.  ...  No\PYGZus{}Alcohol\PYGZus{}Served         none   informal   
132922            ?  ...  No\PYGZus{}Alcohol\PYGZus{}Served    permitted     formal   
134986      Morelos  ...          Wine\PYGZhy{}Beer         none     formal   
135034          SLP  ...  No\PYGZus{}Alcohol\PYGZus{}Served         none   informal   
132955       mexico  ...          Wine\PYGZhy{}Beer         none   informal   

            accessibility   price                  url Rambience franchise  \PYGZbs{}
placeID                                                                      
132654         completely     low                    ?  familiar         f   
135040   no\PYGZus{}accessibility    high                    ?  familiar         f   
132560   no\PYGZus{}accessibility     low                    ?  familiar         f   
132663         completely     low                    ?  familiar         f   
135069   no\PYGZus{}accessibility     low                    ?  familiar         f   
...                   ...     ...                  ...       ...       ...   
132755          partially  medium                    ?  familiar         f   
132922         completely  medium                    ?  familiar         f   
134986   no\PYGZus{}accessibility    high  lasmananitas.com.mx  familiar         f   
135034   no\PYGZus{}accessibility  medium                    ?  familiar         f   
132955         completely     low                    ?  familiar         t   

           area other\PYGZus{}services  
placeID                         
132654   closed           none  
135040   closed           none  
132560     open           none  
132663   closed           none  
135069   closed           none  
...         ...            ...  
132755   closed        variety  
132922   closed           none  
134986   closed           none  
135034   closed           none  
132955   closed        variety  

[130 rows x 23 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
By adding this amount of data, things are getting a bit cluttered, thankfully we can use pandas to get a list of all our columns.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Index([\PYGZsq{}rating\PYGZsq{}, \PYGZsq{}food\PYGZus{}rating\PYGZsq{}, \PYGZsq{}service\PYGZus{}rating\PYGZsq{}, \PYGZsq{}latitude\PYGZsq{}, \PYGZsq{}longitude\PYGZsq{},
       \PYGZsq{}the\PYGZus{}geom\PYGZus{}meter\PYGZsq{}, \PYGZsq{}name\PYGZsq{}, \PYGZsq{}address\PYGZsq{}, \PYGZsq{}city\PYGZsq{}, \PYGZsq{}state\PYGZsq{}, \PYGZsq{}country\PYGZsq{}, \PYGZsq{}fax\PYGZsq{},
       \PYGZsq{}zip\PYGZsq{}, \PYGZsq{}alcohol\PYGZsq{}, \PYGZsq{}smoking\PYGZus{}area\PYGZsq{}, \PYGZsq{}dress\PYGZus{}code\PYGZsq{}, \PYGZsq{}accessibility\PYGZsq{},
       \PYGZsq{}price\PYGZsq{}, \PYGZsq{}url\PYGZsq{}, \PYGZsq{}Rambience\PYGZsq{}, \PYGZsq{}franchise\PYGZsq{}, \PYGZsq{}area\PYGZsq{}, \PYGZsq{}other\PYGZus{}services\PYGZsq{}],
      dtype=\PYGZsq{}object\PYGZsq{})
\end{sphinxVerbatim}

\sphinxAtStartPar
How about we try and see if we can find a difference between countries for the ratings?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{country}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           rating  food\PYGZus{}rating  service\PYGZus{}rating
country                                       
?        1.166045     1.232946        1.069169
Mexico   1.200977     1.229093        1.118162
mexico   1.062660     1.069006        0.900064
\end{sphinxVerbatim}

\sphinxAtStartPar
Ah, it seems we forgot to do some data cleaning here, perhaps you could jump in and fix this string problem, might as well tackle the missing value while we are at it.
Aside from that, we can see that lower\sphinxhyphen{}case Mexico is not doing very well, perhaps the food was so bad they forgot how to write Mexico?

\sphinxAtStartPar
Jokes aside, do you see the ressemblance between this and our rudimentary approach of comparing different categories?
We are slowly getting more and more efficient using these operations, how about the difference between alcohol consumption?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                     rating  food\PYGZus{}rating  service\PYGZus{}rating
alcohol                                                 
Full\PYGZus{}Bar           1.287124     1.218315        1.170311
No\PYGZus{}Alcohol\PYGZus{}Served  1.148075     1.194730        1.042417
Wine\PYGZhy{}Beer          1.231887     1.261840        1.174437
\end{sphinxVerbatim}

\sphinxAtStartPar
Something we can remark here is that the food rating for no alcohol locations seems to be holding up, whilst the general rating and service rating fall behind.
This would suggest that the food rating indeed is for the food, where the type of drinks served have no influence.

\sphinxAtStartPar
As a last we look at the difference between accessibility, does that influences our ratings?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{accessibility}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                    rating  food\PYGZus{}rating  service\PYGZus{}rating
accessibility                                          
completely        1.132494     1.203597        1.049709
no\PYGZus{}accessibility  1.196189     1.206242        1.091278
partially         1.275356     1.330294        1.219991
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems having partial accessibility is the way to go here, performing better than complete accessibility.
We can however find that is due to a low sample size of 9 restaurants, making it prone to variation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{accessibility}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
no\PYGZus{}accessibility    76
completely          45
partially            9
Name: accessibility, dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
You should get the hang of it by now, perhaps you can play some more with the other categories.

\sphinxAtStartPar
There is one thing I still would like to address, you perhaps have notices that in the beginning I first took the average rating per restaurant and later again took the average per category.
This is a bad practice as a bad restaurant with one review has equal influence as a good restaurant with 100 reviews, perhaps you can think of a way to group all reviews from a category instead of the average for each restaurant?

\sphinxAtStartPar
In the previous section we added the cuisine type, perhaps you could do some groupby operations on that too here?


\chapter{Pivot}
\label{\detokenize{c3_data_preprocessing/pivot:pivot}}\label{\detokenize{c3_data_preprocessing/pivot::doc}}
\sphinxAtStartPar
When using the groupby operation we used 1 categorical variable to seperate/group our data into those categories.
Here we go a step further and use 2 categories to aggregate our data, resulting in a comparison matrix.

\sphinxAtStartPar
Aside from that, the pivot operation can in general be used to go from a long data format, to a wide data format.
To keep things uniform we stick with the same cuisine dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rating\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c3\PYGZus{}data\PYGZus{}preprocessing/data/cuisine/rating\PYGZus{}final.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{rating\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     userID  placeID  rating  food\PYGZus{}rating  service\PYGZus{}rating
0     U1077   135085       2            2               2
1     U1077   135038       2            2               1
2     U1077   132825       2            2               2
3     U1077   135060       1            2               2
4     U1068   135104       1            1               2
...     ...      ...     ...          ...             ...
1156  U1043   132630       1            1               1
1157  U1011   132715       1            1               0
1158  U1068   132733       1            1               0
1159  U1068   132594       1            1               1
1160  U1068   132660       0            0               0

[1161 rows x 5 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
And again we merge with the geolocations data, I feel that it becomse obvious here how these operations are very related to eachother.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c3\PYGZus{}data\PYGZus{}preprocessing/data/cuisine/geoplaces2.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
A subtle difference between last time is that I did not first group per restaurant, however this leads to a dataframe that has a lot of redundant information!
Try to look in the merged dataframe and spot the copies of data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}rating\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{rating\PYGZus{}df}\PYG{p}{,} \PYG{n}{geo\PYGZus{}df}\PYG{p}{,} \PYG{n}{on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{placeID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{geo\PYGZus{}rating\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     userID  placeID  rating  food\PYGZus{}rating  service\PYGZus{}rating   latitude  \PYGZbs{}
0     U1077   135085       2            2               2  22.150802   
1     U1108   135085       1            2               1  22.150802   
2     U1081   135085       1            2               1  22.150802   
3     U1056   135085       2            2               2  22.150802   
4     U1134   135085       2            1               2  22.150802   
...     ...      ...     ...          ...             ...        ...   
1156  U1061   132958       2            2               2  22.144979   
1157  U1025   132958       1            0               0  22.144979   
1158  U1097   132958       2            1               1  22.144979   
1159  U1096   132958       1            2               2  22.144979   
1160  U1136   132958       2            2               2  22.144979   

       longitude                                     the\PYGZus{}geom\PYGZus{}meter  \PYGZbs{}
0    \PYGZhy{}100.982680  0101000020957F00009F823DA6094858C18A2D4D37F9A4...   
1    \PYGZhy{}100.982680  0101000020957F00009F823DA6094858C18A2D4D37F9A4...   
2    \PYGZhy{}100.982680  0101000020957F00009F823DA6094858C18A2D4D37F9A4...   
3    \PYGZhy{}100.982680  0101000020957F00009F823DA6094858C18A2D4D37F9A4...   
4    \PYGZhy{}100.982680  0101000020957F00009F823DA6094858C18A2D4D37F9A4...   
...          ...                                                ...   
1156 \PYGZhy{}101.005683  0101000020957F000049095EB34A4858C15CB4BD1EE1AB...   
1157 \PYGZhy{}101.005683  0101000020957F000049095EB34A4858C15CB4BD1EE1AB...   
1158 \PYGZhy{}101.005683  0101000020957F000049095EB34A4858C15CB4BD1EE1AB...   
1159 \PYGZhy{}101.005683  0101000020957F000049095EB34A4858C15CB4BD1EE1AB...   
1160 \PYGZhy{}101.005683  0101000020957F000049095EB34A4858C15CB4BD1EE1AB...   

                        name                         address  ...  \PYGZbs{}
0     Tortas Locas Hipocampo  Venustiano Carranza 719 Centro  ...   
1     Tortas Locas Hipocampo  Venustiano Carranza 719 Centro  ...   
2     Tortas Locas Hipocampo  Venustiano Carranza 719 Centro  ...   
3     Tortas Locas Hipocampo  Venustiano Carranza 719 Centro  ...   
4     Tortas Locas Hipocampo  Venustiano Carranza 719 Centro  ...   
...                      ...                             ...  ...   
1156      tacos los volcanes          avenida hivno nacional  ...   
1157      tacos los volcanes          avenida hivno nacional  ...   
1158      tacos los volcanes          avenida hivno nacional  ...   
1159      tacos los volcanes          avenida hivno nacional  ...   
1160      tacos los volcanes          avenida hivno nacional  ...   

                alcohol   smoking\PYGZus{}area dress\PYGZus{}code     accessibility   price  \PYGZbs{}
0     No\PYGZus{}Alcohol\PYGZus{}Served  not permitted   informal  no\PYGZus{}accessibility  medium   
1     No\PYGZus{}Alcohol\PYGZus{}Served  not permitted   informal  no\PYGZus{}accessibility  medium   
2     No\PYGZus{}Alcohol\PYGZus{}Served  not permitted   informal  no\PYGZus{}accessibility  medium   
3     No\PYGZus{}Alcohol\PYGZus{}Served  not permitted   informal  no\PYGZus{}accessibility  medium   
4     No\PYGZus{}Alcohol\PYGZus{}Served  not permitted   informal  no\PYGZus{}accessibility  medium   
...                 ...            ...        ...               ...     ...   
1156  No\PYGZus{}Alcohol\PYGZus{}Served           none   informal        completely     low   
1157  No\PYGZus{}Alcohol\PYGZus{}Served           none   informal        completely     low   
1158  No\PYGZus{}Alcohol\PYGZus{}Served           none   informal        completely     low   
1159  No\PYGZus{}Alcohol\PYGZus{}Served           none   informal        completely     low   
1160  No\PYGZus{}Alcohol\PYGZus{}Served           none   informal        completely     low   

     url Rambience franchise    area other\PYGZus{}services  
0      ?  familiar         f  closed           none  
1      ?  familiar         f  closed           none  
2      ?  familiar         f  closed           none  
3      ?  familiar         f  closed           none  
4      ?  familiar         f  closed           none  
...   ..       ...       ...     ...            ...  
1156   ?     quiet         t  closed           none  
1157   ?     quiet         t  closed           none  
1158   ?     quiet         t  closed           none  
1159   ?     quiet         t  closed           none  
1160   ?     quiet         t  closed           none  

[1161 rows x 25 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Now that we have our workable data, we can choose 2 categories and create a comparison matrix using the pivot operation.
Yet there might be a problem that we still have to resolve, can you figure out the problem reading the error at the end of the stack trace below?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{pivot}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ValueError}\PYG{g+gWhitespace}{                                }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}20513}\PYG{o}{/}\PYG{l+m+mf}{1351770208.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{n}{geo\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{pivot}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/frame.py} in \PYG{n+ni}{pivot}\PYG{n+nt}{(self, index, columns, values)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{7791}         \PYG{k+kn}{from} \PYG{n+nn}{pandas}\PYG{n+nn}{.}\PYG{n+nn}{core}\PYG{n+nn}{.}\PYG{n+nn}{reshape}\PYG{n+nn}{.}\PYG{n+nn}{pivot} \PYG{k+kn}{import} \PYG{n}{pivot}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{7792} 
\PYG{n+ne}{\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{7793}         \PYG{k}{return} \PYG{n}{pivot}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{n}{index}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{n}{columns}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{n}{values}\PYG{p}{)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{7794} 
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{7795}     \PYG{n}{\PYGZus{}shared\PYGZus{}docs}\PYG{p}{[}

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/reshape/pivot.py} in \PYG{n+ni}{pivot}\PYG{n+nt}{(data, index, columns, values)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{515}         \PYG{k}{else}\PYG{p}{:}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{516}             \PYG{n}{indexed} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{\PYGZus{}constructor\PYGZus{}sliced}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{n}{values}\PYG{p}{]}\PYG{o}{.}\PYG{n}{\PYGZus{}values}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{n}{multiindex}\PYG{p}{)}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{517}     \PYG{k}{return} \PYG{n}{indexed}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(}\PYG{n}{columns\PYGZus{}listlike}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{518} 
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{519} 

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/series.py} in \PYG{n+ni}{unstack}\PYG{n+nt}{(self, level, fill\PYGZus{}value)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{4079}         \PYG{k+kn}{from} \PYG{n+nn}{pandas}\PYG{n+nn}{.}\PYG{n+nn}{core}\PYG{n+nn}{.}\PYG{n+nn}{reshape}\PYG{n+nn}{.}\PYG{n+nn}{reshape} \PYG{k+kn}{import} \PYG{n}{unstack}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{4080} 
\PYG{n+ne}{\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{4081}         \PYG{k}{return} \PYG{n}{unstack}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{level}\PYG{p}{,} \PYG{n}{fill\PYGZus{}value}\PYG{p}{)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{4082} 
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{4083}     \PYG{c+c1}{\PYGZsh{} \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/reshape/reshape.py} in \PYG{n+ni}{unstack}\PYG{n+nt}{(obj, level, fill\PYGZus{}value)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{458}         \PYG{k}{if} \PYG{n}{is\PYGZus{}1d\PYGZus{}only\PYGZus{}ea\PYGZus{}dtype}\PYG{p}{(}\PYG{n}{obj}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{)}\PYG{p}{:}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{459}             \PYG{k}{return} \PYG{n}{\PYGZus{}unstack\PYGZus{}extension\PYGZus{}series}\PYG{p}{(}\PYG{n}{obj}\PYG{p}{,} \PYG{n}{level}\PYG{p}{,} \PYG{n}{fill\PYGZus{}value}\PYG{p}{)}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{460}         \PYG{n}{unstacker} \PYG{o}{=} \PYG{n}{\PYGZus{}Unstacker}\PYG{p}{(}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{461}             \PYG{n}{obj}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{level}\PYG{o}{=}\PYG{n}{level}\PYG{p}{,} \PYG{n}{constructor}\PYG{o}{=}\PYG{n}{obj}\PYG{o}{.}\PYG{n}{\PYGZus{}constructor\PYGZus{}expanddim}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{462}         \PYG{p}{)}

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/reshape/reshape.py} in \PYG{n+ni}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{n+nt}{(self, index, level, constructor)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{131}             \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Unstacked DataFrame is too big, causing int32 overflow}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{132} 
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{133}         \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}make\PYGZus{}selectors}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{134} 
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{135}     \PYG{n+nd}{@cache\PYGZus{}readonly}

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/reshape/reshape.py} in \PYG{n+ni}{\PYGZus{}make\PYGZus{}selectors}\PYG{n+nt}{(self)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{183} 
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{184}         \PYG{k}{if} \PYG{n}{mask}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{index}\PYG{p}{)}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{185}             \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Index contains duplicate entries, cannot reshape}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{186} 
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{187}         \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{group\PYGZus{}index} \PYG{o}{=} \PYG{n}{comp\PYGZus{}index}

\PYG{n+ne}{ValueError}: Index contains duplicate entries, cannot reshape
\end{sphinxVerbatim}

\sphinxAtStartPar
It says: ‘Index contains duplicate entries, cannot reshape’ meaning that some combinations of our 2 categories, alcohol and smoking area have duplicates, which is understandable.
I opted to solve this by grouping over the 2 categories and taking the mean for each combination, then i take this grouped data and pivot by setting the alcohol consumption as index and the smoking are as columns.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{grouped\PYGZus{}geo\PYGZus{}rating\PYGZus{}df} \PYG{o}{=} \PYG{n}{geo\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{grouped\PYGZus{}geo\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{pivot}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
smoking\PYGZus{}area           none  not permitted  only at bar  permitted   section
alcohol                                                                     
Full\PYGZus{}Bar           1.305556       0.857143          NaN   1.500000  1.272727
No\PYGZus{}Alcohol\PYGZus{}Served  1.186788       1.124402          NaN   1.114286  1.265823
Wine\PYGZhy{}Beer          1.217391       1.000000     1.368421   1.300000  1.275000
\end{sphinxVerbatim}

\sphinxAtStartPar
Wonderful! Now we have for each combination an average rating, notice however that not every combination has the same sample size, so comparing might be tricky if you only have a few ratings.

\sphinxAtStartPar
To figure that out I counted the ratings per combination.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{pivot}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
smoking\PYGZus{}area        none  not permitted  only at bar  permitted  section
alcohol                                                                 
Full\PYGZus{}Bar            36.0            7.0          NaN        4.0     33.0
No\PYGZus{}Alcohol\PYGZus{}Served  439.0          209.0          NaN       35.0     79.0
Wine\PYGZhy{}Beer          161.0            9.0         19.0       10.0    120.0
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems that there might e a correlation between the 2 categories, as a lot of place where smoking is not permitted/none, there is no alcohol served, which makes sense.
Comparing the ratings with alcohol allowance for places where smoking is not permitted is not a good idea, the counts are 7, 209 and 9, very unbalanced.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Index([\PYGZsq{}placeID\PYGZsq{}, \PYGZsq{}latitude\PYGZsq{}, \PYGZsq{}longitude\PYGZsq{}, \PYGZsq{}the\PYGZus{}geom\PYGZus{}meter\PYGZsq{}, \PYGZsq{}name\PYGZsq{}, \PYGZsq{}address\PYGZsq{},
       \PYGZsq{}city\PYGZsq{}, \PYGZsq{}state\PYGZsq{}, \PYGZsq{}country\PYGZsq{}, \PYGZsq{}fax\PYGZsq{}, \PYGZsq{}zip\PYGZsq{}, \PYGZsq{}alcohol\PYGZsq{}, \PYGZsq{}smoking\PYGZus{}area\PYGZsq{},
       \PYGZsq{}dress\PYGZus{}code\PYGZsq{}, \PYGZsq{}accessibility\PYGZsq{}, \PYGZsq{}price\PYGZsq{}, \PYGZsq{}url\PYGZsq{}, \PYGZsq{}Rambience\PYGZsq{}, \PYGZsq{}franchise\PYGZsq{},
       \PYGZsq{}area\PYGZsq{}, \PYGZsq{}other\PYGZus{}services\PYGZsq{}],
      dtype=\PYGZsq{}object\PYGZsq{})
\end{sphinxVerbatim}

\sphinxAtStartPar
I printed the columns above, perhaps you could figure out a relation between the price category and the (R)ambience of the restaurant?
Perhaps there are other combinations of which I did not think of, try some out!


\chapter{Using SQL}
\label{\detokenize{c3_data_preprocessing/sql_integration:using-sql}}\label{\detokenize{c3_data_preprocessing/sql_integration::doc}}
\sphinxAtStartPar
In this notebook we are going to do things different, instead of using python and pandas for data wrangling/processing we outsource them to the SQL, a language used for databases.

\sphinxAtStartPar
As it would be complicated to setup a complete SQL server, I opted to create a local database using SQLite which is built\sphinxhyphen{}in the sqlalchemy library used by python to interact with a database.

\sphinxAtStartPar
We start by importing or necessary libraries

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{sqlalchemy}
\end{sphinxVerbatim}

\sphinxAtStartPar
As mentioned we are going to create a local SQL database and dump it to a .db file. In order to do that we first have to read our data from comma seperated value (CSV) files that were provided within the repository.

\sphinxAtStartPar
We use pandas to read them and collect them into an object data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c3\PYGZus{}data\PYGZus{}preprocessing/data/cuisine/rating\PYGZus{}final.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cuisine}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c3\PYGZus{}data\PYGZus{}preprocessing/data/cuisine/chefmozcuisine.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{parking}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c3\PYGZus{}data\PYGZus{}preprocessing/data/cuisine/chefmozparking.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user\PYGZus{}cuisine}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c3\PYGZus{}data\PYGZus{}preprocessing/data/cuisine/usercuisine.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user\PYGZus{}payment}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c3\PYGZus{}data\PYGZus{}preprocessing/data/cuisine/userpayment.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user\PYGZus{}profile}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c3\PYGZus{}data\PYGZus{}preprocessing/data/cuisine/userprofile.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{na\PYGZus{}values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{?}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\sphinxAtStartPar
Before we can act with our database we need to create an engine by setting up the connection. In a more complex situation this would need an url to the server running the database, a userid and password for loging and some other configurations.

\sphinxAtStartPar
In this case we only need the location of our .db file, which i will put in the same location as the notebook.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{engine} \PYG{o}{=} \PYG{n}{sqlalchemy}\PYG{o}{.}\PYG{n}{create\PYGZus{}engine}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sqlite:///ratings.db}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Great! we now have an engine that can run our SQL queries, yet for now our database is empty, let us fill it with all the data we collected earlier!

\sphinxAtStartPar
We use the .to\_sql method of pandas to easily convert the pandas dataframe to a table in our database, each name in our data object will be a table with the corresponding data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{table\PYGZus{}name}\PYG{p}{,} \PYG{n}{df} \PYG{o+ow}{in} \PYG{n}{data}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{df}\PYG{o}{.}\PYG{n}{to\PYGZus{}sql}\PYG{p}{(}\PYG{n}{table\PYGZus{}name}\PYG{p}{,} \PYG{n}{engine}\PYG{p}{,} \PYG{n}{if\PYGZus{}exists}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{replace}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
And with this our migration to SQL has been completed, we now have a SQL server running locally that has several tables containing data.
Instead of using python to do the processing we can instruct our server to handle this, usually resulting is faster compute times, yet results may vary!

\sphinxAtStartPar
Let’s start with a simple example, I saw that we have a table with ratings, to see how it looks by selecting all columns.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT * FROM ratings}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
      index userID  placeID  rating  food\PYGZus{}rating  service\PYGZus{}rating
0         0  U1077   135085       2            2               2
1         1  U1077   135038       2            2               1
2         2  U1077   132825       2            2               2
3         3  U1077   135060       1            2               2
4         4  U1068   135104       1            1               2
...     ...    ...      ...     ...          ...             ...
1156   1156  U1043   132630       1            1               1
1157   1157  U1011   132715       1            1               0
1158   1158  U1068   132733       1            1               0
1159   1159  U1068   132594       1            1               1
1160   1160  U1068   132660       0            0               0

[1161 rows x 6 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
It looks that an index has been copied too, we skipped preparation and it already shows. For now we are going to ignore these steps yet we should clean that later.
If you want to save some time, you can LIMIT your search to a number of rows, next I put a limit of 5 to only retrieve the first 5 results

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT * FROM ratings}
\PYG{l+s+sd}{    LIMIT 5}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   index userID  placeID  rating  food\PYGZus{}rating  service\PYGZus{}rating
0      0  U1077   135085       2            2               2
1      1  U1077   135038       2            2               1
2      2  U1077   132825       2            2               2
3      3  U1077   135060       1            2               2
4      4  U1068   135104       1            1               2
\end{sphinxVerbatim}

\sphinxAtStartPar
Great! Here it does not matter as our database is local and not at all large in size, but this trick might save you a lot of time when exploring.

\sphinxAtStartPar
Next we would like to only select specific columns, by changing the asterisk to the wanted columns the server knowns which columns to retrieve.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT userID, rating FROM ratings}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  userID  rating
0  U1077       2
1  U1077       2
2  U1077       2
3  U1077       1
4  U1068       1
\end{sphinxVerbatim}

\sphinxAtStartPar
Aside from less traffic, this tidies up your data as usually most columns are not needed.

\sphinxAtStartPar
Just like columns, entries can also be filtered, in the next example we use an equation to filter only the ratings with a general rating of 2.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT userID, rating FROM ratings}
\PYG{l+s+sd}{    WHERE ratings.rating = 2}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  userID  rating
0  U1077       2
1  U1077       2
2  U1077       2
3  U1067       2
4  U1103       2
\end{sphinxVerbatim}

\sphinxAtStartPar
Similarly you can also filter based on text fields, for this example I retrieve data from another table, cuisine.
No particular columns are selected yet we want to only retrieve the entries where the column Rcuisine contains a text ending on ‘food’ the percent sign is a wildcard indicating that any text can be present here.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT * FROM cuisine}
\PYG{l+s+sd}{    WHERE Rcuisine LIKE \PYGZsq{}\PYGZpc{}food\PYGZsq{}}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   index  placeID   Rcuisine
0      4   135105  Fast\PYGZus{}Food
1      8   135103  Fast\PYGZus{}Food
2     40   135089    Seafood
3     43   135086  Fast\PYGZus{}Food
4     44   135085  Fast\PYGZus{}Food
\end{sphinxVerbatim}

\sphinxAtStartPar
It looks that the server has found 2 types of entries that satisfy my filter, both ‘Fast\_Food’ and ‘Seafood’ were results as they both end in ‘food’, the percent sign in this case filled for ‘Fast\_’ and ‘Sea’.

\sphinxAtStartPar
A third method of filtering entries can be a range of numbers, using the BETWEEN and AND statements.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT userID, placeID, rating FROM ratings}
\PYG{l+s+sd}{    WHERE placeID BETWEEN 132000 AND 135000}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  userID  placeID  rating
0  U1077   132825       2
1  U1068   132740       0
2  U1068   132663       1
3  U1068   132732       0
4  U1068   132630       1
\end{sphinxVerbatim}

\sphinxAtStartPar
Another method would be to use the IN statement and supply a list/tuple of possible entries, in the example we filter on 2 users that placed ratings.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT userID, placeID, rating FROM ratings}
\PYG{l+s+sd}{    WHERE userID IN (\PYGZsq{}U1077\PYGZsq{}, \PYGZsq{}U1103\PYGZsq{})}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  userID  placeID  rating
0  U1077   135085       2
1  U1077   135038       2
2  U1077   132825       2
3  U1077   135060       1
4  U1103   132584       1
\end{sphinxVerbatim}

\sphinxAtStartPar
It is also possible to filter on NULL values (NaN or missing values in SQL), this way we can easily see we again forgot to do our data preparation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT * FROM user\PYGZus{}profile}
\PYG{l+s+sd}{    WHERE smoker is NULL}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   index userID   latitude   longitude smoker drink\PYGZus{}level dress\PYGZus{}preference  \PYGZbs{}
0     23  U1024  22.154021 \PYGZhy{}100.976028   None  abstemious             None   
1    121  U1122  22.169601 \PYGZhy{}100.991821   None  abstemious             None   
2    129  U1130  23.733000  \PYGZhy{}99.133000   None  abstemious             None   

  ambience transport marital\PYGZus{}status hijos  birth\PYGZus{}year interest  personality  \PYGZbs{}
0     None      None           None  None        1930     none  hard\PYGZhy{}worker   
1     None      None           None  None        1930     none  hard\PYGZhy{}worker   
2     None      None           None  None        1989     none  hard\PYGZhy{}worker   

  religion activity   color  weight budget  height  
0     none     None  yellow      40   None     1.2  
1     none     None  yellow      40   None     1.2  
2     none     None  yellow      40   None     1.2  
\end{sphinxVerbatim}

\sphinxAtStartPar
We can quickly fix this by just removing all users that have missing values for smoker, as there are only 3.
The syntax is a bit different as we are not using pandas, but the idea is the same, we just dont parse the result into pandas.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{conn} \PYG{o}{=} \PYG{n}{engine}\PYG{o}{.}\PYG{n}{connect}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{conn}\PYG{o}{.}\PYG{n}{execute}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    DELETE FROM user\PYGZus{}profile}
\PYG{l+s+sd}{    WHERE smoker is NULL}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}sqlalchemy.engine.cursor.LegacyCursorResult at 0x7fc53609e1c0\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
Before we check if they are removed, think about the impact of removing users, do you think we can just do this without consequences? what about the ratings they gave? Perhaps you could remove them too here? Is it still possible?

\sphinxAtStartPar
We do a quick check to see if the users with missing values are gone.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT * FROM user\PYGZus{}profile}
\PYG{l+s+sd}{    WHERE smoker is NULL}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Empty DataFrame
Columns: [index, userID, latitude, longitude, smoker, drink\PYGZus{}level, dress\PYGZus{}preference, ambience, transport, marital\PYGZus{}status, hijos, birth\PYGZus{}year, interest, personality, religion, activity, color, weight, budget, height]
Index: []
\end{sphinxVerbatim}

\sphinxAtStartPar
Thus far we used 2 tables, ratings and cuisine, yet always seperate.
Here we combine the information of both by joining them on a common column; the placeID.

\sphinxAtStartPar
Using the JOIN keyword together with the ON keyword we here perform an inner join.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT ratings.placeID, cuisine.Rcuisine, ratings.rating}
\PYG{l+s+sd}{    FROM ratings JOIN cuisine}
\PYG{l+s+sd}{    ON ratings.placeID == cuisine.placeID}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   placeID   Rcuisine  rating
0   135085  Fast\PYGZus{}Food       2
1   132825    Mexican       2
2   135060    Seafood       1
3   135104    Mexican       1
4   132740    Mexican       0
\end{sphinxVerbatim}

\sphinxAtStartPar
Now we can see per rating, not only which placeID is related but also the cuisine of that place.
This way we can create new views on our data without having overly complicated structures with redundant data.

\sphinxAtStartPar
Next to joining we can also aggregate data, here I created a query that counts the ratings in the ratings table, giving us the total amount of ratings.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{count\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT COUNT(rating) FROM ratings}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{count\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   COUNT(rating)
0           1161
\end{sphinxVerbatim}

\sphinxAtStartPar
The strengh of aggregation becomes useful when using the GROUP BY keyword, where we can group our data based on columns.
The next query calculates the average rating from the rating table grouped on the placeID, note when using grouping all other selected columns need to have an aggregation function in order to work.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{avg\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT placeID, AVG(rating) FROM ratings}
\PYG{l+s+sd}{    GROUP BY placeID}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{avg\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   placeID  AVG(rating)
0   132560         0.50
1   132561         0.75
2   132564         1.25
3   132572         1.00
4   132583         1.00
\end{sphinxVerbatim}

\sphinxAtStartPar
We can go further and combine joining and grouping, with this we can join the cuisine type from the cuisine table and group on that column, we then take both average and count of ratings.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cuisine\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT cuisine.Rcuisine, AVG(ratings.rating), COUNT(ratings.rating)}
\PYG{l+s+sd}{    FROM ratings JOIN cuisine}
\PYG{l+s+sd}{    ON ratings.placeID == cuisine.placeID}
\PYG{l+s+sd}{    GROUP BY cuisine.Rcuisine}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{cuisine\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            Rcuisine  AVG(ratings.rating)  COUNT(ratings.rating)
0           American             1.153846                     39
1           Armenian             1.250000                      4
2             Bakery             1.400000                      5
3                Bar             1.200000                    140
4    Bar\PYGZus{}Pub\PYGZus{}Brewery             1.305085                     59
5   Breakfast\PYGZhy{}Brunch             1.000000                      9
6            Burgers             1.032258                     31
7   Cafe\PYGZhy{}Coffee\PYGZus{}Shop             1.583333                     12
8          Cafeteria             1.205882                    102
9            Chinese             1.219512                     41
10      Contemporary             1.250000                     32
11            Family             1.571429                     14
12         Fast\PYGZus{}Food             1.164835                     91
13              Game             1.428571                      7
14     International             1.513514                     37
15           Italian             1.038462                     26
16          Japanese             1.344828                     29
17     Mediterranean             1.750000                      4
18           Mexican             1.189076                    238
19          Pizzeria             1.117647                     51
20          Regional             0.500000                      4
21           Seafood             1.241935                     62
22        Vietnamese             1.166667                      6
\end{sphinxVerbatim}

\sphinxAtStartPar
For an American cuisine we have an average rating of 1.15 and a count of 39 ratings.
Keeping track of the count makes sure we known how many ratings are behind the average score.

\sphinxAtStartPar
Let’s say we want to know the type with the highest average rating, we could use the ORDER BY keyword to order our results.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cuisine\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT cuisine.Rcuisine, AVG(ratings.rating), COUNT(ratings.rating)}
\PYG{l+s+sd}{    FROM ratings JOIN cuisine}
\PYG{l+s+sd}{    ON ratings.placeID == cuisine.placeID}
\PYG{l+s+sd}{    GROUP BY cuisine.Rcuisine}
\PYG{l+s+sd}{    ORDER BY AVG(ratings.rating) DESC}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{cuisine\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            Rcuisine  AVG(ratings.rating)  COUNT(ratings.rating)
0      Mediterranean             1.750000                      4
1   Cafe\PYGZhy{}Coffee\PYGZus{}Shop             1.583333                     12
2             Family             1.571429                     14
3      International             1.513514                     37
4               Game             1.428571                      7
5             Bakery             1.400000                      5
6           Japanese             1.344828                     29
7    Bar\PYGZus{}Pub\PYGZus{}Brewery             1.305085                     59
8       Contemporary             1.250000                     32
9           Armenian             1.250000                      4
10           Seafood             1.241935                     62
11           Chinese             1.219512                     41
12         Cafeteria             1.205882                    102
13               Bar             1.200000                    140
14           Mexican             1.189076                    238
15        Vietnamese             1.166667                      6
16         Fast\PYGZus{}Food             1.164835                     91
17          American             1.153846                     39
18          Pizzeria             1.117647                     51
19           Italian             1.038462                     26
20           Burgers             1.032258                     31
21  Breakfast\PYGZhy{}Brunch             1.000000                      9
22          Regional             0.500000                      4
\end{sphinxVerbatim}

\sphinxAtStartPar
So, mediterranean cuisine has the highest rating, yet only 4 ratings are present, not a representable amount.
What we could do is create a query that filters all the places with 5 or more ratings, we can use the HAVING keyword to filter groups whilst performing a GROUP BY operation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{place\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT placeID, COUNT(rating)}
\PYG{l+s+sd}{    FROM ratings }
\PYG{l+s+sd}{    GROUP BY ratings.placeID}
\PYG{l+s+sd}{    HAVING COUNT(rating) \PYGZgt{} 4}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{place\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   placeID  COUNT(rating)
0   132572             15
1   132584              6
2   132594              5
3   132608              6
4   132609              5
\end{sphinxVerbatim}

\sphinxAtStartPar
With this query qe only keep the places with 5 or more ratings, as we chosen 5 as an arbitrary value of statistical significance here.

\sphinxAtStartPar
As a last query I would like to combine the last 2, where we use the filter as a subquery in our query to find the average of each cuisine type.
This means that we take the average of each cuisine type, but only take into account places with 5 or more reviews.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cuisine\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}sql}\PYG{p}{(}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    SELECT cuisine.Rcuisine, AVG(ratings.rating), COUNT(ratings.rating)}
\PYG{l+s+sd}{    FROM ratings JOIN cuisine}
\PYG{l+s+sd}{    ON ratings.placeID == cuisine.placeID}
\PYG{l+s+sd}{    WHERE ratings.placeID in (}
\PYG{l+s+sd}{        SELECT placeID}
\PYG{l+s+sd}{        FROM ratings }
\PYG{l+s+sd}{        GROUP BY ratings.placeID}
\PYG{l+s+sd}{        HAVING COUNT(rating) \PYGZgt{} 4}
\PYG{l+s+sd}{    )}
\PYG{l+s+sd}{    GROUP BY cuisine.Rcuisine}
\PYG{l+s+sd}{    ORDER BY AVG(ratings.rating) DESC}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{engine}
\PYG{p}{)}
\PYG{n}{cuisine\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            Rcuisine  AVG(ratings.rating)  COUNT(ratings.rating)
0             Family             1.600000                     10
1   Cafe\PYGZhy{}Coffee\PYGZus{}Shop             1.583333                     12
2      International             1.513514                     37
3               Game             1.428571                      7
4           Japanese             1.423077                     26
5             Bakery             1.400000                      5
6    Bar\PYGZus{}Pub\PYGZus{}Brewery             1.290909                     55
7            Seafood             1.241935                     62
8            Chinese             1.216216                     37
9          Cafeteria             1.205882                    102
10               Bar             1.181818                    132
11           Mexican             1.181395                    215
12      Contemporary             1.178571                     28
13          American             1.171429                     35
14        Vietnamese             1.166667                      6
15         Fast\PYGZus{}Food             1.159091                     88
16          Pizzeria             1.117647                     51
17  Breakfast\PYGZhy{}Brunch             1.000000                      9
18           Burgers             0.925926                     27
19           Italian             0.857143                     14
\end{sphinxVerbatim}

\sphinxAtStartPar
You can see that Mediterranean now is missing as it only had 4 ratings, yet the Family cuisine still has 10 out of 12 reviews and it’s average even increased.

\sphinxAtStartPar
Although we used SQL which can only perform simple mathematics we were able to manipulate our dataset before even going into the data exploration phase.
When dealing with larger datasets using SQL can drastically improve your data analytical experience and is therefore an essential tool for a data scientist

\sphinxAtStartPar
I’ll leave a blank cell here for you to experiment, for more inspiration you could also check out this \sphinxhref{https://learnsql.com/blog/sql-basics-cheat-sheet/sql-basics-cheat-sheet-a4.pdf}{cheat sheet}


\part{4. Data Visualisation}


\chapter{Introduction}
\label{\detokenize{c4_data_visualisation/introduction:introduction}}\label{\detokenize{c4_data_visualisation/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\chapter{Line plot}
\label{\detokenize{c4_data_visualisation/line:line-plot}}\label{\detokenize{c4_data_visualisation/line::doc}}
\sphinxAtStartPar
The most straight\sphinxhyphen{}forward yet very useful plotting graph is the line plot.
With the line plot we achieve the visualisation of a single feature organized in a usually time based reference.

\sphinxAtStartPar
The line plot is ideal if you want to achieve a time critical pattern residing within your data.
In this example we use the prepared taxi dataframe that comes with our plotting library seaborn.

\sphinxAtStartPar
From all possible plotting libraries in Python we opted for the seaborn as it has an optimal combination of simplicity and beaty, yet other libraries are equally powerful.

\sphinxAtStartPar
We begin by importing our neccesary libraries

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set\PYGZus{}theme}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
For aestetic reasons we change the figure size to something a bit larger

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{rc}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{figure.figsize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
We load our dataset, this dataset contains the trip of taxi’s in regions of New York City with timestamps of pickup and dropoff.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{taxi\PYGZus{}df} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{taxis}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{taxi\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                pickup              dropoff  passengers  distance  fare   tip  \PYGZbs{}
0  2019\PYGZhy{}03\PYGZhy{}23 20:21:09  2019\PYGZhy{}03\PYGZhy{}23 20:27:24           1      1.60   7.0  2.15   
1  2019\PYGZhy{}03\PYGZhy{}04 16:11:55  2019\PYGZhy{}03\PYGZhy{}04 16:19:00           1      0.79   5.0  0.00   
2  2019\PYGZhy{}03\PYGZhy{}27 17:53:01  2019\PYGZhy{}03\PYGZhy{}27 18:00:25           1      1.37   7.5  2.36   
3  2019\PYGZhy{}03\PYGZhy{}10 01:23:59  2019\PYGZhy{}03\PYGZhy{}10 01:49:51           1      7.70  27.0  6.15   
4  2019\PYGZhy{}03\PYGZhy{}30 13:27:42  2019\PYGZhy{}03\PYGZhy{}30 13:37:14           3      2.16   9.0  1.10   

   tolls  total   color      payment            pickup\PYGZus{}zone  \PYGZbs{}
0    0.0  12.95  yellow  credit card        Lenox Hill West   
1    0.0   9.30  yellow         cash  Upper West Side South   
2    0.0  14.16  yellow  credit card          Alphabet City   
3    0.0  36.95  yellow  credit card              Hudson Sq   
4    0.0  13.40  yellow  credit card           Midtown East   

            dropoff\PYGZus{}zone pickup\PYGZus{}borough dropoff\PYGZus{}borough  
0    UN/Turtle Bay South      Manhattan       Manhattan  
1  Upper West Side South      Manhattan       Manhattan  
2           West Village      Manhattan       Manhattan  
3         Yorkville West      Manhattan       Manhattan  
4         Yorkville West      Manhattan       Manhattan  
\end{sphinxVerbatim}

\sphinxAtStartPar
As we saw earlier, it is important to prepare the data, due to storage specification they did not parse the dates into a datetime format, which we do here.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{taxi\PYGZus{}df}\PYG{o}{.}\PYG{n}{pickup} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{n}{taxi\PYGZus{}df}\PYG{o}{.}\PYG{n}{pickup}\PYG{p}{)}
\PYG{n}{taxi\PYGZus{}df}\PYG{o}{.}\PYG{n}{dropoff} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{n}{taxi\PYGZus{}df}\PYG{o}{.}\PYG{n}{dropoff}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Before we can do anything with this dataset, we need to format it into a proper format, for our first graph I would like to view the total amount of passengers per day.
This means we have to take our data and resample on the pickup date, taking the sum.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pass\PYGZus{}df} \PYG{o}{=} \PYG{n}{taxi\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pickup}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{resample}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            passengers  distance     fare     tip  tolls    total
pickup                                                           
2019\PYGZhy{}02\PYGZhy{}28           1      0.90     5.00    0.00   0.00     6.30
2019\PYGZhy{}03\PYGZhy{}01         370    640.29  2946.97  442.47  60.34  4213.83
2019\PYGZhy{}03\PYGZhy{}02         310    548.70  2358.00  333.97  28.80  3319.02
2019\PYGZhy{}03\PYGZhy{}03         264    554.04  2187.89  307.47  34.56  3027.32
2019\PYGZhy{}03\PYGZhy{}04         267    583.81  2335.74  334.98  63.36  3269.08
\end{sphinxVerbatim}

\sphinxAtStartPar
You can almost see the plot here, we have an index of dates and a feature ‘passengers’, these two will make the backbone of our visualisation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{passengers}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}pickup\PYGZsq{}, ylabel=\PYGZsq{}passengers\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{line_11_1}.png}

\sphinxAtStartPar
Looks about right, however I don’t like the start of it, the data started late on that first day, resampling shows we only have 1 passenger for that day.
This is not representable, so we remove that record.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pass\PYGZus{}df} \PYG{o}{=} \PYG{n}{pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2019\PYGZhy{}03\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{]}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{passengers}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{line_13_0}.png}

\sphinxAtStartPar
Much better, however the plot feels like there is a lot of fluctuations, so it would be practical to apply a rolling sum or mean.
This rolling operation takes the last x values and applies an operation (sum, mean,…) to it, creating a smoother graph and is visually more sensitive to trends.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rolling\PYGZus{}pass\PYGZus{}df} \PYG{o}{=} \PYG{n}{pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{rolling}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{rolling\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{rolling\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{passengers}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{line_15_0}.png}

\sphinxAtStartPar
By applying a rolling mean, we can see that the average amount of passengers per day is decreasing.
I feel there is no need to panick, as this is only 1 month of data and seasonal fluxtuations do happen.

\sphinxAtStartPar
Something else that triggers my curiosity is the amount these passengers paid, can we perhaps see a trend there?
It would be ideal to plot these together so the comparison is simple.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{rolling\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{rolling\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{passengers}\PYG{p}{)}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{rolling\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{rolling\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{line_17_0}.png}

\sphinxAtStartPar
As we only have a few passengers per trip, yet trips can be costly the ranges of these 2 features are completely different.
Before we think about scaling, we actually do want to know the scale here, we just cant fit them in the same graph.

\sphinxAtStartPar
A first approach would be to use a secondary axis, where the right side of the y\sphinxhyphen{}axis is used to show the fare scale.
You can see that the graph is already getting more complicated code\sphinxhyphen{}wise, this is where using the right library is key as they usually have built in features for that.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{rolling\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{rolling\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{passengers}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{passengers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{ax2} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{twinx}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{rolling\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{rolling\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax2}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fare}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{legend}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{figure}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.legend.Legend at 0x7fc9f08c1fd0\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{line_19_1}.png}

\sphinxAtStartPar
Interesting! It shows that there was a period where they did not follow eachother perfect, yet the trend is almost exact for these features.

\sphinxAtStartPar
Another method where you can compare them would require feature engineering, where we calculate the fare per passenger per day, apply the rolling window and plot.
Perhaps you could figure that out? create a new feature that divides the fare by the passengers, recreate the rolling dataframe and use seaborn to plot the results.

\sphinxAtStartPar
At the start we used the sum of passengers per day, however we could also visualise the average amount of passengers per ride.
The reason why I would like to do this is because earlier I saw a difference in trend for the fare and the amount of passengers, an explanation for this could be that the average amount of passengers dropped, resulting in lower passengers, yet the total expenditure of fares would remain constant.

\sphinxAtStartPar
Let us figure this out, we here calculate the average (mean) of the passengers per day.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{avg\PYGZus{}pass\PYGZus{}df} \PYG{o}{=} \PYG{n}{taxi\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pickup}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{resample}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{avg\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            passengers  distance       fare       tip     tolls      total
pickup                                                                    
2019\PYGZhy{}02\PYGZhy{}28    1.000000  0.900000   5.000000  0.000000  0.000000   6.300000
2019\PYGZhy{}03\PYGZhy{}01    1.535270  2.656805  12.228091  1.835975  0.250373  17.484772
2019\PYGZhy{}03\PYGZhy{}02    1.565657  2.771212  11.909091  1.686717  0.145455  16.762727
2019\PYGZhy{}03\PYGZhy{}03    1.562130  3.278343  12.946095  1.819349  0.204497  17.913136
2019\PYGZhy{}03\PYGZhy{}04    1.561404  3.414094  13.659298  1.958947  0.370526  19.117427
\end{sphinxVerbatim}

\sphinxAtStartPar
Doing more or less exactly the same we can create a simple plot with the average amount of passengers in a taxi.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{avg\PYGZus{}pass\PYGZus{}df} \PYG{o}{=} \PYG{n}{avg\PYGZus{}pass\PYGZus{}df}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{avg\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{avg\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{passengers}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{line_25_0}.png}

\sphinxAtStartPar
For the same reasons, this plot is not suitable as it has too much variance.
We apply a rolling mean of 7 days and re\sphinxhyphen{}evaluate.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rolling\PYGZus{}avg\PYGZus{}pass\PYGZus{}df} \PYG{o}{=} \PYG{n}{avg\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{rolling}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{rolling\PYGZus{}avg\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{rolling\PYGZus{}avg\PYGZus{}pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{passengers}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{line_27_0}.png}

\sphinxAtStartPar
We find a dip in passengers per ride that looks to be in the same time interval, therefore we could conclude here that fares did not get more expensive, rather the sharing of cabs was less.
You could try and find a method to add the data of these two graphs together, yet this is already advanced visualisation.

\sphinxAtStartPar
Another question that I have for you, do you think that the dip is relevant? Not specifically from a business point of view, rather from a statistical view, Perhaps if you look at the range of the y\sphinxhyphen{}axis you might feel that our plot is a bit magnified. This is a good example of how you can use ranges of your axi to make data more dramatic. Be weary of these malpractices!

\sphinxAtStartPar
We are not done yet, as our dataset contains much more information.
Harnessing the powers of the preprocessing we learned, we could include other (mostly categorical) feature into our line plot.

\sphinxAtStartPar
Here we take the payment option (either cash or card) and use it to create 2 time series in long format (2 datasets below each other).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pass\PYGZus{}payment\PYGZus{}df} \PYG{o}{=} \PYG{n}{taxi\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{payment}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}
    \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pickup}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{resample}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{p}{)}
\PYG{n}{pass\PYGZus{}payment\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                        passengers  distance     fare     tip  tolls    total
payment     pickup                                                           
cash        2019\PYGZhy{}02\PYGZhy{}28           1      0.90     5.00    0.00   0.00     6.30
            2019\PYGZhy{}03\PYGZhy{}01         104    112.31   571.50    0.00   5.76   748.76
            2019\PYGZhy{}03\PYGZhy{}02          86    159.46   690.50    0.00   5.76   863.96
            2019\PYGZhy{}03\PYGZhy{}03          67    172.34   641.50    0.00  17.28   782.18
            2019\PYGZhy{}03\PYGZhy{}04          71    130.60   571.50    0.00   0.00   710.95
...                            ...       ...      ...     ...    ...      ...
credit card 2019\PYGZhy{}03\PYGZhy{}27         263    532.61  2260.64  485.63  69.12  3342.29
            2019\PYGZhy{}03\PYGZhy{}28         227    403.41  1886.07  404.45  40.32  2802.94
            2019\PYGZhy{}03\PYGZhy{}29         211    404.61  1831.98  410.13  23.04  2747.85
            2019\PYGZhy{}03\PYGZhy{}30         268    540.71  2211.10  487.97  78.62  3249.49
            2019\PYGZhy{}03\PYGZhy{}31         202    376.78  1632.93  345.83  29.16  2408.42

[63 rows x 6 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Seaborn does not like this long format type, therefore we unstack the first index and create a wide format.
For those wo are punctilious, you can notice we created a missing value, with wat should we fill it? (Our luck that seaborn can handle missing values!)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pass\PYGZus{}payment\PYGZus{}df}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           passengers             distance               fare              \PYGZbs{}
payment          cash credit card     cash credit card   cash credit card   
pickup                                                                      
2019\PYGZhy{}02\PYGZhy{}28        1.0         NaN     0.90         NaN    5.0         NaN   
2019\PYGZhy{}03\PYGZhy{}01      104.0       264.0   112.31      527.08  571.5     2363.97   
2019\PYGZhy{}03\PYGZhy{}02       86.0       222.0   159.46      377.74  690.5     1651.50   
2019\PYGZhy{}03\PYGZhy{}03       67.0       196.0   172.34      381.60  641.5     1526.39   
2019\PYGZhy{}03\PYGZhy{}04       71.0       196.0   130.60      453.21  571.5     1764.24   

            tip              tolls               total              
payment    cash credit card   cash credit card    cash credit card  
pickup                                                              
2019\PYGZhy{}02\PYGZhy{}28  0.0         NaN   0.00         NaN    6.30         NaN  
2019\PYGZhy{}03\PYGZhy{}01  0.0      442.47   5.76       54.58  748.76     3446.47  
2019\PYGZhy{}03\PYGZhy{}02  0.0      333.97   5.76       23.04  863.96     2430.36  
2019\PYGZhy{}03\PYGZhy{}03  0.0      307.47  17.28       17.28  782.18     2224.34  
2019\PYGZhy{}03\PYGZhy{}04  0.0      334.98   0.00       63.36  710.95     2558.13  
\end{sphinxVerbatim}

\sphinxAtStartPar
Same data, different structure, now seaborn understands the format and we can go back to visualisation.

\sphinxAtStartPar
For simplicity we start with a simple passengers line plot

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{pass\PYGZus{}payment\PYGZus{}df}\PYG{o}{.}\PYG{n}{passengers}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{line_35_0}.png}

\sphinxAtStartPar
You can see that there are generally more people paying by card, which is more convenient in such an occasion.
Note that here we should not use a seperate y\sphinxhyphen{}axis as we are comparing 2 sets of data that are similar by origin.

\sphinxAtStartPar
We do the same for fares.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{pass\PYGZus{}payment\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{line_37_0}.png}

\sphinxAtStartPar
This is more or less a no\sphinxhyphen{}brainer, as more people pay by card, the fares by card are also more.
So we can’t really compare fares with this plot, we have to be creative.

\sphinxAtStartPar
I opted to go for an average fare per passenger, as this is in my opinion more relevant than the amount of rides

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pass\PYGZus{}payment\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fare\PYGZus{}pass}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{pass\PYGZus{}payment\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{/}\PYG{n}{pass\PYGZus{}payment\PYGZus{}df}\PYG{o}{.}\PYG{n}{passengers}
\PYG{n}{pass\PYGZus{}payment\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                    passengers  distance   fare  tip  tolls   total  fare\PYGZus{}pass
payment pickup                                                                
cash    2019\PYGZhy{}02\PYGZhy{}28           1      0.90    5.0  0.0   0.00    6.30   5.000000
        2019\PYGZhy{}03\PYGZhy{}01         104    112.31  571.5  0.0   5.76  748.76   5.495192
        2019\PYGZhy{}03\PYGZhy{}02          86    159.46  690.5  0.0   5.76  863.96   8.029070
        2019\PYGZhy{}03\PYGZhy{}03          67    172.34  641.5  0.0  17.28  782.18   9.574627
        2019\PYGZhy{}03\PYGZhy{}04          71    130.60  571.5  0.0   0.00  710.95   8.049296
\end{sphinxVerbatim}

\sphinxAtStartPar
We created a new feature both containing info of fares and passengers, using this we create a new visualisations.

\sphinxAtStartPar
In this visualisation we show for both payment options the average fare amount per passenger in the cab.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{pass\PYGZus{}payment\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare\PYGZus{}pass}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{rolling}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{,} \PYG{n}{min\PYGZus{}periods}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{line_41_0}.png}

\sphinxAtStartPar
We can conclude that the average amount that has to be paid per person is lower for cash, indicating that people jump to their debit card as soon as the amount gets too high.

\sphinxAtStartPar
As a last I would like to emphasise that the x\sphinxhyphen{}axis, being time does not have to be linear.
To illustrate this we create a weekly passenger rate and impose each week over the others.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pass\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{Grouper}\PYG{p}{(}\PYG{n}{freq}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}
    \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{x}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{day\PYGZus{}name}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{x}\PYG{o}{.}\PYG{n}{passengers}\PYG{p}{)}
\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pickup
2019\PYGZhy{}03\PYGZhy{}03    AxesSubplot(0.125,0.125;0.775x0.755)
2019\PYGZhy{}03\PYGZhy{}10    AxesSubplot(0.125,0.125;0.775x0.755)
2019\PYGZhy{}03\PYGZhy{}17    AxesSubplot(0.125,0.125;0.775x0.755)
2019\PYGZhy{}03\PYGZhy{}24    AxesSubplot(0.125,0.125;0.775x0.755)
2019\PYGZhy{}03\PYGZhy{}31    AxesSubplot(0.125,0.125;0.775x0.755)
Freq: W\PYGZhy{}SUN, dtype: object
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{line_43_1}.png}

\sphinxAtStartPar
Here we can see there is a weekly trend occuring, where Sundays and Mondays are usually less busy days.
The origin of this is hard to argue, as it might be less traffic, less taxi drivers working,…

\sphinxAtStartPar
Perhaps you could complete this visualisation by investigating the distance and/or tips?


\chapter{Histogram plot}
\label{\detokenize{c4_data_visualisation/histogram:histogram-plot}}\label{\detokenize{c4_data_visualisation/histogram::doc}}
\sphinxAtStartPar
When visualising one dimensional data without relating it to other information an option would be histograms.
Histograms are used when describing distributions in your data, it is not the values itself you are visualising, rather the counts/frequencies of each value.

\sphinxAtStartPar
We again start with importing our libraries

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set\PYGZus{}theme}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{rc}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{figure.figsize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
For this example we will be using the prepared dataset from seaborn containing mileages of several cars.
Information about the cars is also given.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mpg\PYGZus{}df} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{mpg\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    mpg  cylinders  displacement  horsepower  weight  acceleration  \PYGZbs{}
0  18.0          8         307.0       130.0    3504          12.0   
1  15.0          8         350.0       165.0    3693          11.5   
2  18.0          8         318.0       150.0    3436          11.0   
3  16.0          8         304.0       150.0    3433          12.0   
4  17.0          8         302.0       140.0    3449          10.5   

   model\PYGZus{}year origin                       name  
0          70    usa  chevrolet chevelle malibu  
1          70    usa          buick skylark 320  
2          70    usa         plymouth satellite  
3          70    usa              amc rebel sst  
4          70    usa                ford torino  
\end{sphinxVerbatim}

\sphinxAtStartPar
We start of simple by plotting the distribution of horsepower in our dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{histplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{mpg\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horsepower}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}horsepower\PYGZsq{}, ylabel=\PYGZsq{}Count\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{histogram_5_1}.png}

\sphinxAtStartPar
A first thing that is visible is that our feature is not normally distributed, we have a long tail to the higer end.

\sphinxAtStartPar
For histograms we can specify the amount of bins in which we seperate the counts, seaborn selects a suitable number yet we can change this.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{histplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{mpg\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horsepower}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}horsepower\PYGZsq{}, ylabel=\PYGZsq{}Count\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{histogram_7_1}.png}

\sphinxAtStartPar
As you can see, the previous option looks a lot better.
Taking the right amount of bins is important.

\sphinxAtStartPar
In order to add more information to our plot, we can use categorical data to split our data into multiple histograms.
Here we used the origin of the cars to split into 3 categories, notice how each of them has their own area, japan and europe are on the lower end whilst usa is centered in higher horsepower.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{histplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{mpg\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horsepower}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{origin}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{multiple}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{stack}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}horsepower\PYGZsq{}, ylabel=\PYGZsq{}Count\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{histogram_9_1}.png}

\sphinxAtStartPar
A neat feature of seaborn is that it can join histograms and scatter plots (in the next section) together.

\sphinxAtStartPar
Here we see how the visualisations of 2 one dimensional histograms perfectly combine together into a scatter plot, where 2 dimensional data is shown (both mileage and horsepower).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{jointplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{mpg\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horsepower}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}seaborn.axisgrid.JointGrid at 0x7ff9e42c8fa0\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{histogram_11_1}.png}

\sphinxAtStartPar
Histograms are a really powerfull tool when it comes to validating your data, we can easily the distribution of each feature, see if they are normally distributed and visualise distributions of subgroups.

\sphinxAtStartPar
Yet for final visualisations they are often not interesting enough.


\chapter{Box plot}
\label{\detokenize{c4_data_visualisation/box:box-plot}}\label{\detokenize{c4_data_visualisation/box::doc}}
\sphinxAtStartPar
In the previous section we looked into visualising the distributions of 1 dimensional data.
We used histograms for this, but there is a second more statistical option for this, the Boxplot.

\sphinxAtStartPar
To be brief, the boxplot shows a box containing the InterQuartile Data that we already talked about and also has 2 whiskers, showing the threshold for outliers.
Actual outliers are then printed seperately, making this plot ideal for outlier detection aswel as distributions.

\sphinxAtStartPar
I personally think this option is more suited for multiple categories compared to histograms, yet your mileage may vary.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set\PYGZus{}theme}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{rc}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{figure.figsize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
For this section we will look into the discovery of extrasolar planets, or planets that are ourside our own solar system.
For each planet they listed the method of discovery, orbital period, mass, distance and year of discovery.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{planet\PYGZus{}df} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{planets}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{planet\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            method  number  orbital\PYGZus{}period   mass  distance  year
0  Radial Velocity       1         269.300   7.10     77.40  2006
1  Radial Velocity       1         874.774   2.21     56.95  2008
2  Radial Velocity       1         763.000   2.60     19.84  2011
3  Radial Velocity       1         326.030  19.40    110.62  2007
4  Radial Velocity       1         516.220  10.50    119.47  2009
\end{sphinxVerbatim}

\sphinxAtStartPar
Let’s say we would like to show the distances of each discovery method, if we would use a bar plot, the results might be hard to interpret.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{barplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{planet\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{distance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{method}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{xscale}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[None]
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{box_5_1}.png}

\sphinxAtStartPar
Whilst bar plots can be a good idea, here they are not.

\sphinxAtStartPar
Only use bar plots when visualising singular data points who are related to zero, not aggregations of multiple data points.
Bar plots do not work if:
\begin{itemize}
\item {} 
\sphinxAtStartPar
your datapoints have no relation to zero

\item {} 
\sphinxAtStartPar
your categories are related with different intervals

\item {} 
\sphinxAtStartPar
you are dealing with groups of datapoints, not single datapoints (this case)

\end{itemize}

\sphinxAtStartPar
anyway, we could use a histogram similar to previous section, let’s see how that turns out.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{histplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{planet\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{distance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{method}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{multiple}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{stack}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{log\PYGZus{}scale}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{box_7_0}.png}

\sphinxAtStartPar
The histogram seems to be working, yet the methods with lower count are suppressed.
A boxplot can overcome this and we can also compare medians of each method with eachother.

\sphinxAtStartPar
Take a few minutes to understand the next plot, at first it is very confusing, yet when adapted this is the most powerful visualisation of data exploration.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{planet\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{distance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{method}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{xscale}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[None]
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{box_9_1}.png}

\sphinxAtStartPar
Can you see now why the bar plot here is a bad idea?
Some methods have a broader distribution and relating our data to zero makes no real sense.
With financial data this is different as budgets always start with 0.

\sphinxAtStartPar
Here we can conclude that some methods of detecting a planet requires a further or closer distance.
You could say that if you want to discover a far extrasolar plant pick one of the last methods

\sphinxAtStartPar
An addition to the boxplot, where we focus more on distribution instead of statistics, would be the violin plot.
Can you see why they would call it like that?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{violinplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{planet\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{method}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}year\PYGZsq{}, ylabel=\PYGZsq{}method\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{box_11_1}.png}

\sphinxAtStartPar
As an exercise calculate the median, Q1 and Q3 of the distance per method and see if you come to the same conclusion as the boxplot


\chapter{Scatter plot}
\label{\detokenize{c4_data_visualisation/scatter:scatter-plot}}\label{\detokenize{c4_data_visualisation/scatter::doc}}
\sphinxAtStartPar
Thus far we dealt with one dimensional data in our visualisations, sometimes adding a category to divide our data.
Here we take it a step further, scatter plots are to visualise the relation between 2 numerical features.

\sphinxAtStartPar
One remark that I would like to make here is that discrete numerical features (age, n\_persons,…) are possible to use, yet when dealing with a small range (e.g. 0\sphinxhyphen{}10) the results are skewed.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set\PYGZus{}theme}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{rc}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{figure.figsize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
For scatter plots I opted to use a dataset containing tips from a restaurant, the tips are divided in gender, smoker, time of day and day of week.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tips\PYGZus{}df} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tips}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{tips\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   total\PYGZus{}bill   tip     sex smoker  day    time  size
0       16.99  1.01  Female     No  Sun  Dinner     2
1       10.34  1.66    Male     No  Sun  Dinner     3
2       21.01  3.50    Male     No  Sun  Dinner     3
3       23.68  3.31    Male     No  Sun  Dinner     2
4       24.59  3.61  Female     No  Sun  Dinner     4
\end{sphinxVerbatim}

\sphinxAtStartPar
The most simple scatter plot we can make would be showing the relation between the total bill and the tip, we would assume the tip is proportional to the size of the bill.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{tips\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{total\PYGZus{}bill}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tip}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{scatter_5_0}.png}

\sphinxAtStartPar
Just as expected, when the total bill rises, the tip grows too, we have some generous persons, and some less generous, but nothing out of the ordinary.

\sphinxAtStartPar
To get a better idea of the tipping habits we could calculate the tip per person in the bill, which is noted by size.
We divide the tip by the amount of people and plot again.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tips\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tip\PYGZus{}per\PYGZus{}person}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{tips\PYGZus{}df}\PYG{o}{.}\PYG{n}{tip}\PYG{o}{/}\PYG{n}{tips\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{tips\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{total\PYGZus{}bill}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tip\PYGZus{}per\PYGZus{}person}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{scatter_7_0}.png}

\sphinxAtStartPar
It is much harder to see a relation now, so we could argue that depending of the service everyone gives a specific amount.
So it is not the size of the bill that is defining the tip, rather the amount of persons (although this is very similar) in the bill.

\sphinxAtStartPar
Aside from feature engineering, we can also add categorical features, using different colors for each feature.
Here we added if they smoked or not.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{tips\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{total\PYGZus{}bill}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tip}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoker}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{scatter_9_0}.png}

\sphinxAtStartPar
It is hard to see if smoking had an effect on either the bill or the tip, which indicates that your plot is not that useful.
This is not true if you wanted to prove that there is no effect of smoking obviously!

\sphinxAtStartPar
We can also add a numerical feature into the scatter plot, by using sizes of our dots in the scatter plot.
The size of the group now influences the size of our dots.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{tips\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{total\PYGZus{}bill}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tip}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{size}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{scatter_11_0}.png}

\sphinxAtStartPar
Whilst it might not be really visible because of the linear nature of the size \sphinxhyphen{} it is only going from 1 to 6 \sphinxhyphen{} the relation is not obvious.
Perhaps you could do some feature engineering where you artificially increase the size by taking the square?
You could argue if that is still representable, but for the sake of the exercise let’s say it is.

\sphinxAtStartPar
In the beginning I talked about numerical features with a low range, the size of our group is one of them.
See what happens when i would use it in a scatter plot.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{tips\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{size}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tip}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{scatter_15_0}.png}

\sphinxAtStartPar
It clearly shows that a higher size means statistical higher tips, up to a cut\sphinxhyphen{}off of 5 appearantly.
Yet do you feel this is an aesthetically satisfying plot?

\sphinxAtStartPar
Not going to much in the field of machine learning, seaborn has an interesting feature built\sphinxhyphen{}in.
They offer a regression plot, where a linear regression is draw with a confidence interval (the light blue area).
Not wanting to give mathematical number it shows what it thinks is the relation between the 2 variables.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{regplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{tips\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{total\PYGZus{}bill}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tip}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{scatter_17_0}.png}

\sphinxAtStartPar
It seems to be very confident about the relation, how about where we corrected for group size?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{regplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{tips\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{total\PYGZus{}bill}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tip\PYGZus{}per\PYGZus{}person}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{scatter_19_0}.png}

\sphinxAtStartPar
Less confident, less appearent.
Keep in mind that it will always see a relations, the question is how confident!


\chapter{Heatmap plot}
\label{\detokenize{c4_data_visualisation/heatmap:heatmap-plot}}\label{\detokenize{c4_data_visualisation/heatmap::doc}}
\sphinxAtStartPar
A heatmap also deals with 2 dimensional data and cares about the relation.
Here instead of numerical data with dots, we are using categorical data where every combination of the 2 categories has a singular value.

\sphinxAtStartPar
This results into a matrix that we visualize where each index of the matrix has its own color based on a color gradient.
This plot got its name as it is used to find ‘hot spots’ between combinations of 2 categorical features.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set\PYGZus{}theme}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{rc}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{figure.figsize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
To make optimal use of this plot, we are going to take on a rather complex dataset, where we have measurements of brain networks.
The idea is that we have several networks with several nodes in 2 hemispheres, the content of the data is not as important here, what matters is that we want to find correlations between different nodes in the brain.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{brain\PYGZus{}df} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{brain\PYGZus{}networks}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{header}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{index\PYGZus{}col}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{brain\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
network          1                     2                     3             \PYGZbs{}
node             1                     1                     1              
hemi            lh         rh         lh         rh         lh         rh   
0        56.055744  92.031036   3.391576  38.659683  26.203819 \PYGZhy{}49.715569   
1        55.547253  43.690075 \PYGZhy{}65.495987 \PYGZhy{}13.974523 \PYGZhy{}28.274963 \PYGZhy{}39.050129   
2        60.997768  63.438793 \PYGZhy{}51.108582 \PYGZhy{}13.561346 \PYGZhy{}18.842947  \PYGZhy{}1.214659   
3        18.514868  12.657158 \PYGZhy{}34.576603 \PYGZhy{}32.665958  \PYGZhy{}7.420454  17.119448   
4        \PYGZhy{}2.527392 \PYGZhy{}63.104668 \PYGZhy{}13.814151 \PYGZhy{}15.837989 \PYGZhy{}45.216927   3.483550   

network          4                     5             ...         16  \PYGZbs{}
node             1                     1             ...          3   
hemi            lh         rh         lh         rh  ...         rh   
0        47.461037  26.746613 \PYGZhy{}35.898861  \PYGZhy{}1.889181  ...   0.607904   
1        \PYGZhy{}1.210660 \PYGZhy{}19.012897  19.568010  15.902983  ...  57.495071   
2       \PYGZhy{}65.575806 \PYGZhy{}85.777428  19.247454  37.209419  ...  28.317369   
3       \PYGZhy{}41.800869 \PYGZhy{}58.610184  32.896915  11.199619  ...  71.439629   
4       \PYGZhy{}62.613335 \PYGZhy{}49.076508  18.396759   3.219077  ...  95.597565   

network                                17                                   \PYGZbs{}
node             4                      1                     2              
hemi            lh          rh         lh         rh         lh         rh   
0       \PYGZhy{}70.270546   77.365776 \PYGZhy{}21.734550   1.028253   7.791784  68.903725   
1       \PYGZhy{}76.393219  127.261360 \PYGZhy{}13.035799  46.381824 \PYGZhy{}15.752450  31.000332   
2         9.063977   45.493263  26.033442  34.212200   1.326110 \PYGZhy{}22.580757   
3        65.842979  \PYGZhy{}10.697547  55.297466   4.255006  \PYGZhy{}2.420144  12.098393   
4        50.960453  \PYGZhy{}23.197300  43.067562  52.219875  28.232882 \PYGZhy{}11.719750   

network                                    
node             3                      4  
hemi            lh          rh         lh  
0       \PYGZhy{}10.520872  120.490463 \PYGZhy{}39.686432  
1       \PYGZhy{}39.607521   24.764011 \PYGZhy{}36.771008  
2        12.985169  \PYGZhy{}75.027451   6.434262  
3       \PYGZhy{}15.819172  \PYGZhy{}37.361431  \PYGZhy{}4.650954  
4         5.453649    5.169828  87.809135  

[5 rows x 62 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
luckily for us, the pandas library has an easy method of finding out what the correlation is between different columns of numerical data.
These correlations are denoted between \sphinxhyphen{}1 (completely opposite) to 1 (completely related).
Take a minute to understand how the columns and index changed using the operation, you can see that a node in a network and hemisphere has a correlation of 1.00 with itself.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{brain\PYGZus{}df}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
network                   1                   2                   3            \PYGZbs{}
node                      1                   1                   1             
hemi                     lh        rh        lh        rh        lh        rh   
network node hemi                                                               
1       1    lh    1.000000  0.881516 \PYGZhy{}0.042699 \PYGZhy{}0.074437 \PYGZhy{}0.342849 \PYGZhy{}0.169498   
             rh    0.881516  1.000000  0.013073  0.033733 \PYGZhy{}0.351509 \PYGZhy{}0.162006   
2       1    lh   \PYGZhy{}0.042699  0.013073  1.000000  0.813394 \PYGZhy{}0.006940 \PYGZhy{}0.039375   
             rh   \PYGZhy{}0.074437  0.033733  0.813394  1.000000 \PYGZhy{}0.027324 \PYGZhy{}0.023608   
3       1    lh   \PYGZhy{}0.342849 \PYGZhy{}0.351509 \PYGZhy{}0.006940 \PYGZhy{}0.027324  1.000000  0.553183   
...                     ...       ...       ...       ...       ...       ...   
17      2    lh   \PYGZhy{}0.206379 \PYGZhy{}0.273370 \PYGZhy{}0.151724 \PYGZhy{}0.224447  0.026579 \PYGZhy{}0.056687   
             rh   \PYGZhy{}0.212601 \PYGZhy{}0.266456 \PYGZhy{}0.124508 \PYGZhy{}0.172704 \PYGZhy{}0.089109 \PYGZhy{}0.144020   
        3    lh   \PYGZhy{}0.142770 \PYGZhy{}0.174222 \PYGZhy{}0.179912 \PYGZhy{}0.250455 \PYGZhy{}0.012675 \PYGZhy{}0.047434   
             rh   \PYGZhy{}0.204326 \PYGZhy{}0.223572 \PYGZhy{}0.044706 \PYGZhy{}0.090798 \PYGZhy{}0.024644 \PYGZhy{}0.103875   
        4    lh   \PYGZhy{}0.219283 \PYGZhy{}0.273626 \PYGZhy{}0.209557 \PYGZhy{}0.216674  0.013747 \PYGZhy{}0.058838   

network                   4                   5            ...        16  \PYGZbs{}
node                      1                   1            ...         3   
hemi                     lh        rh        lh        rh  ...        rh   
network node hemi                                          ...             
1       1    lh   \PYGZhy{}0.373050 \PYGZhy{}0.361726  0.431619  0.418708  ... \PYGZhy{}0.106642   
             rh   \PYGZhy{}0.333244 \PYGZhy{}0.337476  0.431953  0.519916  ... \PYGZhy{}0.173530   
2       1    lh   \PYGZhy{}0.019773  0.007099 \PYGZhy{}0.147374 \PYGZhy{}0.104164  ... \PYGZhy{}0.215429   
             rh   \PYGZhy{}0.017577 \PYGZhy{}0.014632 \PYGZhy{}0.173501 \PYGZhy{}0.094717  ... \PYGZhy{}0.184458   
3       1    lh    0.528787  0.503403 \PYGZhy{}0.157154 \PYGZhy{}0.185008  ... \PYGZhy{}0.146451   
...                     ...       ...       ...       ...  ...       ...   
17      2    lh    0.020064  0.084837 \PYGZhy{}0.359879 \PYGZhy{}0.394522  ...  0.173117   
             rh    0.007278  0.029909 \PYGZhy{}0.299152 \PYGZhy{}0.295150  ...  0.299440   
        3    lh    0.070114  0.100063 \PYGZhy{}0.245179 \PYGZhy{}0.303354  ... \PYGZhy{}0.055529   
             rh    0.101791  0.128318 \PYGZhy{}0.302654 \PYGZhy{}0.277378  ...  0.079460   
        4    lh   \PYGZhy{}0.069100 \PYGZhy{}0.031653 \PYGZhy{}0.282767 \PYGZhy{}0.279381  ...  0.418857   

network                                      17                                \PYGZbs{}
node                      4                   1                   2             
hemi                     lh        rh        lh        rh        lh        rh   
network node hemi                                                               
1       1    lh   \PYGZhy{}0.162254 \PYGZhy{}0.232501 \PYGZhy{}0.099781 \PYGZhy{}0.161649 \PYGZhy{}0.206379 \PYGZhy{}0.212601   
             rh   \PYGZhy{}0.224436 \PYGZhy{}0.277954 \PYGZhy{}0.212964 \PYGZhy{}0.262915 \PYGZhy{}0.273370 \PYGZhy{}0.266456   
2       1    lh   \PYGZhy{}0.239876 \PYGZhy{}0.093679 \PYGZhy{}0.240455 \PYGZhy{}0.190721 \PYGZhy{}0.151724 \PYGZhy{}0.124508   
             rh   \PYGZhy{}0.244956 \PYGZhy{}0.061151 \PYGZhy{}0.255101 \PYGZhy{}0.169402 \PYGZhy{}0.224447 \PYGZhy{}0.172704   
3       1    lh   \PYGZhy{}0.033931 \PYGZhy{}0.156972 \PYGZhy{}0.015964 \PYGZhy{}0.149944  0.026579 \PYGZhy{}0.089109   
...                     ...       ...       ...       ...       ...       ...   
17      2    lh    0.478606  0.258958  0.499351  0.319184  1.000000  0.597620   
             rh    0.204444  0.453497  0.272868  0.440901  0.597620  1.000000   
        3    lh    0.259191  0.046663  0.454838  0.188905  0.601382  0.345253   
             rh    0.005291  0.296318  0.087061  0.224760  0.319382  0.456019   
        4    lh    0.603491  0.172167  0.589364  0.451264  0.517481  0.256544   

network                                          
node                      3                   4  
hemi                     lh        rh        lh  
network node hemi                                
1       1    lh   \PYGZhy{}0.142770 \PYGZhy{}0.204326 \PYGZhy{}0.219283  
             rh   \PYGZhy{}0.174222 \PYGZhy{}0.223572 \PYGZhy{}0.273626  
2       1    lh   \PYGZhy{}0.179912 \PYGZhy{}0.044706 \PYGZhy{}0.209557  
             rh   \PYGZhy{}0.250455 \PYGZhy{}0.090798 \PYGZhy{}0.216674  
3       1    lh   \PYGZhy{}0.012675 \PYGZhy{}0.024644  0.013747  
...                     ...       ...       ...  
17      2    lh    0.601382  0.319382  0.517481  
             rh    0.345253  0.456019  0.256544  
        3    lh    1.000000  0.379705  0.264381  
             rh    0.379705  1.000000  0.090302  
        4    lh    0.264381  0.090302  1.000000  

[62 rows x 62 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
This result is way to much to see a pattern, yet if we add a color scale and give each a gradation, we can see some correlations.

\sphinxAtStartPar
Can you see how nodes from the same network are related with a more whitish color?
The heatmap might be fairly intimidating at first but is a powerful tool when handling bigger datasets.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{brain\PYGZus{}df}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}network\PYGZhy{}node\PYGZhy{}hemi\PYGZsq{}, ylabel=\PYGZsq{}network\PYGZhy{}node\PYGZhy{}hemi\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{heatmap_7_1}.png}

\sphinxAtStartPar
Without going into the medical details we can also apply some machine learning to it and create a clustermap.
This map is a way to group nodes from similar networks into clusters, an advances technique!

\sphinxAtStartPar
Gaze over the colors and look at the axi, notice how the computer figured out how to group the most similar nodes from networks.
Also, I did not create this by myself, so don’t give me credit for this!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Select a subset of the networks}
\PYG{n}{used\PYGZus{}networks} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{13}\PYG{p}{,} \PYG{l+m+mi}{17}\PYG{p}{]}
\PYG{n}{used\PYGZus{}columns} \PYG{o}{=} \PYG{p}{(}\PYG{n}{brain\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{get\PYGZus{}level\PYGZus{}values}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{network}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
                          \PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}
                          \PYG{o}{.}\PYG{n}{isin}\PYG{p}{(}\PYG{n}{used\PYGZus{}networks}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{brain\PYGZus{}df} \PYG{o}{=} \PYG{n}{brain\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{used\PYGZus{}columns}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Create a categorical palette to identify the networks}
\PYG{n}{network\PYGZus{}pal} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{husl\PYGZus{}palette}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mf}{.45}\PYG{p}{)}
\PYG{n}{network\PYGZus{}lut} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n+nb}{map}\PYG{p}{(}\PYG{n+nb}{str}\PYG{p}{,} \PYG{n}{used\PYGZus{}networks}\PYG{p}{)}\PYG{p}{,} \PYG{n}{network\PYGZus{}pal}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Convert the palette to vectors that will be drawn on the side of the matrix}
\PYG{n}{networks} \PYG{o}{=} \PYG{n}{brain\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{get\PYGZus{}level\PYGZus{}values}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{network}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{network\PYGZus{}colors} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{n}{networks}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{n}{brain\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{network\PYGZus{}lut}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Draw the full plot}
\PYG{n}{g} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{clustermap}\PYG{p}{(}\PYG{n}{brain\PYGZus{}df}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{center}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{vlag}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                   \PYG{n}{row\PYGZus{}colors}\PYG{o}{=}\PYG{n}{network\PYGZus{}colors}\PYG{p}{,} \PYG{n}{col\PYGZus{}colors}\PYG{o}{=}\PYG{n}{network\PYGZus{}colors}\PYG{p}{,}
                   \PYG{n}{dendrogram\PYGZus{}ratio}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{.1}\PYG{p}{,} \PYG{l+m+mf}{.2}\PYG{p}{)}\PYG{p}{,}
                   \PYG{n}{cbar\PYGZus{}pos}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{.02}\PYG{p}{,} \PYG{l+m+mf}{.32}\PYG{p}{,} \PYG{l+m+mf}{.03}\PYG{p}{,} \PYG{l+m+mf}{.2}\PYG{p}{)}\PYG{p}{,}
                   \PYG{n}{linewidths}\PYG{o}{=}\PYG{l+m+mf}{.75}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{13}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{g}\PYG{o}{.}\PYG{n}{ax\PYGZus{}row\PYGZus{}dendrogram}\PYG{o}{.}\PYG{n}{remove}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{heatmap_9_0}.png}


\part{5. Data Exploration}


\chapter{Introduction}
\label{\detokenize{c5_data_exploration/introduction:introduction}}\label{\detokenize{c5_data_exploration/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\chapter{Variable identification}
\label{\detokenize{c5_data_exploration/variable_identification:variable-identification}}\label{\detokenize{c5_data_exploration/variable_identification::doc}}
\sphinxAtStartPar
in this notebook we are going to look into a few simple but interesting techniques about getting to know more about what is inside the dataset you are given. Whenever you start out on a new project these steps are usually the first that are performed in order to know how to proceed.

\sphinxAtStartPar
We start out by loading the titanic dataset from seaborn

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{n}{titanic\PYGZus{}df} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{titanic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set\PYGZus{}theme}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{rc}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{figure.figsize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}


\section{description}
\label{\detokenize{c5_data_exploration/variable_identification:description}}
\sphinxAtStartPar
Let us start out simple and retrieve information about each column, using the .info method we can get non\sphinxhyphen{}null counts (giving us an idea if there are nans) and the type of each column (to see if we need to change types).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 891 entries, 0 to 890
Data columns (total 15 columns):
 \PYGZsh{}   Column       Non\PYGZhy{}Null Count  Dtype   
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}       \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   
 0   survived     891 non\PYGZhy{}null    int64   
 1   pclass       891 non\PYGZhy{}null    int64   
 2   sex          891 non\PYGZhy{}null    object  
 3   age          714 non\PYGZhy{}null    float64 
 4   sibsp        891 non\PYGZhy{}null    int64   
 5   parch        891 non\PYGZhy{}null    int64   
 6   fare         891 non\PYGZhy{}null    float64 
 7   embarked     889 non\PYGZhy{}null    object  
 8   class        891 non\PYGZhy{}null    category
 9   who          891 non\PYGZhy{}null    object  
 10  adult\PYGZus{}male   891 non\PYGZhy{}null    bool    
 11  deck         203 non\PYGZhy{}null    category
 12  embark\PYGZus{}town  889 non\PYGZhy{}null    object  
 13  alive        891 non\PYGZhy{}null    object  
 14  alone        891 non\PYGZhy{}null    bool    
dtypes: bool(2), category(2), float64(2), int64(4), object(5)
memory usage: 80.7+ KB
\end{sphinxVerbatim}

\sphinxAtStartPar
it looks like all types are already correctlyaddressed, but we can see a lot of nans are present for age and deck, this might be a problem!

\sphinxAtStartPar
For numerical columns we can get a bunch of information using the .describe method. this can also be used for categories but has less info

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
         survived      pclass         age       sibsp       parch        fare
count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000
mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208
std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429
min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000
25\PYGZpc{}      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400
50\PYGZpc{}      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200
75\PYGZpc{}      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000
max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{n}{include}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{object}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
         sex embarked  class  who deck  embark\PYGZus{}town alive
count    891      889    891  891  203          889   891
unique     2        3      3    3    7            3     2
top     male        S  Third  man    C  Southampton    no
freq     577      644    491  537   59          644   549
\end{sphinxVerbatim}


\section{Uniques, frequencies and ranges}
\label{\detokenize{c5_data_exploration/variable_identification:uniques-frequencies-and-ranges}}
\sphinxAtStartPar
the describe method is a bit lacklusting for categorical features, so we use some good old data wrangling to get more info, asking for unique values gives us all the possible values for a column. Aside from the uniques, we can also  get the value counts or frequencies and the range of a column.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{embark\PYGZus{}town}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([\PYGZsq{}Southampton\PYGZsq{}, \PYGZsq{}Cherbourg\PYGZsq{}, \PYGZsq{}Queenstown\PYGZsq{}, nan], dtype=object)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{embark\PYGZus{}town}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Southampton    644
Cherbourg      168
Queenstown      77
Name: embark\PYGZus{}town, dtype: int64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(0.42, 80.0)
\end{sphinxVerbatim}


\section{mean and deviation}
\label{\detokenize{c5_data_exploration/variable_identification:mean-and-deviation}}
\sphinxAtStartPar
to get more information about a numerical range, we calculate the mean and deviation. Note that these statistics imply that our column is normally distributed!

\sphinxAtStartPar
You can also see that I applied the dropna method, this because the calculations cannot handle nan values, but this means our outcome might be distorted from the truth, thread carefuly.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{statistics}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
29.69911764705882
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
28.0
\end{sphinxVerbatim}


\section{median and interquantile range}
\label{\detokenize{c5_data_exploration/variable_identification:median-and-interquantile-range}}
\sphinxAtStartPar
When our distribution is not normal, using the median and IQR is advised.
First we apply the shapiro wilk test and it has a very low p\sphinxhyphen{}value (the second value) which means we can reject the null\sphinxhyphen{}hypothesis that there is a normal distribution. more info about shapiro\sphinxhyphen{}wilk can be found on \sphinxhref{https://en.wikipedia.org/wiki/Shapiro\%E2\%80\%93Wilk\_test}{wikipedia}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{shapiro}
\PYG{n}{shapiro}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ShapiroResult(statistic=0.9814548492431641, pvalue=7.322165629375377e\PYGZhy{}08)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
28.0
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{iqr}
\PYG{n}{iqr}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
17.875
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats}\PYG{n+nn}{.}\PYG{n+nn}{mstats} \PYG{k+kn}{import} \PYG{n}{mquantiles}
\PYG{n}{mquantiles}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([20., 28., 38.])
\end{sphinxVerbatim}

\sphinxAtStartPar
Appearently the average of 29.70 is fairly higher than the median at 28, meaning that there is a shift towards older people.
You can also see this on the following plot, where we note the mean, median and mode.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{histplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{age}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cyan}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{age}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{magenta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{age}\PYG{o}{.}\PYG{n}{mode}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{yellow}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.lines.Line2D at 0x7fa1b3657e50\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{variable_identification_21_1}.png}


\section{modes and frequencies}
\label{\detokenize{c5_data_exploration/variable_identification:modes-and-frequencies}}
\sphinxAtStartPar
When we don’t have numerical data we can still find some interesting results, here we use the mode ( most frequent value) and the proporties of each value to deduce the proporties of people that embarked in the 3 different towns. Nearly 3/4 people embarked in one harbour.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{embark\PYGZus{}town}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mode}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0    Southampton
dtype: object
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{embark\PYGZus{}town}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Southampton    0.722783
Cherbourg      0.188552
Queenstown     0.086420
Name: embark\PYGZus{}town, dtype: float64
\end{sphinxVerbatim}


\chapter{Uni\sphinxhyphen{}variate analysis}
\label{\detokenize{c5_data_exploration/univariate_analysis:uni-variate-analysis}}\label{\detokenize{c5_data_exploration/univariate_analysis::doc}}
\sphinxAtStartPar
In this notebook we will go a bit deeper into the analysis of a single column or variable of our dataset. This means we will be looking into how visualisations might be useful to attain more information. We start out again by loading the titanic dataset and obtaining the same info as before.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{n}{titanic\PYGZus{}df} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{titanic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set\PYGZus{}style}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{rc}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{figure.figsize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 891 entries, 0 to 890
Data columns (total 15 columns):
 \PYGZsh{}   Column       Non\PYGZhy{}Null Count  Dtype   
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}       \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   
 0   survived     891 non\PYGZhy{}null    int64   
 1   pclass       891 non\PYGZhy{}null    int64   
 2   sex          891 non\PYGZhy{}null    object  
 3   age          714 non\PYGZhy{}null    float64 
 4   sibsp        891 non\PYGZhy{}null    int64   
 5   parch        891 non\PYGZhy{}null    int64   
 6   fare         891 non\PYGZhy{}null    float64 
 7   embarked     889 non\PYGZhy{}null    object  
 8   class        891 non\PYGZhy{}null    category
 9   who          891 non\PYGZhy{}null    object  
 10  adult\PYGZus{}male   891 non\PYGZhy{}null    bool    
 11  deck         203 non\PYGZhy{}null    category
 12  embark\PYGZus{}town  889 non\PYGZhy{}null    object  
 13  alive        891 non\PYGZhy{}null    object  
 14  alone        891 non\PYGZhy{}null    bool    
dtypes: bool(2), category(2), float64(2), int64(4), object(5)
memory usage: 80.7+ KB
\end{sphinxVerbatim}


\section{Nominal data}
\label{\detokenize{c5_data_exploration/univariate_analysis:nominal-data}}
\sphinxAtStartPar
Lets take a look into a norminal column, the embark town has 3 different options and we already saw how to count the values and calculate proportions.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{embark\PYGZus{}town}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Southampton    644
Cherbourg      168
Queenstown      77
Name: embark\PYGZus{}town, dtype: int64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{embark\PYGZus{}town}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Southampton    0.722783
Cherbourg      0.188552
Queenstown     0.086420
Name: embark\PYGZus{}town, dtype: float64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{statistics}
\PYG{n}{statistics}\PYG{o}{.}\PYG{n}{mode}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{embark\PYGZus{}town}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsq{}Southampton\PYGZsq{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{statistics}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{embark\PYGZus{}town}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{TypeError}\PYG{g+gWhitespace}{                                 }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}11541}\PYG{o}{/}\PYG{l+m+mf}{1735617235.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{n}{statistics}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{embark\PYGZus{}town}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{n+nn}{/usr/lib/python3.8/statistics.py} in \PYG{n+ni}{median}\PYG{n+nt}{(data)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{425} 
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{426}     \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{427}\PYG{l+s+s2}{     data = sorted(data)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{428}\PYG{l+s+s2}{     n = len(data)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{429}\PYG{l+s+s2}{     if n == 0:}

\PYG{n+ne}{TypeError}: \PYGZsq{}\PYGZlt{}\PYGZsq{} not supported between instances of \PYGZsq{}float\PYGZsq{} and \PYGZsq{}str\PYGZsq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
Hmmm, it seems we can not take the median because python does not know the order of the categories. Let’s kick it up a notch and use some plots to make these proportions more clear, we’ll use a bar chart to do this.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{countplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{embark\PYGZus{}town}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f4e7f61d100\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{univariate_analysis_9_1}.png}

\sphinxAtStartPar
Something important that I would like to mention here is that this serves as a method to validate sample size, if e.g. only a handful persons would embark on a location, the statistics in this group will have a high variance which will not always shot in your visualisations.
Be mindful to check sample sizes of categories when applying statistics.

\sphinxAtStartPar
The bar chart is ideal to compare the values to eachother, yet if we would like to visualise the proportions to eachother, we need a pie plot.
Here we use the ‘who’ feature containing information about the person itself, we have 3 categories: man, woman, child.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{who}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{o}{.}\PYG{n}{pie}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f4e7d2ca430\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{univariate_analysis_11_1}.png}

\sphinxAtStartPar
The saying goes ‘Woman and children first’ which would mean the survivors are mainly those 2 groups, let us confirm that by subselecting only the survivors and recreate the pie plot.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{survived}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{who}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{o}{.}\PYG{n}{pie}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f4e7d285dc0\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{univariate_analysis_13_1}.png}

\sphinxAtStartPar
You can see that the groups are now reversed, where men are proportionally less represented.
By using a pie plot we circumvent the problem where we have a bias towards size of our dataset, the pie plot applies scaling by itself.


\section{Ordinal data}
\label{\detokenize{c5_data_exploration/univariate_analysis:ordinal-data}}
\sphinxAtStartPar
Whilst there was no order in the town where passengers embarked, there is in the class of the ticket they bought. So we need to keep this in mind when exploring. We can not just say they belonged to any class as there is a difference in these classes! However the same statistics apply, but with a different twist.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Third     491
First     216
Second    184
Name: class, dtype: int64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}\PYG{o}{/}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Third     0.551066
First     0.242424
Second    0.206510
Name: class, dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
it seems more people travelled on the titanic in first class than second class! nothing you would see nowadays.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{statistics}\PYG{o}{.}\PYG{n}{mode}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsq{}Third\PYGZsq{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{statistics}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsq{}Third\PYGZsq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we can use the median, as there is an order in the classes! By using a bar plot we can visualise the distribution, because the graphing library knows the order of the categories, they will also be properly displayed, how convenient.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{countplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f4e7d250580\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{univariate_analysis_22_1}.png}

\sphinxAtStartPar
You could however also create a line plot with this, as there is a relation between the classes, as shown below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{First}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Second}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Third}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f4e7d224370\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{univariate_analysis_24_1}.png}

\sphinxAtStartPar
This plot feels underwhelming with only 3 points, but we could make it more interesting, we divide our data on who survived and count the amount of persons per class that survived or not. It is clear to say the a higher class meant higher chances of survival.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{alive}\PYG{p}{)}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reindex}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{First}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Second}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Third}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f4e7d1f9550\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{univariate_analysis_26_1}.png}

\sphinxAtStartPar
Personally as there is no time related factor in our x\sphinxhyphen{}axis, the line or parallel plot here is not as convenient.
Since we have a situation where there is a confinement that the amount of survived can not be more that the total, I would opt for a bar plot, which is show below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{countplot}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{titanic\PYGZus{}df}\PYG{p}{,} \PYG{n}{color} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{countplot}\PYG{p}{(}\PYG{n}{x} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{survived}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f4e7d278b50\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{univariate_analysis_28_1}.png}


\section{Continuous data}
\label{\detokenize{c5_data_exploration/univariate_analysis:continuous-data}}
\sphinxAtStartPar
After categories which are discrete we also have continuous data, which is by nature always ordered. Here we can perform all the other statistical methods along with the mean, but again keep in mind that using the mean does come with a lot of responsibility.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{statistics}\PYG{o}{.}\PYG{n}{mode}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
24.0
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{statistics}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
28.0
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{statistics}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
29.699117647058824
\end{sphinxVerbatim}

\sphinxAtStartPar
A very potent method of showing the distribution is a histogram or distribution plot as shown below, here we can see the long tail on the right which we correctly predicted earlier when we saw that the mean was slightly higher than the median.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{histplot}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f4e7ce39f10\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{univariate_analysis_34_1}.png}

\sphinxAtStartPar
going into more mathematical calculations, we can calculate the interquartile ranges, the upper and lower bounds and therefore find any outliers

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{q1}\PYG{p}{,} \PYG{n}{q3} \PYG{o}{=} \PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{quantile}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.25}\PYG{p}{,} \PYG{l+m+mf}{0.75}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{q3}\PYG{o}{\PYGZhy{}}\PYG{n}{q1}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
17.875
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{lower\PYGZus{}bound} \PYG{o}{=} \PYG{n}{q1} \PYG{o}{\PYGZhy{}} \PYG{p}{(}\PYG{l+m+mf}{1.5} \PYG{o}{*} \PYG{n}{q1}\PYG{p}{)}
\PYG{n}{upper\PYGZus{}bound} \PYG{o}{=} \PYG{n}{q3} \PYG{o}{+} \PYG{p}{(}\PYG{l+m+mf}{1.5} \PYG{o}{*} \PYG{n}{q3}\PYG{p}{)}
\PYG{n}{lower\PYGZus{}bound}\PYG{p}{,} \PYG{n}{upper\PYGZus{}bound}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(\PYGZhy{}10.0625, 95.0)
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems that for age, no outliers have been found, which is not really suprising as you don’t have any control over your age, unfortunately…

\sphinxAtStartPar
Another numerical feature they had control over was the fare, we give a visualisation of the distrubition here.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{histplot}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fare}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f4e7cddbf10\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{univariate_analysis_39_1}.png}

\sphinxAtStartPar
This distribution looks horrific, we could also look at the mean and median differences to see this tremendous shift towards higher fares.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{median}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
median
14.4542
mean
32.204207968574636
\end{sphinxVerbatim}

\sphinxAtStartPar
Perhaps you can use the outlier detection of above to find the upper outlier treshold?

\sphinxAtStartPar
Let us assume that the upper bound for fares is about 50, which is lower than some tickets.
By removing these values we can correct our distribution and get a more evened out result.
This is especially useful in cases of machine learning where we would not not our algorithm to be biased due to a few extraordinary values, we would have to seperate these specific cases to ensure higher accuracy.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{histplot}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{\PYGZlt{}}\PYG{l+m+mi}{50}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fare}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f4e7cbf1220\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{univariate_analysis_45_1}.png}

\sphinxAtStartPar
Much better, here we can clearly see our values, keep the records with outliers seperate for other purposes.
Again looking at the new mean and median we see a lot less difference, indicating a better distribution.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{median}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{\PYGZlt{}}\PYG{l+m+mi}{50}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{\PYGZlt{}}\PYG{l+m+mi}{50}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
median
11.1333
mean
15.500598493150687
\end{sphinxVerbatim}


\chapter{Bi\sphinxhyphen{}variate analysis}
\label{\detokenize{c5_data_exploration/bivariate_analysis:bi-variate-analysis}}\label{\detokenize{c5_data_exploration/bivariate_analysis::doc}}
\sphinxAtStartPar
In this notebook we are going to look at correlations between two columns in our dataset, this is were it becomes interesting as it opens more opportunities to explore our dataset. We start out by importing necessary libraries and loading the titanic dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{scipy} \PYG{k+kn}{import} \PYG{n}{stats}
\PYG{n}{titanic\PYGZus{}df} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{titanic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set\PYGZus{}style}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{rc}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{figure.figsize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 891 entries, 0 to 890
Data columns (total 15 columns):
 \PYGZsh{}   Column       Non\PYGZhy{}Null Count  Dtype   
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}       \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   
 0   survived     891 non\PYGZhy{}null    int64   
 1   pclass       891 non\PYGZhy{}null    int64   
 2   sex          891 non\PYGZhy{}null    object  
 3   age          714 non\PYGZhy{}null    float64 
 4   sibsp        891 non\PYGZhy{}null    int64   
 5   parch        891 non\PYGZhy{}null    int64   
 6   fare         891 non\PYGZhy{}null    float64 
 7   embarked     889 non\PYGZhy{}null    object  
 8   class        891 non\PYGZhy{}null    category
 9   who          891 non\PYGZhy{}null    object  
 10  adult\PYGZus{}male   891 non\PYGZhy{}null    bool    
 11  deck         203 non\PYGZhy{}null    category
 12  embark\PYGZus{}town  889 non\PYGZhy{}null    object  
 13  alive        891 non\PYGZhy{}null    object  
 14  alone        891 non\PYGZhy{}null    bool    
dtypes: bool(2), category(2), float64(2), int64(4), object(5)
memory usage: 80.7+ KB
\end{sphinxVerbatim}


\section{Categorical vs categorical}
\label{\detokenize{c5_data_exploration/bivariate_analysis:categorical-vs-categorical}}
\sphinxAtStartPar
The first comparison we can do is between 2 categorical variables, in this dataset we can use the class of the passenger and the town they embarked the titanic, let’s make a contingency table first.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{contingency\PYGZus{}table} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{crosstab}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{embark\PYGZus{}town}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{contingency\PYGZus{}table}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
class        First  Second  Third
embark\PYGZus{}town                      
Cherbourg       85      17     66
Queenstown       2       3     72
Southampton    127     164    353
\end{sphinxVerbatim}

\sphinxAtStartPar
With all these numbers it is fairly hard to find if there is a correlation between these 2 variables. Let statistics do the work and get the chi squared test involved, we do not apply a continuity correction as the embarkment is a nominal variable.

\sphinxAtStartPar
The results of the Cramer V test (simplified chi squared test).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{chi}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{dof}\PYG{p}{,} \PYG{n}{exp} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{chi2\PYGZus{}contingency}\PYG{p}{(}\PYG{n}{contingency\PYGZus{}table}\PYG{p}{,} \PYG{n}{correction}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{chi}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{dof}\PYG{p}{,} \PYG{n}{exp}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(123.75190952951289,
 8.435267819894384e\PYGZhy{}26,
 4,
 array([[ 40.44094488,  34.77165354,  92.78740157],
        [ 18.53543307,  15.93700787,  42.52755906],
        [155.02362205, 133.29133858, 355.68503937]]))
\end{sphinxVerbatim}

\sphinxAtStartPar
in order of appearance:
\begin{itemize}
\item {} 
\sphinxAtStartPar
the test statistic chi is very high, indicating a correlation

\item {} 
\sphinxAtStartPar
the p value is low, so this is definitely not by chance

\item {} 
\sphinxAtStartPar
there are 4 ‘degrees of freedom’

\item {} 
\sphinxAtStartPar
the expected frequency table shows what it thinks the proporties should look like

\end{itemize}

\sphinxAtStartPar
What we could do now is create a heatmap with the contingency table but subtract the expected non\sphinxhyphen{}biased values and scale using the expected values (real \sphinxhyphen{} expected)/expected.
This gives us the biggest changes in respect with ‘random’ values.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}
    \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{(}\PYG{n}{contingency\PYGZus{}table}\PYG{o}{\PYGZhy{}}\PYG{n}{exp}\PYG{p}{)}\PYG{o}{/}\PYG{n}{exp}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{n}{contingency\PYGZus{}table}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{n}{contingency\PYGZus{}table}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}
\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}class\PYGZsq{}, ylabel=\PYGZsq{}embark\PYGZus{}town\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{bivariate_analysis_8_1}.png}

\sphinxAtStartPar
There seems to be much more people from first class that have embarked in Cherbourg, and the lower classes are more represented from Queenstown.
The population from southampton only sees a positive deviation in second class.

\sphinxAtStartPar
To demonstrate that there can also be no correlation we now calculate the proportions of survival for each town and class combination.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{survived\PYGZus{}df} \PYG{o}{=} \PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{embark\PYGZus{}town}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{survived}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{/}\PYG{n}{contingency\PYGZus{}table}
\PYG{n}{survived\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
class           First    Second     Third
embark\PYGZus{}town                              
Cherbourg    0.694118  0.529412  0.378788
Queenstown   0.500000  0.666667  0.375000
Southampton  0.582677  0.463415  0.189802
\end{sphinxVerbatim}

\sphinxAtStartPar
If we would do a Cramer V test now, we assume there would be no significance, as it would not make sense that the embarked town has no influence on the chances (proportion of survived persons) of survival.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{chi}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{dof}\PYG{p}{,} \PYG{n}{exp} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{chi2\PYGZus{}contingency}\PYG{p}{(}\PYG{n}{survived\PYGZus{}df}\PYG{p}{,} \PYG{n}{correction}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{p}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.9989353452702686
\end{sphinxVerbatim}

\sphinxAtStartPar
As you can see, the p value is 0.99, indicating that the differences in embarkment are purely coincidental!


\section{Categorical vs continuous}
\label{\detokenize{c5_data_exploration/bivariate_analysis:categorical-vs-continuous}}
\sphinxAtStartPar
The most interesting exploration (in my opinion) happens when we combine categorical and continuous data, as more graphing opportunities are present.
When doing this comparison, we usually use the student t\sphinxhyphen{}test or Z\sphinxhyphen{}test, you can spend hours arguing the difference and which to use, yet I will stick for simplicity with the t\sphinxhyphen{}test for robuustness.

\sphinxAtStartPar
we can use the t\sphinxhyphen{}test to check if a continuous variable changes between 2 categories of a categorical variable.

\sphinxAtStartPar
let us seperate the men from the women and see if they had to pay a different fare amount

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{t}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{ttest\PYGZus{}ind}\PYG{p}{(}
    \PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{who}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{man}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{who}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{woman}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{p}{)}
\PYG{n}{t}\PYG{p}{,} \PYG{n}{p}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(\PYGZhy{}5.817465335062089, 8.614583735152227e\PYGZhy{}09)
\end{sphinxVerbatim}

\sphinxAtStartPar
Our p\sphinxhyphen{}value again is very low, indicating there is a difference in the groups.
The t statistic is \sphinxhyphen{}5.82, meaning that the second group (women) are paying more for fares.

\sphinxAtStartPar
We print out the means to verify

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean male fare}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{who}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{man}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean female fare}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{who}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{woman}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mean male fare
24.864181750465548
mean female fare
46.570711070110704
\end{sphinxVerbatim}

\sphinxAtStartPar
By the looks of this, the fares are heavily gender biased.
To put this into more detail, we pivot the means of each group including class into a table, as female might be more in the upper classes.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{who}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
class       First     Second      Third
who                                    
child  139.382633  28.323905  23.220190
man     65.951086  19.054124  11.340213
woman  104.317995  20.868624  15.354351
\end{sphinxVerbatim}

\sphinxAtStartPar
This already makes more sense, it is mainly the first class difference that drives up the prices, yet the difference seems to be still present.

\sphinxAtStartPar
Can you perform a t\sphinxhyphen{}test on the gender fare gap in the third class, is it still significant?

\sphinxAtStartPar
A t\sphinxhyphen{}test is ideal if you would like to compare 2 groups, yet often we have multiple groups.
For this we can use a (one\_way) ANOVA or ANalysis Of VAriance.

\sphinxAtStartPar
We seperate on class and check if the fare is significantly different.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{F}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{f\PYGZus{}oneway}\PYG{p}{(}
    \PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{pclass}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{pclass}\PYG{o}{==}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{pclass}\PYG{o}{==}\PYG{l+m+mi}{3}\PYG{p}{]}
\PYG{p}{)}
\PYG{n}{F}\PYG{p}{,} \PYG{n}{p}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(242.34415651744814, 1.0313763209141171e\PYGZhy{}84)
\end{sphinxVerbatim}

\sphinxAtStartPar
This was more or less a no\sphinxhyphen{}brainer, as it is advertised that higher classes come with a higher pricetag.
We can use a nice histogram to show this division of class.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{histplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{!=}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fare}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{log\PYGZus{}scale}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{multiple}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{stack}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{25}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}fare\PYGZsq{}, ylabel=\PYGZsq{}Count\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{bivariate_analysis_26_1}.png}

\sphinxAtStartPar
A less cluttered plot would be to use a boxplot, containing less information about the distribution, yet still showing simple statistics.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{!=}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{class}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fare}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yscale}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{bivariate_analysis_28_0}.png}

\sphinxAtStartPar
We could do something similar, but taking the age instead of the fare, giving us the following result.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{F}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{f\PYGZus{}oneway}\PYG{p}{(}
    \PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{age}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{pclass}\PYG{o}{==}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{age}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{pclass}\PYG{o}{==}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{age}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{pclass}\PYG{o}{==}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}
\PYG{p}{)}
\PYG{n}{F}\PYG{p}{,} \PYG{n}{p}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(57.443484340676214, 7.487984171959904e\PYGZhy{}24)
\end{sphinxVerbatim}

\sphinxAtStartPar
The p value indicates there is surely a difference in age between classes, how about we look at the means for each class.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pclass}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{age}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pclass
1    38.233441
2    29.877630
3    25.140620
Name: age, dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
What about any statistical significant differences in ages for the groups that survived and didn’t, could you perform this analysis?
Report your findings in a histogram.


\section{Continuous vs continuous}
\label{\detokenize{c5_data_exploration/bivariate_analysis:continuous-vs-continuous}}
\sphinxAtStartPar
A thirds option to explore the interactions within your dataset is by comparing 2 continuous variables.

\sphinxAtStartPar
Seaborn has a nice functionality where can perform a jointplot that not only shows us the scatter plot but also the distributions,
When we perform this plot we notice the inbalanced distribution of the fares.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{jointplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fare}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}seaborn.axisgrid.JointGrid at 0x7f9fa6f080d0\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{bivariate_analysis_36_1}.png}

\sphinxAtStartPar
What we could do is remove outliers, if I recall correclty we set a upper bound of 77.5, let’s do that here and replot.
I’ve also added the type of person as a color, you can here see that women and children pay more as we saw earlier.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{jointplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{\PYGZlt{}}\PYG{l+m+mf}{77.5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fare}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{who}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}seaborn.axisgrid.JointGrid at 0x7f9fa6ac27f0\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{bivariate_analysis_38_1}.png}

\sphinxAtStartPar
To make this more mathematically sound, we are using the spearman rank correlation test, not the pearson as we are dealing with non normal data. You could check that with a shapiro wilk test but i’ll leave that up to you!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{corr}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{spearmanr}\PYG{p}{(}\PYG{n}{a}\PYG{o}{=}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fare}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{corr}\PYG{p}{,} \PYG{n}{p}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(0.1350512177342878, 0.00029580903243060916)
\end{sphinxVerbatim}

\sphinxAtStartPar
with a p\sphinxhyphen{}value of only 0.000296 we can safely reject the null\sphinxhyphen{}hypothesis, meaning there is a correlation.
The correlation coefficient here is only 0.135, meaning for any person each year of age would make their fare about 0.135 dollars more expensive on average, which in that time was a fair amount of money.

\sphinxAtStartPar
To make this more visual, I added a lmplot that performs a linear regression, you can see how the line goes up in fare as the age goes up.
I had to use a logarithmic y\sphinxhyphen{}scale as the distribution is still not normal.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{lmplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fare}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{yscale}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}seaborn.axisgrid.FacetGrid at 0x7f9fa4916100\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{bivariate_analysis_42_1}.png}

\sphinxAtStartPar
Now this correlation of 0.135 dollar is relevant for ANY person, man, female, child, first class, second,…

\sphinxAtStartPar
Perhaps we could find several subgroups with a higher or lower correlation, I will perform the correlation with the outliers removed.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{corr}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{spearmanr}\PYG{p}{(}\PYG{n}{a}\PYG{o}{=}\PYG{n}{titanic\PYGZus{}df}\PYG{p}{[}\PYG{n}{titanic\PYGZus{}df}\PYG{o}{.}\PYG{n}{fare}\PYG{o}{\PYGZlt{}}\PYG{l+m+mf}{77.5}\PYG{p}{]}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{fare}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{corr}\PYG{p}{,} \PYG{n}{p}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(0.09269934275477329, 0.019762193968013368)
\end{sphinxVerbatim}

\sphinxAtStartPar
When we remove outliers, we have a less strong correlations, indicating that the outliers \sphinxhyphen{} with high fares \sphinxhyphen{} are in general older persons.

\sphinxAtStartPar
Try to experiment with subsetting the data and find a group where age matters more for the correlation.


\chapter{New Data Sources}
\label{\detokenize{c5_data_exploration/external_data:new-data-sources}}\label{\detokenize{c5_data_exploration/external_data::doc}}
\sphinxAtStartPar
In this notebook we are going to look into adding new data to your dataset.
We start out with a taxi dataset describing all pickup points from taxis in a specific date interval, notice that the dataset is divided up into months. Each month has their specific csv file saved in an AWS location.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{from} \PYG{n+nn}{urllib}\PYG{n+nn}{.}\PYG{n+nn}{request} \PYG{k+kn}{import} \PYG{n}{urlopen}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data\PYGZus{}url\PYGZus{}files} \PYG{o}{=} \PYG{n}{urlopen}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/toddwschneider/nyc\PYGZhy{}taxi\PYGZhy{}data/master/setup\PYGZus{}files/raw\PYGZus{}data\PYGZus{}urls.txt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{data\PYGZus{}urls} \PYG{o}{=} \PYG{n}{data\PYGZus{}url\PYGZus{}files}\PYG{o}{.}\PYG{n}{read}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{decode}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{utf\PYGZhy{}8}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{data\PYGZus{}urls}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{12}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}https://s3.amazonaws.com/nyc\PYGZhy{}tlc/trip+data/fhv\PYGZus{}tripdata\PYGZus{}2015\PYGZhy{}01.csv\PYGZsq{},
 \PYGZsq{}https://s3.amazonaws.com/nyc\PYGZhy{}tlc/trip+data/fhv\PYGZus{}tripdata\PYGZus{}2015\PYGZhy{}02.csv\PYGZsq{},
 \PYGZsq{}https://s3.amazonaws.com/nyc\PYGZhy{}tlc/trip+data/fhv\PYGZus{}tripdata\PYGZus{}2015\PYGZhy{}03.csv\PYGZsq{},
 \PYGZsq{}https://s3.amazonaws.com/nyc\PYGZhy{}tlc/trip+data/fhv\PYGZus{}tripdata\PYGZus{}2015\PYGZhy{}04.csv\PYGZsq{},
 \PYGZsq{}https://s3.amazonaws.com/nyc\PYGZhy{}tlc/trip+data/fhv\PYGZus{}tripdata\PYGZus{}2015\PYGZhy{}05.csv\PYGZsq{},
 \PYGZsq{}https://s3.amazonaws.com/nyc\PYGZhy{}tlc/trip+data/fhv\PYGZus{}tripdata\PYGZus{}2015\PYGZhy{}06.csv\PYGZsq{},
 \PYGZsq{}https://s3.amazonaws.com/nyc\PYGZhy{}tlc/trip+data/fhv\PYGZus{}tripdata\PYGZus{}2015\PYGZhy{}07.csv\PYGZsq{},
 \PYGZsq{}https://s3.amazonaws.com/nyc\PYGZhy{}tlc/trip+data/fhv\PYGZus{}tripdata\PYGZus{}2015\PYGZhy{}08.csv\PYGZsq{},
 \PYGZsq{}https://s3.amazonaws.com/nyc\PYGZhy{}tlc/trip+data/fhv\PYGZus{}tripdata\PYGZus{}2015\PYGZhy{}09.csv\PYGZsq{},
 \PYGZsq{}https://s3.amazonaws.com/nyc\PYGZhy{}tlc/trip+data/fhv\PYGZus{}tripdata\PYGZus{}2015\PYGZhy{}10.csv\PYGZsq{},
 \PYGZsq{}https://s3.amazonaws.com/nyc\PYGZhy{}tlc/trip+data/fhv\PYGZus{}tripdata\PYGZus{}2015\PYGZhy{}11.csv\PYGZsq{},
 \PYGZsq{}https://s3.amazonaws.com/nyc\PYGZhy{}tlc/trip+data/fhv\PYGZus{}tripdata\PYGZus{}2015\PYGZhy{}12.csv\PYGZsq{}]
\end{sphinxVerbatim}

\sphinxAtStartPar
Due to slow parsing of data we will here only parse the uber data from jan\sphinxhyphen{}mar 2015

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{datasets} \PYG{o}{=} \PYG{p}{[}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{url}\PYG{p}{)} \PYG{k}{for} \PYG{n}{url} \PYG{o+ow}{in} \PYG{n}{data\PYGZus{}urls}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cab\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{n}{datasets}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{cab\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{cab\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
shape: (9153861, 3)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  Dispatching\PYGZus{}base\PYGZus{}num          Pickup\PYGZus{}date  locationID
0               B00013  2015\PYGZhy{}01\PYGZhy{}01 00:30:00         NaN
1               B00013  2015\PYGZhy{}01\PYGZhy{}01 01:22:00         NaN
2               B00013  2015\PYGZhy{}01\PYGZhy{}01 01:23:00         NaN
3               B00013  2015\PYGZhy{}01\PYGZhy{}01 01:44:00         NaN
4               B00013  2015\PYGZhy{}01\PYGZhy{}01 02:00:00         NaN
\end{sphinxVerbatim}

\sphinxAtStartPar
We would like to find out how many uber rides were performed each day so we:
\begin{itemize}
\item {} 
\sphinxAtStartPar
parse the date string to a datetime format

\item {} 
\sphinxAtStartPar
set the date as index

\item {} 
\sphinxAtStartPar
resample to ‘1D’ or one day (and chose count as aggregation)

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cab\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{datetime}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{n}{cab\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Pickup\PYGZus{}date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n+nb}{format}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{Y/}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{m/}\PYG{l+s+si}{\PYGZpc{}d}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{H:}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{M:}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{S}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cab\PYGZus{}df} \PYG{o}{=} \PYG{n}{cab\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{datetime}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cab\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                    Dispatching\PYGZus{}base\PYGZus{}num          Pickup\PYGZus{}date  locationID
datetime                                                                 
2015\PYGZhy{}01\PYGZhy{}01 00:30:00               B00013  2015\PYGZhy{}01\PYGZhy{}01 00:30:00         NaN
2015\PYGZhy{}01\PYGZhy{}01 01:22:00               B00013  2015\PYGZhy{}01\PYGZhy{}01 01:22:00         NaN
2015\PYGZhy{}01\PYGZhy{}01 01:23:00               B00013  2015\PYGZhy{}01\PYGZhy{}01 01:23:00         NaN
2015\PYGZhy{}01\PYGZhy{}01 01:44:00               B00013  2015\PYGZhy{}01\PYGZhy{}01 01:44:00         NaN
2015\PYGZhy{}01\PYGZhy{}01 02:00:00               B00013  2015\PYGZhy{}01\PYGZhy{}01 02:00:00         NaN
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cabs\PYGZus{}taken} \PYG{o}{=} \PYG{n}{cab\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dispatching\PYGZus{}base\PYGZus{}num}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{resample}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{rename}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cabs\PYGZus{}taken}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{cabs\PYGZus{}taken}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
datetime
2015\PYGZhy{}01\PYGZhy{}01    77789
2015\PYGZhy{}01\PYGZhy{}02    61832
2015\PYGZhy{}01\PYGZhy{}03    81955
2015\PYGZhy{}01\PYGZhy{}04    62691
2015\PYGZhy{}01\PYGZhy{}05    71063
Freq: D, Name: cabs\PYGZus{}taken, dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
great! now we have an idea on how many ubers were taken each day, let us use a simple line plot to show the results.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{lineplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{cabs\PYGZus{}taken}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}datetime\PYGZsq{}, ylabel=\PYGZsq{}cabs\PYGZus{}taken\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{external_data_13_1}.png}

\sphinxAtStartPar
This dataset is nice, but by itself pretty useless, why don’t we look up some weather information to see if this influences our traffic.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{url} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/toddwschneider/nyc\PYGZhy{}taxi\PYGZhy{}data/master/data/central\PYGZus{}park\PYGZus{}weather.csv}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{weather} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{url}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weather}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       STATION                         NAME        DATE   AWND  PRCP  SNOW  \PYGZbs{}
0  USW00094728  NY CITY CENTRAL PARK, NY US  2009\PYGZhy{}01\PYGZhy{}01  11.18   0.0   0.0   
1  USW00094728  NY CITY CENTRAL PARK, NY US  2009\PYGZhy{}01\PYGZhy{}02   6.26   0.0   0.0   
2  USW00094728  NY CITY CENTRAL PARK, NY US  2009\PYGZhy{}01\PYGZhy{}03  10.07   0.0   0.0   
3  USW00094728  NY CITY CENTRAL PARK, NY US  2009\PYGZhy{}01\PYGZhy{}04   7.61   0.0   0.0   
4  USW00094728  NY CITY CENTRAL PARK, NY US  2009\PYGZhy{}01\PYGZhy{}05   6.93   0.0   0.0   

   SNWD  TMAX  TMIN  
0   0.0    26    15  
1   0.0    34    23  
2   0.0    38    29  
3   0.0    42    25  
4   0.0    43    38  
\end{sphinxVerbatim}

\sphinxAtStartPar
you can see a variaty of information, more info on the column names can be found \sphinxhref{https://docs.opendata.aws/noaa-ghcn-pds/readme.html}{here}again we need to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
parse the date

\item {} 
\sphinxAtStartPar
set it to the index

\item {} 
\sphinxAtStartPar
resampling is not needed as it is already in day\sphinxhyphen{}to\sphinxhyphen{}day intervals

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weather}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DATE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=}  \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{n}{weather}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DATE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n+nb}{format}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{Y/}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{m/}\PYG{l+s+si}{\PYGZpc{}d}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{weather} \PYG{o}{=} \PYG{n}{weather}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DATE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weather}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                STATION                         NAME   AWND  PRCP  SNOW  SNWD  \PYGZbs{}
DATE                                                                            
2009\PYGZhy{}01\PYGZhy{}01  USW00094728  NY CITY CENTRAL PARK, NY US  11.18   0.0   0.0   0.0   
2009\PYGZhy{}01\PYGZhy{}02  USW00094728  NY CITY CENTRAL PARK, NY US   6.26   0.0   0.0   0.0   
2009\PYGZhy{}01\PYGZhy{}03  USW00094728  NY CITY CENTRAL PARK, NY US  10.07   0.0   0.0   0.0   
2009\PYGZhy{}01\PYGZhy{}04  USW00094728  NY CITY CENTRAL PARK, NY US   7.61   0.0   0.0   0.0   
2009\PYGZhy{}01\PYGZhy{}05  USW00094728  NY CITY CENTRAL PARK, NY US   6.93   0.0   0.0   0.0   

            TMAX  TMIN  
DATE                    
2009\PYGZhy{}01\PYGZhy{}01    26    15  
2009\PYGZhy{}01\PYGZhy{}02    34    23  
2009\PYGZhy{}01\PYGZhy{}03    38    29  
2009\PYGZhy{}01\PYGZhy{}04    42    25  
2009\PYGZhy{}01\PYGZhy{}05    43    38  
\end{sphinxVerbatim}

\sphinxAtStartPar
Having 2 dataset, now we need to merge them. Since we already prepared the date as index, this should be easy.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{cabs\PYGZus{}taken}\PYG{p}{,} \PYG{n}{weather}\PYG{p}{,} \PYG{n}{left\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{right\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            cabs\PYGZus{}taken      STATION                         NAME   AWND  PRCP  \PYGZbs{}
2015\PYGZhy{}01\PYGZhy{}01       77789  USW00094728  NY CITY CENTRAL PARK, NY US   7.16  0.00   
2015\PYGZhy{}01\PYGZhy{}02       61832  USW00094728  NY CITY CENTRAL PARK, NY US   7.16  0.00   
2015\PYGZhy{}01\PYGZhy{}03       81955  USW00094728  NY CITY CENTRAL PARK, NY US   6.49  0.71   
2015\PYGZhy{}01\PYGZhy{}04       62691  USW00094728  NY CITY CENTRAL PARK, NY US   6.49  0.30   
2015\PYGZhy{}01\PYGZhy{}05       71063  USW00094728  NY CITY CENTRAL PARK, NY US  10.51  0.00   

            SNOW  SNWD  TMAX  TMIN  
2015\PYGZhy{}01\PYGZhy{}01   0.0   0.0    39    27  
2015\PYGZhy{}01\PYGZhy{}02   0.0   0.0    42    35  
2015\PYGZhy{}01\PYGZhy{}03   0.0   0.0    42    33  
2015\PYGZhy{}01\PYGZhy{}04   0.0   0.0    56    41  
2015\PYGZhy{}01\PYGZhy{}05   0.0   0.0    49    21  
\end{sphinxVerbatim}

\sphinxAtStartPar
One would assume that when it is a rainy day, people would use more cabs. so let us seperate based on precipitation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rained} \PYG{o}{=} \PYG{n}{merged\PYGZus{}df}\PYG{p}{[}\PYG{n}{merged\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PRCP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{no\PYGZus{}rain} \PYG{o}{=} \PYG{n}{merged\PYGZus{}df}\PYG{p}{[}\PYG{n}{merged\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PRCP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{average uber rides on a rainy day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{rained}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cabs\PYGZus{}taken}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{average uber rides on a dry day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{no\PYGZus{}rain}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cabs\PYGZus{}taken}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
average uber rides on a rainy day
99837.29411764706
average uber rides on a dry day
102846.30357142857
\end{sphinxVerbatim}

\sphinxAtStartPar
ouch! it looks like the average new yorker doesn’t mind getting wet, or they take a cab any day…using a regression plot we can see it more clear

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{regplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{merged\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PRCP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cabs\PYGZus{}taken}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}PRCP\PYGZsq{}, ylabel=\PYGZsq{}cabs\PYGZus{}taken\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{external_data_27_1}.png}

\sphinxAtStartPar
Ok, here we see that it might just be because a lot of days are dry and the dataset is skewed. Not reliable info.What about temperatures, can we see a difference if the lowest temperature changes?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{regplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{merged\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TMIN}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cabs\PYGZus{}taken}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}TMIN\PYGZsq{}, ylabel=\PYGZsq{}cabs\PYGZus{}taken\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{external_data_29_1}.png}

\sphinxAtStartPar
Appearantly when the temperature lowers, yorkers seem to be taking more cab rides. So global warming might be disastrous for capitalism after all?


\chapter{Feature Enhancing}
\label{\detokenize{c5_data_exploration/feature_engineering:feature-enhancing}}\label{\detokenize{c5_data_exploration/feature_engineering::doc}}
\sphinxAtStartPar
This rather simple notebook is a small illustration how feature enhancing might work in specific cases, we have a dataset containing cars and their fuel efficiency. What we will try to illustrate here is that sometimes combinations or formulas using the original data might display patterns not visible with the previous data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{spearmanr}
\end{sphinxVerbatim}

\sphinxAtStartPar
we load the mpg dataset and have a look at it.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mpg} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mpg}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    mpg  cylinders  displacement  horsepower  weight  acceleration  \PYGZbs{}
0  18.0          8         307.0       130.0    3504          12.0   
1  15.0          8         350.0       165.0    3693          11.5   
2  18.0          8         318.0       150.0    3436          11.0   
3  16.0          8         304.0       150.0    3433          12.0   
4  17.0          8         302.0       140.0    3449          10.5   

   model\PYGZus{}year origin                       name  
0          70    usa  chevrolet chevelle malibu  
1          70    usa          buick skylark 320  
2          70    usa         plymouth satellite  
3          70    usa              amc rebel sst  
4          70    usa                ford torino  
\end{sphinxVerbatim}

\sphinxAtStartPar
We’ll try to explore our dataset by printing out some regression plots between features of the car and the mileage per gallon.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{regplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{acceleration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{corr}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{n}{spearmanr}\PYG{p}{(}\PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{acceleration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{acceleration correlation: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{o}{*}\PYG{n+nb}{round}\PYG{p}{(}\PYG{n}{corr}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
acceleration correlation: 43.87\PYGZpc{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{feature_engineering_6_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{regplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weight}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{corr}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{n}{spearmanr}\PYG{p}{(}\PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weight}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weight correlation: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{o}{*}\PYG{n+nb}{round}\PYG{p}{(}\PYG{n}{corr}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
weight correlation: \PYGZhy{}87.49\PYGZpc{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{feature_engineering_7_1}.png}

\sphinxAtStartPar
We can see that the acceleration has a positive influence on the miles per gallon, whilst the weight has a negative influence, what about the acceleration per weight?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{new\PYGZus{}feature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{acceleration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{/}\PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weight}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mpg}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    mpg  cylinders  displacement  horsepower  weight  acceleration  \PYGZbs{}
0  18.0          8         307.0       130.0    3504          12.0   
1  15.0          8         350.0       165.0    3693          11.5   
2  18.0          8         318.0       150.0    3436          11.0   
3  16.0          8         304.0       150.0    3433          12.0   
4  17.0          8         302.0       140.0    3449          10.5   

   model\PYGZus{}year origin                       name  new\PYGZus{}feature  
0          70    usa  chevrolet chevelle malibu     0.003425  
1          70    usa          buick skylark 320     0.003114  
2          70    usa         plymouth satellite     0.003201  
3          70    usa              amc rebel sst     0.003495  
4          70    usa                ford torino     0.003044  
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{regplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{new\PYGZus{}feature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{corr}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{n}{spearmanr}\PYG{p}{(}\PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{mpg}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{new\PYGZus{}feature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{new feature correlation: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{o}{*}\PYG{n+nb}{round}\PYG{p}{(}\PYG{n}{corr}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
new feature correlation: 84.19\PYGZpc{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{feature_engineering_11_1}.png}

\sphinxAtStartPar
It seems we are not able to create a new feature with even more correlation, not every story has to be a success. We can report this to our boss and explain the results.


\chapter{Cluster analysis}
\label{\detokenize{c5_data_exploration/cluster_analysis:cluster-analysis}}\label{\detokenize{c5_data_exploration/cluster_analysis::doc}}
\sphinxAtStartPar
Before starting this notebook I would like to state that what is explained here will be elaborated later in the course and might look complicated at this point. If you do not feel familiar with these concepts that is perfectly fine.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\end{sphinxVerbatim}

\sphinxAtStartPar
We will load a digits dataset from sklearn, the machine learning library, these are 8x8 pixel images showing handwritten digits with the correct answer.In the dataset there are 1797 images giving the dataset a dimension of (1797, 8*8)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{load\PYGZus{}digits}
\PYG{n}{digits} \PYG{o}{=} \PYG{n}{load\PYGZus{}digits}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{digits}\PYG{o}{.}\PYG{n}{data}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(1797, 64)
\end{sphinxVerbatim}

\sphinxAtStartPar
Before we start, let’s print out a few of them, the following cell will do that.
Again, plotting is not yet seen, so the following cells might be overwhelming.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axes} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{16}\PYG{p}{)}\PYG{p}{,}
                         \PYG{n}{subplot\PYGZus{}kw}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{xticks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{yticks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                         \PYG{n}{gridspec\PYGZus{}kw}\PYG{o}{=}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{hspace}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{wspace}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{axes}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{digits}\PYG{o}{.}\PYG{n}{images}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{binary}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{interpolation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nearest}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{digits}\PYG{o}{.}\PYG{n}{target}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
            \PYG{n}{transform}\PYG{o}{=}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{transAxes}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{cluster_analysis_5_0}.png}

\sphinxAtStartPar
In cluster analysis we will try to figure out clusters within the dataset, keep in mind that these cluster are constructed without knowning the correct answer.
Here we use the Isomap algorithm to create clusters, by using fit and transform methods we can create the clusters

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{KMeans}
\PYG{n}{kmeans} \PYG{o}{=} \PYG{n}{KMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{clusters} \PYG{o}{=} \PYG{n}{kmeans}\PYG{o}{.}\PYG{n}{fit\PYGZus{}predict}\PYG{p}{(}\PYG{n}{digits}\PYG{o}{.}\PYG{n}{data}\PYG{p}{)}
\PYG{n}{kmeans}\PYG{o}{.}\PYG{n}{cluster\PYGZus{}centers\PYGZus{}}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(10, 64)
\end{sphinxVerbatim}

\sphinxAtStartPar
Now that the algorithm seperated the dataset into 10 clusters, we can ask it to print the center of each cluster.This gives us an idea how the average digit in that cluster looks like.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{centers} \PYG{o}{=} \PYG{n}{kmeans}\PYG{o}{.}\PYG{n}{cluster\PYGZus{}centers\PYGZus{}}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{axi}\PYG{p}{,} \PYG{n}{center} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{,} \PYG{n}{centers}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{axi}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{xticks}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{n}{yticks}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{axi}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{center}\PYG{p}{,} \PYG{n}{interpolation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nearest}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{binary}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{cluster_analysis_9_0}.png}

\sphinxAtStartPar
Those look similar to the actual numbers, confirming that arabic numbers have good visual seperation inbetween.Aside from the centers we can also print a few examples from the clusters.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axes} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{,}
                         \PYG{n}{subplot\PYGZus{}kw}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{xticks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{yticks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{p}{[}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
                         \PYG{n}{gridspec\PYGZus{}kw}\PYG{o}{=}\PYG{n+nb}{dict}\PYG{p}{(}\PYG{n}{hspace}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{wspace}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{axes}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{digits}\PYG{o}{.}\PYG{n}{images}\PYG{p}{[}\PYG{n}{clusters}\PYG{o}{==}\PYG{n}{i}\PYG{o}{\PYGZpc{}}\PYG{k}{10}][int(i/10)], cmap=\PYGZsq{}binary\PYGZsq{}, interpolation=\PYGZsq{}nearest\PYGZsq{})
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{i}\PYG{o}{\PYGZpc{}}\PYG{k}{10}),
            \PYG{n}{transform}\PYG{o}{=}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{transAxes}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{cluster_analysis_11_0}.png}

\sphinxAtStartPar
You can see that the cluster number does not match the actual number, that’s because our algorithm does not understand which numbers there are.It does however understand the differences between the numbers!
This technique can also be used for other datasets where no outcome is given, but we would like to separate our dataset into clusters.

\sphinxAtStartPar
To make this more visible we will use another example of a dataset about the leafs of 3 types of iris flowers.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{iris\PYGZus{}df} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{iris\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   sepal\PYGZus{}length  sepal\PYGZus{}width  petal\PYGZus{}length  petal\PYGZus{}width species
0           5.1          3.5           1.4          0.2  setosa
1           4.9          3.0           1.4          0.2  setosa
2           4.7          3.2           1.3          0.2  setosa
3           4.6          3.1           1.5          0.2  setosa
4           5.0          3.6           1.4          0.2  setosa
\end{sphinxVerbatim}

\sphinxAtStartPar
What we could do here is ask the algorithm to create 3 clusters of records, as the dataset contains 3 types of iris flowers.

\sphinxAtStartPar
We do not supply the algorithm with the information of the species, yet it has to figure out by itself how to seperate the records.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kmeans} \PYG{o}{=} \PYG{n}{KMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{iris\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cluster}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{kmeans}\PYG{o}{.}\PYG{n}{fit\PYGZus{}predict}\PYG{p}{(}\PYG{n}{iris\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{species}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{iris\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   sepal\PYGZus{}length  sepal\PYGZus{}width  petal\PYGZus{}length  petal\PYGZus{}width species  cluster
0           5.1          3.5           1.4          0.2  setosa        1
1           4.9          3.0           1.4          0.2  setosa        1
2           4.7          3.2           1.3          0.2  setosa        1
3           4.6          3.1           1.5          0.2  setosa        1
4           5.0          3.6           1.4          0.2  setosa        1
\end{sphinxVerbatim}

\sphinxAtStartPar
We can see our data now has an additional feature cluster which contains either 0, 1 or 2.
If the clustering has been performed as expected, the clusters should coincide with the species.
Using a plot we can find out.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{iris\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sepal\PYGZus{}length}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{petal\PYGZus{}width}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{species}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}sepal\PYGZus{}length\PYGZsq{}, ylabel=\PYGZsq{}petal\PYGZus{}width\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{cluster_analysis_17_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{iris\PYGZus{}df}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sepal\PYGZus{}length}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{petal\PYGZus{}width}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cluster}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}sepal\PYGZus{}length\PYGZsq{}, ylabel=\PYGZsq{}petal\PYGZus{}width\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{cluster_analysis_18_1}.png}

\sphinxAtStartPar
For some reason seaborn thinks it is useful to change color scheme, yet you can see that there is an uncanny similarity between the clusters and the species, the algorithm was succesful in finding the different species.

\sphinxAtStartPar
Without giving the information we were able to cluster the different species of iris flowers yet we have no idea which cluster belongs to which species.
It is the reasers responsibility to take conclusion in what the different clusters mean!


\chapter{VIF: Variance Inflation Factor}
\label{\detokenize{c5_data_exploration/variance_inflation:vif-variance-inflation-factor}}\label{\detokenize{c5_data_exploration/variance_inflation::doc}}
\sphinxAtStartPar
in this notebook we will investigate the variance inflation which can occur in a dataset. As an example here, we will use the ‘Mile Per Gallon’ dataset contianing a set of cars and their fuel efficiency. Some columns in the dataset might

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{from} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{stats}\PYG{n+nn}{.}\PYG{n+nn}{outliers\PYGZus{}influence} \PYG{k+kn}{import} \PYG{n}{variance\PYGZus{}inflation\PYGZus{}factor}
\PYG{n}{mpg} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ModuleNotFoundError}\PYG{g+gWhitespace}{                       }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}13569}\PYG{o}{/}\PYG{l+m+mf}{3881140987.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{1} \PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{2} \PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{3} \PYG{k+kn}{from} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{stats}\PYG{n+nn}{.}\PYG{n+nn}{outliers\PYGZus{}influence} \PYG{k+kn}{import} \PYG{n}{variance\PYGZus{}inflation\PYGZus{}factor}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{4} \PYG{n}{mpg} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+ne}{ModuleNotFoundError}: No module named \PYGZsq{}statsmodels\PYGZsq{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mpg}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    mpg  cylinders  displacement  horsepower  weight  acceleration  \PYGZbs{}
0  18.0          8         307.0       130.0    3504          12.0   
1  15.0          8         350.0       165.0    3693          11.5   
2  18.0          8         318.0       150.0    3436          11.0   
3  16.0          8         304.0       150.0    3433          12.0   
4  17.0          8         302.0       140.0    3449          10.5   

   model\PYGZus{}year origin                       name  
0          70    usa  chevrolet chevelle malibu  
1          70    usa          buick skylark 320  
2          70    usa         plymouth satellite  
3          70    usa              amc rebel sst  
4          70    usa                ford torino  
\end{sphinxVerbatim}

\sphinxAtStartPar
as you can see, we also imported a function ‘variance\_inflation\_factor’ which will help us calculate this, more information can be found on \sphinxhref{https://en.wikipedia.org/wiki/Variance\_inflation\_factor}{wikipedia}.

\sphinxAtStartPar
to use the function, we refer to the \sphinxhref{https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers\_influence.variance\_inflation\_factor.html}{documentation}. The function is a bit stubborn and requires the following:
\begin{itemize}
\item {} 
\sphinxAtStartPar
only numerical values (so we to drop the categories)

\item {} 
\sphinxAtStartPar
no nan values (dropping nans)

\item {} 
\sphinxAtStartPar
as a numpy array instead of a pandas dataframe

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cols\PYGZus{}to\PYGZus{}keep} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cylinders}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{displacement}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horsepower}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weight}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{acceleration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model\PYGZus{}year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{vif\PYGZus{}compatible\PYGZus{}df} \PYG{o}{=} \PYG{n}{mpg}\PYG{p}{[}\PYG{n}{cols\PYGZus{}to\PYGZus{}keep}\PYG{p}{]}
\PYG{n}{vif\PYGZus{}compatible\PYGZus{}df} \PYG{o}{=} \PYG{n}{vif\PYGZus{}compatible\PYGZus{}df}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{vif\PYGZus{}compatible\PYGZus{}df} \PYG{o}{=} \PYG{n}{vif\PYGZus{}compatible\PYGZus{}df}\PYG{o}{.}\PYG{n}{values}
\PYG{n}{vif\PYGZus{}compatible\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[   8. ,  307. ,  130. , 3504. ,   12. ,   70. ],
       [   8. ,  350. ,  165. , 3693. ,   11.5,   70. ],
       [   8. ,  318. ,  150. , 3436. ,   11. ,   70. ],
       ...,
       [   4. ,  135. ,   84. , 2295. ,   11.6,   82. ],
       [   4. ,  120. ,   79. , 2625. ,   18.6,   82. ],
       [   4. ,  119. ,   82. , 2720. ,   19.4,   82. ]])
\end{sphinxVerbatim}

\sphinxAtStartPar
this looks a lot different! we don’t know anymore what all of that means, but the computer does, now we run it through the function.
Notice how we have to specify a specific column, the resulting inflation factor is that for the chosen column

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} we pick column 0 which is \PYGZsq{}cylinders\PYGZsq{} according to cols\PYGZus{}to\PYGZus{}keep}
\PYG{n}{variance\PYGZus{}inflation\PYGZus{}factor}\PYG{p}{(}\PYG{n}{vif\PYGZus{}compatible\PYGZus{}df}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
115.97777160980726
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{idx}\PYG{p}{,} \PYG{n}{col} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{cols\PYGZus{}to\PYGZus{}keep}\PYG{p}{)}\PYG{p}{:}
  \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{col} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{: }\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{variance\PYGZus{}inflation\PYGZus{}factor}\PYG{p}{(}\PYG{n}{vif\PYGZus{}compatible\PYGZus{}df}\PYG{p}{,} \PYG{n}{idx}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
cylinders: 	115.97777160980726
displacement: 	86.48595590611876
horsepower: 	60.25657462146676
weight: 	137.4717563697324
acceleration: 	69.40087667701684
model\PYGZus{}year: 	109.3200159587966
\end{sphinxVerbatim}


\section{TODO}
\label{\detokenize{c5_data_exploration/variance_inflation:todo}}
\sphinxAtStartPar
The variance inflation gives a numerical value to how little variation there is between one column and the others in a dataset, you will see how the numbers will gradually go down as you remove more and more columns.This way we have a quantifyable method of removing data from our dataset in case there is too much ‘duplicate’ information.There is no real cut\sphinxhyphen{}off value that specifies of a column should or should not be removed, so make sure you can argument your decision.
\begin{itemize}
\item {} 
\sphinxAtStartPar
experiment with removing columns in the cols\_to\_keep list

\item {} 
\sphinxAtStartPar
What do you think would be the ideal dataset here? we would like to predict the fuel economy (mpg) of a car.

\end{itemize}


\chapter{Principle Component Analysis}
\label{\detokenize{c5_data_exploration/principle_component_analysis:principle-component-analysis}}\label{\detokenize{c5_data_exploration/principle_component_analysis::doc}}
\sphinxAtStartPar
In this notebook we will not try to remove data from our dataset, but transform the variation in our features (columns) into less features.We will do this using the concept of PCA (principle component analysis).
The dataset we will be using here is about the dimensions of iris flowers, in total 150 flowers were measured of 3 species.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{decomposition} \PYG{k+kn}{import} \PYG{n}{PCA}
\PYG{n}{iris} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
you can see that we imported a function PCA from sklearn, this will do the calculations for us, but we still need to specify some parameters.Before we do that, let us use the first 2 columns of the dataset to plot a scatter and see if we can distinguish the different species of flowers.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{iris}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   sepal\PYGZus{}length  sepal\PYGZus{}width  petal\PYGZus{}length  petal\PYGZus{}width species
0           5.1          3.5           1.4          0.2  setosa
1           4.9          3.0           1.4          0.2  setosa
2           4.7          3.2           1.3          0.2  setosa
3           4.6          3.1           1.5          0.2  setosa
4           5.0          3.6           1.4          0.2  setosa
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{iris}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sepal\PYGZus{}length}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{iris}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sepal\PYGZus{}width}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{n}{iris}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{species}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:xlabel=\PYGZsq{}sepal\PYGZus{}length\PYGZsq{}, ylabel=\PYGZsq{}sepal\PYGZus{}width\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{principle_component_analysis_4_1}.png}

\sphinxAtStartPar
That already looks pretty good, but versicolor and virginica are still hard to differentiate. Let’s see if we can compress the variation of all 4 columns into 2 axi.We do this by creating a PCA transformer and specifying we want only 2 output components

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pca} \PYG{o}{=} \PYG{n}{PCA}\PYG{p}{(}\PYG{n}{n\PYGZus{}components}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
We also need to prepare our dataframe, we do this by only dropping our outcome (that which we do not need for the transform)

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{X} \PYG{o}{=} \PYG{n}{iris}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{species}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{X}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   sepal\PYGZus{}length  sepal\PYGZus{}width  petal\PYGZus{}length  petal\PYGZus{}width
0           5.1          3.5           1.4          0.2
1           4.9          3.0           1.4          0.2
2           4.7          3.2           1.3          0.2
3           4.6          3.1           1.5          0.2
4           5.0          3.6           1.4          0.2
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{iris\PYGZus{}pca} \PYG{o}{=} \PYG{n}{pca}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{iris\PYGZus{}pca}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PC1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PC2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
          PC1       PC2
0   \PYGZhy{}2.684126  0.319397
1   \PYGZhy{}2.714142 \PYGZhy{}0.177001
2   \PYGZhy{}2.888991 \PYGZhy{}0.144949
3   \PYGZhy{}2.745343 \PYGZhy{}0.318299
4   \PYGZhy{}2.728717  0.326755
..        ...       ...
145  1.944110  0.187532
146  1.527167 \PYGZhy{}0.375317
147  1.764346  0.078859
148  1.900942  0.116628
149  1.390189 \PYGZhy{}0.282661

[150 rows x 2 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Running it through the PCA transformer using the fit\_transform function gives us a numpy 2 dimensional array (which is similar to a pandas dataframe) with 2 columns.When inserted into a scatter plot they show us (nearly) all variance of 4 columns compressed into a 2 dimensional plot.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{scatterplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{iris\PYGZus{}pca}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{iris\PYGZus{}pca}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{n}{iris}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{species}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{principle_component_analysis_11_1}.png}


\section{TODO}
\label{\detokenize{c5_data_exploration/principle_component_analysis:todo}}
\sphinxAtStartPar
it is clear that this function is very potent concerning data visualisation, do you think you can improve on the mpg dataset?
\begin{itemize}
\item {} 
\sphinxAtStartPar
experiment with the PCA transformer using the mpg dataset

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mpg} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mpg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{mpg}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    mpg  cylinders  displacement  horsepower  weight  acceleration  \PYGZbs{}
0  18.0          8         307.0       130.0    3504          12.0   
1  15.0          8         350.0       165.0    3693          11.5   
2  18.0          8         318.0       150.0    3436          11.0   
3  16.0          8         304.0       150.0    3433          12.0   
4  17.0          8         302.0       140.0    3449          10.5   

   model\PYGZus{}year origin                       name  
0          70    usa  chevrolet chevelle malibu  
1          70    usa          buick skylark 320  
2          70    usa         plymouth satellite  
3          70    usa              amc rebel sst  
4          70    usa                ford torino  
\end{sphinxVerbatim}


\part{6. Machine Learning}


\chapter{Machine Learning}
\label{\detokenize{c6_machine_learning/introduction:machine-learning}}\label{\detokenize{c6_machine_learning/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\part{7. Case Studies}


\chapter{Case study: Olist}
\label{\detokenize{c7_case_studies/Olist:case-study-olist}}\label{\detokenize{c7_case_studies/Olist::doc}}
\sphinxAtStartPar
In this case study we will create an overview on how a generic Data Analysis study on a dataset works.

\sphinxAtStartPar
The case study is divided into several parts:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Goals

\item {} 
\sphinxAtStartPar
Parsing

\item {} 
\sphinxAtStartPar
Preparation (cleaning)

\item {} 
\sphinxAtStartPar
Processing

\item {} 
\sphinxAtStartPar
Exploration

\item {} 
\sphinxAtStartPar
Visualization

\item {} 
\sphinxAtStartPar
Conclusion

\end{itemize}


\section{Goals}
\label{\detokenize{c7_case_studies/Olist:goals}}
\sphinxAtStartPar
In this section we will state the goals we try to obtain by analyzing this dataset. Here are the questions that our customer had:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Can we predict prices for products?

\item {} 
\sphinxAtStartPar
Do customers behave predictable, can we recommend specific items to specific customers?

\item {} 
\sphinxAtStartPar
Sellers with more/better reviews seem to do better, can you quantify this?

\item {} 
\sphinxAtStartPar
Are there items with a specific time pattern?

\item {} 
\sphinxAtStartPar
Are products related to geographical information?

\item {} 
\sphinxAtStartPar
Is there anything else remarkable in our data?

\end{itemize}

\sphinxAtStartPar
We’ll (try to) keep these question in mind when performing the case study.


\section{Parsing}
\label{\detokenize{c7_case_studies/Olist:parsing}}
\sphinxAtStartPar
we start out by importing all necessary libraries

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{json}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{set\PYGZus{}matplotlib\PYGZus{}formats}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\PYG{n}{set\PYGZus{}matplotlib\PYGZus{}formats}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{svg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/tmp/ipykernel\PYGZus{}14147/4057771804.py:10: DeprecationWarning: `set\PYGZus{}matplotlib\PYGZus{}formats` is deprecated since IPython 7.23, directly use `matplotlib\PYGZus{}inline.backend\PYGZus{}inline.set\PYGZus{}matplotlib\PYGZus{}formats()`
  set\PYGZus{}matplotlib\PYGZus{}formats(\PYGZsq{}svg\PYGZsq{})
\end{sphinxVerbatim}

\sphinxAtStartPar
in order to download datasets from kaggle, we need an API key to access their API, we’ll make that here

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{mkdir}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/root/.kaggle/kaggle.json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}
        \PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{username}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lorenzf}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{7a44a9e99b27e796177d793a3d85b8cf}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{\PYGZcb{}}
        \PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{PermissionError}\PYG{g+gWhitespace}{                           }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}14147}\PYG{o}{/}\PYG{l+m+mf}{3113012040.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{1} \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{2}     \PYG{n}{os}\PYG{o}{.}\PYG{n}{mkdir}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{3} 
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{4} \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/root/.kaggle/kaggle.json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{5}     \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}

\PYG{n+ne}{PermissionError}: [Errno 13] Permission denied: \PYGZsq{}/root/.kaggle\PYGZsq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
now we can import kaggle too and download the datasets

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kaggle}
\PYG{n}{kaggle}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{dataset\PYGZus{}download\PYGZus{}files}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{olistbr/brazilian\PYGZhy{}ecommerce}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{unzip}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{kaggle}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{dataset\PYGZus{}download\PYGZus{}files}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{olistbr/marketing\PYGZhy{}funnel\PYGZhy{}olist}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{unzip}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run \PYGZsq{}chmod 600 /root/.kaggle/kaggle.json\PYGZsq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
the csv files are now in the ‘./data’ folder, we can now read them using pandas, here is the list of all csv files in our folder

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{os}\PYG{o}{.}\PYG{n}{listdir}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}olist\PYGZus{}order\PYGZus{}reviews\PYGZus{}dataset.csv\PYGZsq{},
 \PYGZsq{}olist\PYGZus{}order\PYGZus{}items\PYGZus{}dataset.csv\PYGZsq{},
 \PYGZsq{}product\PYGZus{}category\PYGZus{}name\PYGZus{}translation.csv\PYGZsq{},
 \PYGZsq{}olist\PYGZus{}products\PYGZus{}dataset.csv\PYGZsq{},
 \PYGZsq{}olist\PYGZus{}closed\PYGZus{}deals\PYGZus{}dataset.csv\PYGZsq{},
 \PYGZsq{}olist\PYGZus{}order\PYGZus{}payments\PYGZus{}dataset.csv\PYGZsq{},
 \PYGZsq{}olist\PYGZus{}marketing\PYGZus{}qualified\PYGZus{}leads\PYGZus{}dataset.csv\PYGZsq{},
 \PYGZsq{}olist\PYGZus{}sellers\PYGZus{}dataset.csv\PYGZsq{},
 \PYGZsq{}olist\PYGZus{}customers\PYGZus{}dataset.csv\PYGZsq{},
 \PYGZsq{}olist\PYGZus{}orders\PYGZus{}dataset.csv\PYGZsq{},
 \PYGZsq{}olist\PYGZus{}geolocation\PYGZus{}dataset.csv\PYGZsq{}]
\end{sphinxVerbatim}

\sphinxAtStartPar
we will now parse interesting dataframes.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{customers} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/olist\PYGZus{}customers\PYGZus{}dataset.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{customers}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{customers}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
shape: (99441, 5)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                        customer\PYGZus{}id  ... customer\PYGZus{}state
0  06b8999e2fba1a1fbc88172c00ba8bc7  ...             SP
1  18955e83d337fd6b2def6b18a428ac77  ...             SP
2  4e7b3e00288586ebd08712fdd0374a03  ...             SP
3  b2b6027bc5c5109e529d4dc6358b12c3  ...             SP
4  4f2d8ab171c80ec8364f7c12e35b23ad  ...             SP

[5 rows x 5 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sellers} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/olist\PYGZus{}sellers\PYGZus{}dataset.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{sellers}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{sellers}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
shape: (3095, 4)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                          seller\PYGZus{}id  ...  seller\PYGZus{}state
0  3442f8959a84dea7ee197c632cb2df15  ...            SP
1  d1b65fc7debc3361ea86b5f14c68d2e2  ...            SP
2  ce3ad9de960102d0677a81f5d0bb7b2d  ...            RJ
3  c0f3eea2e14555b6faeea3dd58c1b1c3  ...            SP
4  51a04a8a6bdcb23deccc82b0b80742cf  ...            SP

[5 rows x 4 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/olist\PYGZus{}products\PYGZus{}dataset.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{products}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{products}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
shape: (32951, 9)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                         product\PYGZus{}id  ... product\PYGZus{}width\PYGZus{}cm
0  1e9e8ef04dbcff4541ed26657ea517e5  ...             14.0
1  3aa071139cb16b67ca9e5dea641aaa2f  ...             20.0
2  96bd76ec8810374ed1b65e291975717f  ...             15.0
3  cef67bcfe19066a932b7673e239eb23d  ...             26.0
4  9dc1a7de274444849c219cff195d0b71  ...             13.0

[5 rows x 9 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{translation} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/product\PYGZus{}category\PYGZus{}name\PYGZus{}translation.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{translation}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{translation}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
shape: (71, 2)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    product\PYGZus{}category\PYGZus{}name product\PYGZus{}category\PYGZus{}name\PYGZus{}english
0            beleza\PYGZus{}saude                 health\PYGZus{}beauty
1  informatica\PYGZus{}acessorios         computers\PYGZus{}accessories
2              automotivo                          auto
3         cama\PYGZus{}mesa\PYGZus{}banho                bed\PYGZus{}bath\PYGZus{}table
4        moveis\PYGZus{}decoracao               furniture\PYGZus{}decor
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{orders} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/olist\PYGZus{}order\PYGZus{}items\PYGZus{}dataset.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{orders}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{orders}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
shape: (112650, 7)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                           order\PYGZus{}id  order\PYGZus{}item\PYGZus{}id  ...   price freight\PYGZus{}value
0  00010242fe8c5a6d1ba2dd792cb16214              1  ...   58.90         13.29
1  00018f77f2f0320c557190d7a144bdd3              1  ...  239.90         19.93
2  000229ec398224ef6ca0657da4fc703e              1  ...  199.00         17.87
3  00024acbcdf0a6daa1e931b038114c75              1  ...   12.99         12.79
4  00042b26cf59d7ce69dfabb4e55b4fd9              1  ...  199.90         18.14

[5 rows x 7 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{order\PYGZus{}reviews} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/olist\PYGZus{}order\PYGZus{}reviews\PYGZus{}dataset.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{order\PYGZus{}reviews}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{order\PYGZus{}reviews}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
shape: (99224, 7)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                          review\PYGZus{}id  ... review\PYGZus{}answer\PYGZus{}timestamp
0  7bc2406110b926393aa56f80a40eba40  ...     2018\PYGZhy{}01\PYGZhy{}18 21:46:59
1  80e641a11e56f04c1ad469d5645fdfde  ...     2018\PYGZhy{}03\PYGZhy{}11 03:05:13
2  228ce5500dc1d8e020d8d1322874b6f0  ...     2018\PYGZhy{}02\PYGZhy{}18 14:36:24
3  e64fb393e7b32834bb789ff8bb30750e  ...     2017\PYGZhy{}04\PYGZhy{}21 22:02:06
4  f7c4243c7fe1938f181bec41a392bdeb  ...     2018\PYGZhy{}03\PYGZhy{}02 10:26:53

[5 rows x 7 columns]
\end{sphinxVerbatim}


\section{Preparation}
\label{\detokenize{c7_case_studies/Olist:preparation}}
\sphinxAtStartPar
here we perform tasks to prepare the data in a more pleasing format.


\subsection{Data Types}
\label{\detokenize{c7_case_studies/Olist:data-types}}
\sphinxAtStartPar
Before we do anything with our data, it is good to see if our data types are in order

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{customers}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 99441 entries, 0 to 99440
Data columns (total 5 columns):
 \PYGZsh{}   Column                    Non\PYGZhy{}Null Count  Dtype 
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}                    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{} 
 0   customer\PYGZus{}id               99441 non\PYGZhy{}null  object
 1   customer\PYGZus{}unique\PYGZus{}id        99441 non\PYGZhy{}null  object
 2   customer\PYGZus{}zip\PYGZus{}code\PYGZus{}prefix  99441 non\PYGZhy{}null  int64 
 3   customer\PYGZus{}city             99441 non\PYGZhy{}null  object
 4   customer\PYGZus{}state            99441 non\PYGZhy{}null  object
dtypes: int64(1), object(4)
memory usage: 3.8+ MB
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{customers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{customer\PYGZus{}city}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{customers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{customer\PYGZus{}city}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{customers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{customer\PYGZus{}state}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{customers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{customer\PYGZus{}state}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{customers}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 99441 entries, 0 to 99440
Data columns (total 5 columns):
 \PYGZsh{}   Column                    Non\PYGZhy{}Null Count  Dtype   
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}                    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   
 0   customer\PYGZus{}id               99441 non\PYGZhy{}null  object  
 1   customer\PYGZus{}unique\PYGZus{}id        99441 non\PYGZhy{}null  object  
 2   customer\PYGZus{}zip\PYGZus{}code\PYGZus{}prefix  99441 non\PYGZhy{}null  int64   
 3   customer\PYGZus{}city             99441 non\PYGZhy{}null  category
 4   customer\PYGZus{}state            99441 non\PYGZhy{}null  category
dtypes: category(2), int64(1), object(2)
memory usage: 2.7+ MB
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sellers}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 3095 entries, 0 to 3094
Data columns (total 4 columns):
 \PYGZsh{}   Column                  Non\PYGZhy{}Null Count  Dtype 
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}                  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{} 
 0   seller\PYGZus{}id               3095 non\PYGZhy{}null   object
 1   seller\PYGZus{}zip\PYGZus{}code\PYGZus{}prefix  3095 non\PYGZhy{}null   int64 
 2   seller\PYGZus{}city             3095 non\PYGZhy{}null   object
 3   seller\PYGZus{}state            3095 non\PYGZhy{}null   object
dtypes: int64(1), object(3)
memory usage: 96.8+ KB
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sellers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{seller\PYGZus{}city}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{sellers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{seller\PYGZus{}city}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sellers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{seller\PYGZus{}state}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{sellers}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{seller\PYGZus{}state}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sellers}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 3095 entries, 0 to 3094
Data columns (total 4 columns):
 \PYGZsh{}   Column                  Non\PYGZhy{}Null Count  Dtype   
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}                  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   
 0   seller\PYGZus{}id               3095 non\PYGZhy{}null   object  
 1   seller\PYGZus{}zip\PYGZus{}code\PYGZus{}prefix  3095 non\PYGZhy{}null   int64   
 2   seller\PYGZus{}city             3095 non\PYGZhy{}null   category
 3   seller\PYGZus{}state            3095 non\PYGZhy{}null   category
dtypes: category(2), int64(1), object(1)
memory usage: 83.1+ KB
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 32951 entries, 0 to 32950
Data columns (total 9 columns):
 \PYGZsh{}   Column                      Non\PYGZhy{}Null Count  Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}                      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   product\PYGZus{}id                  32951 non\PYGZhy{}null  object 
 1   product\PYGZus{}category\PYGZus{}name       32341 non\PYGZhy{}null  object 
 2   product\PYGZus{}name\PYGZus{}lenght         32341 non\PYGZhy{}null  float64
 3   product\PYGZus{}description\PYGZus{}lenght  32341 non\PYGZhy{}null  float64
 4   product\PYGZus{}photos\PYGZus{}qty          32341 non\PYGZhy{}null  float64
 5   product\PYGZus{}weight\PYGZus{}g            32949 non\PYGZhy{}null  float64
 6   product\PYGZus{}length\PYGZus{}cm           32949 non\PYGZhy{}null  float64
 7   product\PYGZus{}height\PYGZus{}cm           32949 non\PYGZhy{}null  float64
 8   product\PYGZus{}width\PYGZus{}cm            32949 non\PYGZhy{}null  float64
dtypes: float64(7), object(2)
memory usage: 2.3+ MB
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{products}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{products}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 32951 entries, 0 to 32950
Data columns (total 9 columns):
 \PYGZsh{}   Column                      Non\PYGZhy{}Null Count  Dtype   
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}                      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   
 0   product\PYGZus{}id                  32951 non\PYGZhy{}null  object  
 1   product\PYGZus{}category\PYGZus{}name       32341 non\PYGZhy{}null  category
 2   product\PYGZus{}name\PYGZus{}lenght         32341 non\PYGZhy{}null  float64 
 3   product\PYGZus{}description\PYGZus{}lenght  32341 non\PYGZhy{}null  float64 
 4   product\PYGZus{}photos\PYGZus{}qty          32341 non\PYGZhy{}null  float64 
 5   product\PYGZus{}weight\PYGZus{}g            32949 non\PYGZhy{}null  float64 
 6   product\PYGZus{}length\PYGZus{}cm           32949 non\PYGZhy{}null  float64 
 7   product\PYGZus{}height\PYGZus{}cm           32949 non\PYGZhy{}null  float64 
 8   product\PYGZus{}width\PYGZus{}cm            32949 non\PYGZhy{}null  float64 
dtypes: category(1), float64(7), object(1)
memory usage: 2.0+ MB
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{orders}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 112650 entries, 0 to 112649
Data columns (total 7 columns):
 \PYGZsh{}   Column               Non\PYGZhy{}Null Count   Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}               \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   order\PYGZus{}id             112650 non\PYGZhy{}null  object 
 1   order\PYGZus{}item\PYGZus{}id        112650 non\PYGZhy{}null  int64  
 2   product\PYGZus{}id           112650 non\PYGZhy{}null  object 
 3   seller\PYGZus{}id            112650 non\PYGZhy{}null  object 
 4   shipping\PYGZus{}limit\PYGZus{}date  112650 non\PYGZhy{}null  object 
 5   price                112650 non\PYGZhy{}null  float64
 6   freight\PYGZus{}value        112650 non\PYGZhy{}null  float64
dtypes: float64(2), int64(1), object(4)
memory usage: 6.0+ MB
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{orders}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shipping\PYGZus{}limit\PYGZus{}date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{n}{orders}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shipping\PYGZus{}limit\PYGZus{}date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{orders}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 112650 entries, 0 to 112649
Data columns (total 7 columns):
 \PYGZsh{}   Column               Non\PYGZhy{}Null Count   Dtype         
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}               \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}         
 0   order\PYGZus{}id             112650 non\PYGZhy{}null  object        
 1   order\PYGZus{}item\PYGZus{}id        112650 non\PYGZhy{}null  int64         
 2   product\PYGZus{}id           112650 non\PYGZhy{}null  object        
 3   seller\PYGZus{}id            112650 non\PYGZhy{}null  object        
 4   shipping\PYGZus{}limit\PYGZus{}date  112650 non\PYGZhy{}null  datetime64[ns]
 5   price                112650 non\PYGZhy{}null  float64       
 6   freight\PYGZus{}value        112650 non\PYGZhy{}null  float64       
dtypes: datetime64[ns](1), float64(2), int64(1), object(3)
memory usage: 6.0+ MB
\end{sphinxVerbatim}


\subsection{Missing values}
\label{\detokenize{c7_case_studies/Olist:missing-values}}
\sphinxAtStartPar
for each dataframe we apply a few checks in order to see the quality of data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{customer missing values: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{customers}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
customer missing values: 
customer\PYGZus{}id                 False
customer\PYGZus{}unique\PYGZus{}id          False
customer\PYGZus{}zip\PYGZus{}code\PYGZus{}prefix    False
customer\PYGZus{}city               False
customer\PYGZus{}state              False
dtype: bool
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sellers missing values: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{sellers}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sellers missing values: 
seller\PYGZus{}id                 False
seller\PYGZus{}zip\PYGZus{}code\PYGZus{}prefix    False
seller\PYGZus{}city               False
seller\PYGZus{}state              False
dtype: bool
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{products missing values: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{products}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
products missing values: 
product\PYGZus{}id                    False
product\PYGZus{}category\PYGZus{}name          True
product\PYGZus{}name\PYGZus{}lenght            True
product\PYGZus{}description\PYGZus{}lenght     True
product\PYGZus{}photos\PYGZus{}qty             True
product\PYGZus{}weight\PYGZus{}g               True
product\PYGZus{}length\PYGZus{}cm              True
product\PYGZus{}height\PYGZus{}cm              True
product\PYGZus{}width\PYGZus{}cm               True
dtype: bool
\end{sphinxVerbatim}

\sphinxAtStartPar
we can see that there are missing values for products, let’s see how many!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
product\PYGZus{}id                      0
product\PYGZus{}category\PYGZus{}name         610
product\PYGZus{}name\PYGZus{}lenght           610
product\PYGZus{}description\PYGZus{}lenght    610
product\PYGZus{}photos\PYGZus{}qty            610
product\PYGZus{}weight\PYGZus{}g                2
product\PYGZus{}length\PYGZus{}cm               2
product\PYGZus{}height\PYGZus{}cm               2
product\PYGZus{}width\PYGZus{}cm                2
dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
as there are not ‘that many’ products with missing information, I opted to drop them out. But maybe later i’ll come back to that decision if these products seem crucial.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products} \PYG{o}{=} \PYG{n}{products}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{orders missing values: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{orders}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
orders missing values: 
order\PYGZus{}id               False
order\PYGZus{}item\PYGZus{}id          False
product\PYGZus{}id             False
seller\PYGZus{}id              False
shipping\PYGZus{}limit\PYGZus{}date    False
price                  False
freight\PYGZus{}value          False
dtype: bool
\end{sphinxVerbatim}


\subsection{Duplicates}
\label{\detokenize{c7_case_studies/Olist:duplicates}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{customer duplicates: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{customers}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
customer duplicates: 
False
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{seller duplicates: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{sellers}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
seller duplicates: 
False
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{products duplicates: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{products}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
products duplicates: 
False
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{orders duplicates: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{orders}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
orders duplicates: 
False
\end{sphinxVerbatim}

\sphinxAtStartPar
No duplicates, that’s a good sign, it means that each customer, seller and product is unique!


\subsection{Indexing}
\label{\detokenize{c7_case_studies/Olist:indexing}}
\sphinxAtStartPar
It is more convenient to work with an index, usually we can use ids as index

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{customers} \PYG{o}{=} \PYG{n}{customers}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{customer\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{customers}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                                customer\PYGZus{}unique\PYGZus{}id  ...  customer\PYGZus{}state
customer\PYGZus{}id                                                         ...                
06b8999e2fba1a1fbc88172c00ba8bc7  861eff4711a542e4b93843c6dd7febb0  ...              SP
18955e83d337fd6b2def6b18a428ac77  290c77bc529b7ac935b93aa66c333dc3  ...              SP
4e7b3e00288586ebd08712fdd0374a03  060e732b5b29e8181a18229c7b0b2b5e  ...              SP
b2b6027bc5c5109e529d4dc6358b12c3  259dac757896d24d7702b9acbbff3f3c  ...              SP
4f2d8ab171c80ec8364f7c12e35b23ad  345ecd01c38d18a9036ed96c73b8d066  ...              SP

[5 rows x 4 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sellers} \PYG{o}{=} \PYG{n}{sellers}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{seller\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{sellers}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                  seller\PYGZus{}zip\PYGZus{}code\PYGZus{}prefix  ... seller\PYGZus{}state
seller\PYGZus{}id                                                 ...             
3442f8959a84dea7ee197c632cb2df15                   13023  ...           SP
d1b65fc7debc3361ea86b5f14c68d2e2                   13844  ...           SP
ce3ad9de960102d0677a81f5d0bb7b2d                   20031  ...           RJ
c0f3eea2e14555b6faeea3dd58c1b1c3                    4195  ...           SP
51a04a8a6bdcb23deccc82b0b80742cf                   12914  ...           SP

[5 rows x 3 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products} \PYG{o}{=} \PYG{n}{products}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{products}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                  product\PYGZus{}category\PYGZus{}name  ...  product\PYGZus{}width\PYGZus{}cm
product\PYGZus{}id                                               ...                  
1e9e8ef04dbcff4541ed26657ea517e5             perfumaria  ...              14.0
3aa071139cb16b67ca9e5dea641aaa2f                  artes  ...              20.0
96bd76ec8810374ed1b65e291975717f          esporte\PYGZus{}lazer  ...              15.0
cef67bcfe19066a932b7673e239eb23d                  bebes  ...              26.0
9dc1a7de274444849c219cff195d0b71  utilidades\PYGZus{}domesticas  ...              13.0

[5 rows x 8 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{orders} \PYG{o}{=} \PYG{n}{orders}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{order\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{orders}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                  order\PYGZus{}item\PYGZus{}id  ... freight\PYGZus{}value
order\PYGZus{}id                                         ...              
00010242fe8c5a6d1ba2dd792cb16214              1  ...         13.29
00018f77f2f0320c557190d7a144bdd3              1  ...         19.93
000229ec398224ef6ca0657da4fc703e              1  ...         17.87
00024acbcdf0a6daa1e931b038114c75              1  ...         12.79
00042b26cf59d7ce69dfabb4e55b4fd9              1  ...         18.14

[5 rows x 6 columns]
\end{sphinxVerbatim}


\subsection{Translation}
\label{\detokenize{c7_case_studies/Olist:translation}}
\sphinxAtStartPar
for the products we have a specific dataset that contains the translations, we can apply that to the products dataframe

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  \PYG{n}{translation\PYGZus{}dict} \PYG{o}{=} \PYG{n}{translation}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name\PYGZus{}english}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}dict}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{products}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{products}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{cat}\PYG{o}{.}\PYG{n}{rename\PYGZus{}categories}\PYG{p}{(}\PYG{n}{translation\PYGZus{}dict}\PYG{p}{)}
\PYG{n}{products}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                 product\PYGZus{}category\PYGZus{}name  ...  product\PYGZus{}width\PYGZus{}cm
product\PYGZus{}id                                              ...                  
1e9e8ef04dbcff4541ed26657ea517e5             perfumery  ...              14.0
3aa071139cb16b67ca9e5dea641aaa2f                   art  ...              20.0
96bd76ec8810374ed1b65e291975717f        sports\PYGZus{}leisure  ...              15.0
cef67bcfe19066a932b7673e239eb23d                  baby  ...              26.0
9dc1a7de274444849c219cff195d0b71            housewares  ...              13.0

[5 rows x 8 columns]
\end{sphinxVerbatim}


\section{Processing}
\label{\detokenize{c7_case_studies/Olist:processing}}

\subsection{Product pricing}
\label{\detokenize{c7_case_studies/Olist:product-pricing}}
\sphinxAtStartPar
if we want to find out if there is a correlation between pricing and products, we need to match each product with a price, let’s see what happens when we merge orders and products

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{orders}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                  order\PYGZus{}item\PYGZus{}id  ... freight\PYGZus{}value
order\PYGZus{}id                                         ...              
00010242fe8c5a6d1ba2dd792cb16214              1  ...         13.29
00018f77f2f0320c557190d7a144bdd3              1  ...         19.93
000229ec398224ef6ca0657da4fc703e              1  ...         17.87
00024acbcdf0a6daa1e931b038114c75              1  ...         12.79
00042b26cf59d7ce69dfabb4e55b4fd9              1  ...         18.14

[5 rows x 6 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
it seems that we only have prices of complete orders, which makes things more complicated. Below you can see that some orders contain multiple unique products, therefore we cannot easily deduce the price of a single item…

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{orders}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{level}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{.}\PYG{n}{product\PYGZus{}id}\PYG{o}{.}\PYG{n}{nunique}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1    95430
2     2846
3      298
4       70
6       10
5        8
7        3
8        1
dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
well, let us see if we can find all orders with one item, these prices should agree with the price of the product

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{multi\PYGZus{}item\PYGZus{}orders} \PYG{o}{=} \PYG{n}{orders}\PYG{p}{[}\PYG{n}{orders}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{order\PYGZus{}item\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{!=}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}
\PYG{n}{single\PYGZus{}item\PYGZus{}orders} \PYG{o}{=} \PYG{n}{orders}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{n}{multi\PYGZus{}item\PYGZus{}orders}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products\PYGZus{}w\PYGZus{}price} \PYG{o}{=} \PYG{n}{products}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{single\PYGZus{}item\PYGZus{}orders}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{freight\PYGZus{}value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{left}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{left\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{right\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products\PYGZus{}w\PYGZus{}price}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                  product\PYGZus{}category\PYGZus{}name  ...  freight\PYGZus{}value
e17e4f88e31525f7deef66779844ddce              perfumery  ...           7.39
5236307716393b7114b53ee991f36956                    art  ...          17.99
01f66e58769f84129811d43eefd187fb         sports\PYGZus{}leisure  ...           7.82
143d00a4f2dde4e0364ee1821577adb3                   baby  ...           9.54
86cafb8794cb99a9b1b77fc8e48fbbbb             housewares  ...           8.29
...                                                 ...  ...            ...
6e4008bddce63615856554f94e5233db         bed\PYGZus{}bath\PYGZus{}table  ...          11.91
7c8a032bb75e0e4d524b14ba147d4ba5         bed\PYGZus{}bath\PYGZus{}table  ...          17.14
fc957026f2482ab3bddf91ebc9d0dfc5         bed\PYGZus{}bath\PYGZus{}table  ...          12.39
NaN                               computers\PYGZus{}accessories  ...            NaN
f3a47ba087f05d39a74ed1b653f0be1b         bed\PYGZus{}bath\PYGZus{}table  ...          27.05

[90991 rows x 10 columns]
\end{sphinxVerbatim}


\subsection{grouped per category}
\label{\detokenize{c7_case_studies/Olist:grouped-per-category}}
\sphinxAtStartPar
It would be interesting to have the averages of each feature grouped per category.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{avg\PYGZus{}category\PYGZus{}product} \PYG{o}{=} \PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{avg\PYGZus{}category\PYGZus{}product}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                            product\PYGZus{}name\PYGZus{}lenght  ...  freight\PYGZus{}value
product\PYGZus{}category\PYGZus{}name                            ...               
agro\PYGZus{}industry\PYGZus{}and\PYGZus{}commerce            46.189349  ...      28.733963
food                                  48.781022  ...      14.680448
food\PYGZus{}drink                            45.186916  ...      17.074249
art                                   47.687179  ...      19.120052
arts\PYGZus{}and\PYGZus{}craftmanship                 46.791667  ...      16.152500
...                                         ...  ...            ...
signaling\PYGZus{}and\PYGZus{}security                49.641221  ...      22.465238
tablets\PYGZus{}printing\PYGZus{}image                55.444444  ...      15.205278
telephony                             52.207986  ...      15.705825
fixed\PYGZus{}telephony                       47.950000  ...      16.911832
housewares                            48.442928  ...      21.907430

[73 rows x 9 columns]
\end{sphinxVerbatim}


\subsection{seller reviews}
\label{\detokenize{c7_case_studies/Olist:seller-reviews}}
\sphinxAtStartPar
Another thing that says a lot about sales is the seller rating, we combine orders with order reviews for this

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{seller\PYGZus{}review\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}
    \PYG{n}{orders}\PYG{p}{,}
    \PYG{n}{order\PYGZus{}reviews}\PYG{p}{,}
    \PYG{n}{left\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
    \PYG{n}{right\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{order\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{)}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}
    \PYG{n}{sellers}\PYG{p}{,} 
    \PYG{n}{left\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{seller\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} 
    \PYG{n}{right\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}
\PYG{p}{)}
\PYG{n}{seller\PYGZus{}review\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       order\PYGZus{}item\PYGZus{}id  ... seller\PYGZus{}state
51963              1  ...           SP
53184              1  ...           SP
81465              1  ...           SP
25922              1  ...           SP
82616              1  ...           SP

[5 rows x 16 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
We can do a lot of things with this, an option is to get the average review per seller

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{seller\PYGZus{}review\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{seller\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{review\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
seller\PYGZus{}id
6d04126aba80df143fd038e711b8fd96    1.0
b6c6854d4d92a5f6f46be8869da3fa1a    1.0
34aefe746cd81b7f3b23253ea28bef39    1.0
b7ba853e9551f4558440881fd3e5c815    1.0
17adeba047385fb0c67d8e90b4296d21    1.0
                                   ... 
d7827b2af99326a03b0ed9c7a24db0d3    5.0
4aba6a02a788d3ec81c03137144d9a80    5.0
94ca168e8bcb407ab85c5da308863027    5.0
95cca791657aabeff15a07eb152d7841    5.0
186cdd1b2df32caa72cfb410bba768d3    5.0
Name: review\PYGZus{}score, Length: 3090, dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
or the average review per seller state

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{seller\PYGZus{}review\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{seller\PYGZus{}state}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{review\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
seller\PYGZus{}state
AC    1.000000
AM    2.333333
RO    3.857143
PB    3.864865
SE    3.900000
MA    4.002506
SP    4.005078
ES    4.005450
DF    4.033333
PR    4.072292
PI    4.083333
BA    4.090202
SC    4.093865
RJ    4.101670
MG    4.105868
PE    4.132584
CE    4.138298
MT    4.165517
RS    4.214351
GO    4.254826
RN    4.267857
MS    4.469388
PA    4.500000
Name: review\PYGZus{}score, dtype: float64
\end{sphinxVerbatim}


\section{Exploration}
\label{\detokenize{c7_case_studies/Olist:exploration}}

\subsection{Product pricing}
\label{\detokenize{c7_case_studies/Olist:id1}}
\sphinxAtStartPar
for the product pricing we created a dataframe that contained the single item price for most products, lets review the dataframe

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
Index: 90991 entries, e17e4f88e31525f7deef66779844ddce to f3a47ba087f05d39a74ed1b653f0be1b
Data columns (total 10 columns):
 \PYGZsh{}   Column                      Non\PYGZhy{}Null Count  Dtype   
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}                      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   
 0   product\PYGZus{}category\PYGZus{}name       90991 non\PYGZhy{}null  category
 1   product\PYGZus{}name\PYGZus{}lenght         90991 non\PYGZhy{}null  float64 
 2   product\PYGZus{}description\PYGZus{}lenght  90991 non\PYGZhy{}null  float64 
 3   product\PYGZus{}photos\PYGZus{}qty          90991 non\PYGZhy{}null  float64 
 4   product\PYGZus{}weight\PYGZus{}g            90991 non\PYGZhy{}null  float64 
 5   product\PYGZus{}length\PYGZus{}cm           90991 non\PYGZhy{}null  float64 
 6   product\PYGZus{}height\PYGZus{}cm           90991 non\PYGZhy{}null  float64 
 7   product\PYGZus{}width\PYGZus{}cm            90991 non\PYGZhy{}null  float64 
 8   price                       87575 non\PYGZhy{}null  float64 
 9   freight\PYGZus{}value               87575 non\PYGZhy{}null  float64 
dtypes: category(1), float64(9)
memory usage: 7.0+ MB
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                 product\PYGZus{}category\PYGZus{}name  ...  freight\PYGZus{}value
e17e4f88e31525f7deef66779844ddce             perfumery  ...           7.39
5236307716393b7114b53ee991f36956                   art  ...          17.99
01f66e58769f84129811d43eefd187fb        sports\PYGZus{}leisure  ...           7.82
143d00a4f2dde4e0364ee1821577adb3                  baby  ...           9.54
86cafb8794cb99a9b1b77fc8e48fbbbb            housewares  ...           8.29

[5 rows x 10 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{describe}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       product\PYGZus{}name\PYGZus{}lenght  ...  freight\PYGZus{}value
count         90991.000000  ...   87575.000000
mean             48.847600  ...      20.405906
std              10.009026  ...      16.052020
min               5.000000  ...       0.000000
25\PYGZpc{}              42.000000  ...      13.440000
50\PYGZpc{}              52.000000  ...      16.500000
75\PYGZpc{}              57.000000  ...      21.400000
max              76.000000  ...     409.680000

[8 rows x 9 columns]
\end{sphinxVerbatim}


\subsubsection{normal distribution}
\label{\detokenize{c7_case_studies/Olist:normal-distribution}}
\sphinxAtStartPar
When we would want to predict the price of an item, it means the the other information of that item should correlate with said price. we can do that for all numerical values with a correlation plot. Before we do that let us use shapiro wilk to test normality

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{name}\PYG{p}{,} \PYG{n}{col} \PYG{o+ow}{in} \PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{(}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{dtypes} \PYG{o}{==} \PYG{n+nb}{float}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}\PYG{p}{]}\PYG{o}{.}\PYG{n}{iteritems}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
  \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{name}\PYG{p}{)}
  \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{scipy}\PYG{o}{.}\PYG{n}{stats}\PYG{o}{.}\PYG{n}{shapiro}\PYG{p}{(}\PYG{n}{col}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
product\PYGZus{}name\PYGZus{}lenght
(0.9154905080795288, 0.0)
product\PYGZus{}description\PYGZus{}lenght
(0.8121932148933411, 0.0)
product\PYGZus{}photos\PYGZus{}qty
(0.743693470954895, 0.0)
product\PYGZus{}weight\PYGZus{}g
(0.5443710088729858, 0.0)
product\PYGZus{}length\PYGZus{}cm
(0.8115382194519043, 0.0)
product\PYGZus{}height\PYGZus{}cm
(0.8004813194274902, 0.0)
product\PYGZus{}width\PYGZus{}cm
(0.8457856774330139, 0.0)
price
(0.4680249094963074, 0.0)
freight\PYGZus{}value
(0.5769327282905579, 0.0)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/usr/local/lib/python3.7/dist\PYGZhy{}packages/scipy/stats/morestats.py:1676: UserWarning: p\PYGZhy{}value may not be accurate for N \PYGZgt{} 5000.
  warnings.warn(\PYGZdq{}p\PYGZhy{}value may not be accurate for N \PYGZgt{} 5000.\PYGZdq{})
\end{sphinxVerbatim}


\subsubsection{Numerical correlation}
\label{\detokenize{c7_case_studies/Olist:numerical-correlation}}
\sphinxAtStartPar
hmm it seems that we are dealing with very non normal data, which is usually the case if human behaviour is involved. We should be careful when using linear or parametric methods, so instead of calculating the pearson correlation coefficients, I opt to go for spearman rank correlations

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pricing\PYGZus{}corr} \PYG{o}{=} \PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{(}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{dtypes} \PYG{o}{==} \PYG{n+nb}{float}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}\PYG{p}{]}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spearman}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{pricing\PYGZus{}corr}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                            product\PYGZus{}name\PYGZus{}lenght  ...  freight\PYGZus{}value
product\PYGZus{}name\PYGZus{}lenght                    1.000000  ...       0.033853
product\PYGZus{}description\PYGZus{}lenght             0.082110  ...       0.123991
product\PYGZus{}photos\PYGZus{}qty                     0.165681  ...       0.007767
product\PYGZus{}weight\PYGZus{}g                       0.077482  ...       0.460155
product\PYGZus{}length\PYGZus{}cm                      0.055458  ...       0.293482
product\PYGZus{}height\PYGZus{}cm                     \PYGZhy{}0.042872  ...       0.295279
product\PYGZus{}width\PYGZus{}cm                       0.062193  ...       0.283687
price                                  0.026564  ...       0.445154
freight\PYGZus{}value                          0.033853  ...       1.000000

[9 rows x 9 columns]
\end{sphinxVerbatim}


\subsubsection{Variance inflation}
\label{\detokenize{c7_case_studies/Olist:variance-inflation}}
\sphinxAtStartPar
it looks like there seem to be some interesting correlations, the price is (slightly) correlated with things as product description, weight, length, height, width and freight value, indicating that bigger items are priced higher.
We have to take into account that freight value is on itself correlating with the latter and therefore might be inflating our results, lets use VIF to check this

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{stats}\PYG{n+nn}{.}\PYG{n+nn}{outliers\PYGZus{}influence} \PYG{k+kn}{import} \PYG{n}{variance\PYGZus{}inflation\PYGZus{}factor}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/usr/local/lib/python3.7/dist\PYGZhy{}packages/statsmodels/tools/\PYGZus{}testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.
  import pandas.util.testing as tm
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cols\PYGZus{}to\PYGZus{}keep} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}name\PYGZus{}lenght}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}description\PYGZus{}lenght}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}photos\PYGZus{}qty}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}weight\PYGZus{}g}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}length\PYGZus{}cm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}height\PYGZus{}cm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}width\PYGZus{}cm}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{freight\PYGZus{}value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{vif\PYGZus{}compatible\PYGZus{}price} \PYG{o}{=} \PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{p}{[}\PYG{n}{cols\PYGZus{}to\PYGZus{}keep}\PYG{p}{]}
\PYG{n}{vif\PYGZus{}compatible\PYGZus{}price} \PYG{o}{=} \PYG{n}{vif\PYGZus{}compatible\PYGZus{}price}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{vif\PYGZus{}compatible\PYGZus{}price} \PYG{o}{=} \PYG{n}{vif\PYGZus{}compatible\PYGZus{}price}\PYG{o}{.}\PYG{n}{values}
\PYG{n}{vif\PYGZus{}price} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{k}{for} \PYG{n}{idx}\PYG{p}{,} \PYG{n}{col} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{cols\PYGZus{}to\PYGZus{}keep}\PYG{p}{)}\PYG{p}{:}
  \PYG{n}{vif\PYGZus{}price}\PYG{p}{[}\PYG{n}{col}\PYG{p}{]} \PYG{o}{=} \PYG{n}{variance\PYGZus{}inflation\PYGZus{}factor}\PYG{p}{(}\PYG{n}{vif\PYGZus{}compatible\PYGZus{}price}\PYG{p}{,} \PYG{n}{idx}\PYG{p}{)}
  \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{col} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{: }\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{variance\PYGZus{}inflation\PYGZus{}factor}\PYG{p}{(}\PYG{n}{vif\PYGZus{}compatible\PYGZus{}price}\PYG{p}{,} \PYG{n}{idx}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
product\PYGZus{}name\PYGZus{}lenght: 	9.31669476790696
product\PYGZus{}description\PYGZus{}lenght: 	2.56022139812164
product\PYGZus{}photos\PYGZus{}qty: 	2.7618344464628604
product\PYGZus{}weight\PYGZus{}g: 	3.041859125336769
product\PYGZus{}length\PYGZus{}cm: 	6.820907759918533
product\PYGZus{}height\PYGZus{}cm: 	3.686708040653942
product\PYGZus{}width\PYGZus{}cm: 	7.7388710828170835
freight\PYGZus{}value: 	4.1851224676888155
\end{sphinxVerbatim}

\sphinxAtStartPar
As mentioned earlier, the values here are hard to interpret, however the values seem to be lower than my experience expected. If infinite values arise we know that we need to do things different. Let’s assume the collinearity between these columns is ok and they don’t interfere with eachother enough to make a difference in the outcome.


\subsubsection{Categorical correlation}
\label{\detokenize{c7_case_studies/Olist:categorical-correlation}}
\sphinxAtStartPar
Something interesting we haven’t looked into yet is the product category, we could try an ANOVA, but knowing at least one category is different is just a beginning.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{str}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{dtypes}\PYG{p}{[}\PYG{p}{(}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{dtypes} \PYG{o}{==} \PYG{n+nb}{float}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{index}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdq{}[\PYGZsq{}product\PYGZus{}name\PYGZus{}lenght\PYGZsq{}, \PYGZsq{}product\PYGZus{}description\PYGZus{}lenght\PYGZsq{}, \PYGZsq{}product\PYGZus{}photos\PYGZus{}qty\PYGZsq{}, \PYGZsq{}product\PYGZus{}weight\PYGZus{}g\PYGZsq{}, \PYGZsq{}product\PYGZus{}length\PYGZus{}cm\PYGZsq{}, \PYGZsq{}product\PYGZus{}height\PYGZus{}cm\PYGZsq{}, \PYGZsq{}product\PYGZus{}width\PYGZus{}cm\PYGZsq{}, \PYGZsq{}price\PYGZsq{}, \PYGZsq{}freight\PYGZus{}value\PYGZsq{}]\PYGZdq{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products\PYGZus{}w\PYGZus{}price\PYGZus{}p\PYGZus{}category} \PYG{o}{=} \PYG{p}{[}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{n}{category}\PYG{p}{,}\PYG{p}{(}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{dtypes} \PYG{o}{==} \PYG{n+nb}{float}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}\PYG{p}{]}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{category} \PYG{o+ow}{in} \PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{scipy}\PYG{o}{.}\PYG{n}{stats}\PYG{o}{.}\PYG{n}{f\PYGZus{}oneway}\PYG{p}{(}\PYG{o}{*}\PYG{n}{products\PYGZus{}w\PYGZus{}price\PYGZus{}p\PYGZus{}category}\PYG{p}{)}

\PYG{n}{anova\PYGZus{}price} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{k}{for} \PYG{n}{name}\PYG{p}{,} \PYG{n}{test}\PYG{p}{,} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{dtypes}\PYG{p}{[}\PYG{p}{(}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{dtypes} \PYG{o}{==} \PYG{n+nb}{float}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{index}\PYG{p}{)}\PYG{p}{,} \PYG{n}{result}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{result}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
  \PYG{n}{anova\PYGZus{}price}\PYG{p}{[}\PYG{n}{name}\PYG{p}{]} \PYG{o}{=} \PYG{p}{[}\PYG{n}{test}\PYG{p}{,} \PYG{n}{p}\PYG{p}{]}

\PYG{n}{anova\PYGZus{}price} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{o}{.}\PYG{n}{from\PYGZus{}dict}\PYG{p}{(}\PYG{n}{anova\PYGZus{}price}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{p}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{orient}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{anova\PYGZus{}price}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                  test    p
product\PYGZus{}name\PYGZus{}lenght          75.721841  0.0
product\PYGZus{}description\PYGZus{}lenght  188.961232  0.0
product\PYGZus{}photos\PYGZus{}qty          119.219872  0.0
product\PYGZus{}weight\PYGZus{}g            296.729258  0.0
product\PYGZus{}length\PYGZus{}cm           387.834682  0.0
product\PYGZus{}height\PYGZus{}cm           394.784176  0.0
product\PYGZus{}width\PYGZus{}cm            450.042245  0.0
price                       155.185233  0.0
freight\PYGZus{}value                98.666293  0.0
\end{sphinxVerbatim}

\sphinxAtStartPar
it seems that every continuous column has at least one category that differs from the rest, aside from order item id, which is always 1.


\subsubsection{Grouping by category}
\label{\detokenize{c7_case_studies/Olist:grouping-by-category}}
\sphinxAtStartPar
Now comes the tricky part, we would like to know if specific categories perform better on the correlations, but this is impossible to do by hand! However python gives us the opportunity to automate this. To do this properly we have to set a rule:
\begin{itemize}
\item {} 
\sphinxAtStartPar
correlations should be better than the original one without separation of categories

\end{itemize}

\sphinxAtStartPar
Look closely how we do almost exactly the same, however we aggregate (groupby) based on the category name

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pricing\PYGZus{}corr}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                            product\PYGZus{}name\PYGZus{}lenght  ...  freight\PYGZus{}value
product\PYGZus{}name\PYGZus{}lenght                    1.000000  ...       0.033853
product\PYGZus{}description\PYGZus{}lenght             0.082110  ...       0.123991
product\PYGZus{}photos\PYGZus{}qty                     0.165681  ...       0.007767
product\PYGZus{}weight\PYGZus{}g                       0.077482  ...       0.460155
product\PYGZus{}length\PYGZus{}cm                      0.055458  ...       0.293482
product\PYGZus{}height\PYGZus{}cm                     \PYGZhy{}0.042872  ...       0.295279
product\PYGZus{}width\PYGZus{}cm                       0.062193  ...       0.283687
price                                  0.026564  ...       0.445154
freight\PYGZus{}value                          0.033853  ...       1.000000

[9 rows x 9 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pricing\PYGZus{}rel\PYGZus{}corr} \PYG{o}{=} \PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}
    \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{dtypes} \PYG{o}{==} \PYG{n+nb}{float}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}\PYG{p}{]}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spearman}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{pricing\PYGZus{}corr}
    \PYG{p}{)}
\PYG{n}{pricing\PYGZus{}rel\PYGZus{}corr}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                                       product\PYGZus{}name\PYGZus{}lenght  ...  freight\PYGZus{}value
product\PYGZus{}category\PYGZus{}name                                                       ...               
agro\PYGZus{}industry\PYGZus{}and\PYGZus{}commerce product\PYGZus{}name\PYGZus{}lenght                    0.000000  ...       0.036661
                           product\PYGZus{}description\PYGZus{}lenght             0.542952  ...       0.057213
                           product\PYGZus{}photos\PYGZus{}qty                     0.187918  ...      \PYGZhy{}0.009524
                           product\PYGZus{}weight\PYGZus{}g                       0.177061  ...       0.171163
                           product\PYGZus{}length\PYGZus{}cm                     \PYGZhy{}0.053490  ...       0.308595
...                                                                    ...  ...            ...
housewares                 product\PYGZus{}length\PYGZus{}cm                      0.018919  ...       0.121902
                           product\PYGZus{}height\PYGZus{}cm                      0.022937  ...      \PYGZhy{}0.016564
                           product\PYGZus{}width\PYGZus{}cm                       0.031462  ...       0.085940
                           price                                  0.069826  ...       0.098247
                           freight\PYGZus{}value                          0.068276  ...       0.000000

[657 rows x 9 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
for those who are already proficient with python can read that I opted to take the absolute correlation (meaning negatives become positives), this way both negative and positive correlations mean the same thing. Then I subtracted with the overall absolute correlation and divided that whole with the overall correlation giving me a relative change. When this relative change is positive, that category has an increased correlation

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pricing\PYGZus{}corr\PYGZus{}stacked} \PYG{o}{=} \PYG{n}{pricing\PYGZus{}rel\PYGZus{}corr}\PYG{o}{.}\PYG{n}{stack}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{pricing\PYGZus{}corr\PYGZus{}stacked}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
product\PYGZus{}category\PYGZus{}name                                                                    
security\PYGZus{}and\PYGZus{}services              product\PYGZus{}width\PYGZus{}cm            product\PYGZus{}description\PYGZus{}lenght    1.063551
                                   product\PYGZus{}description\PYGZus{}lenght  product\PYGZus{}width\PYGZus{}cm              1.063551
                                   product\PYGZus{}height\PYGZus{}cm           product\PYGZus{}name\PYGZus{}lenght           1.042872
                                   product\PYGZus{}name\PYGZus{}lenght         product\PYGZus{}height\PYGZus{}cm             1.042872
pc\PYGZus{}gamer                           product\PYGZus{}name\PYGZus{}lenght         product\PYGZus{}height\PYGZus{}cm             1.042872
                                                                                               ...   
                                   product\PYGZus{}weight\PYGZus{}g            product\PYGZus{}width\PYGZus{}cm             \PYGZhy{}1.536737
furniture\PYGZus{}mattress\PYGZus{}and\PYGZus{}upholstery  product\PYGZus{}length\PYGZus{}cm           product\PYGZus{}weight\PYGZus{}g             \PYGZhy{}1.542839
                                   product\PYGZus{}weight\PYGZus{}g            product\PYGZus{}length\PYGZus{}cm            \PYGZhy{}1.542839
pc\PYGZus{}gamer                           product\PYGZus{}width\PYGZus{}cm            product\PYGZus{}length\PYGZus{}cm            \PYGZhy{}1.558266
                                   product\PYGZus{}length\PYGZus{}cm           product\PYGZus{}width\PYGZus{}cm             \PYGZhy{}1.558266
Length: 5836, dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
wow! we seem to be having very strong correlation increases up to 99\%!? Is this possible? We should be very suspicious about these results, lets us find out why there are this high increases by calculating the initial correlation of ‘security\_and\_services’

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pricing\PYGZus{}p\PYGZus{}cat\PYGZus{}corr} \PYG{o}{=} \PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}
    \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{(}\PYG{n}{x}\PYG{o}{.}\PYG{n}{dtypes} \PYG{o}{==} \PYG{n+nb}{float}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}\PYG{p}{]}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spearman}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pricing\PYGZus{}p\PYGZus{}cat\PYGZus{}corr}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{security\PYGZus{}and\PYGZus{}services}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
product\PYGZus{}name\PYGZus{}lenght           1.0
product\PYGZus{}description\PYGZus{}lenght    1.0
product\PYGZus{}photos\PYGZus{}qty           \PYGZhy{}1.0
product\PYGZus{}weight\PYGZus{}g              1.0
product\PYGZus{}length\PYGZus{}cm             1.0
product\PYGZus{}height\PYGZus{}cm             1.0
product\PYGZus{}width\PYGZus{}cm              1.0
price                         1.0
freight\PYGZus{}value                 1.0
Name: (security\PYGZus{}and\PYGZus{}services, price), dtype: float64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pricing\PYGZus{}corr}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
product\PYGZus{}name\PYGZus{}lenght           0.026564
product\PYGZus{}description\PYGZus{}lenght    0.218892
product\PYGZus{}photos\PYGZus{}qty            0.026766
product\PYGZus{}weight\PYGZus{}g              0.524087
product\PYGZus{}length\PYGZus{}cm             0.260411
product\PYGZus{}height\PYGZus{}cm             0.356680
product\PYGZus{}width\PYGZus{}cm              0.274180
price                         1.000000
freight\PYGZus{}value                 0.445154
Name: price, dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
This is not normal, a perfect correlation might indicate a category with only one record, let us print the subset of data belonging to this category

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{p}{[}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{security\PYGZus{}and\PYGZus{}services}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                  product\PYGZus{}category\PYGZus{}name  ...  freight\PYGZus{}value
bede3503afed051733eeb4a84d1adcc5  security\PYGZus{}and\PYGZus{}services  ...          15.45
2c4ada2e75c2ad41dd93cebb5df5f023  security\PYGZus{}and\PYGZus{}services  ...          25.77

[2 rows x 10 columns]
\end{sphinxVerbatim}


\subsubsection{Dealing with small subsets in data}
\label{\detokenize{c7_case_studies/Olist:dealing-with-small-subsets-in-data}}
\sphinxAtStartPar
as expected, we only have 2 item here making things a lot more complicated. We can solve this by making a compromise, since predicting prices for categories (of there is a difference in categories) with little to no examples is inaccurate, we can choose to drop all small categories. This means that our prediction is not capable for certain items however.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{category\PYGZus{}sizes} \PYG{o}{=} \PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{small\PYGZus{}categories} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{category\PYGZus{}sizes}\PYG{p}{[}\PYG{n}{category\PYGZus{}sizes}\PYG{o}{\PYGZlt{}}\PYG{l+m+mi}{50}\PYG{p}{]}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{values}\PYG{p}{)}
\PYG{n}{small\PYGZus{}categories}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}security\PYGZus{}and\PYGZus{}services\PYGZsq{},
 \PYGZsq{}fashion\PYGZus{}childrens\PYGZus{}clothes\PYGZsq{},
 \PYGZsq{}pc\PYGZus{}gamer\PYGZsq{},
 \PYGZsq{}cds\PYGZus{}dvds\PYGZus{}musicals\PYGZsq{},
 \PYGZsq{}la\PYGZus{}cuisine\PYGZsq{},
 \PYGZsq{}portateis\PYGZus{}cozinha\PYGZus{}e\PYGZus{}preparadores\PYGZus{}de\PYGZus{}alimentos\PYGZsq{},
 \PYGZsq{}home\PYGZus{}comfort\PYGZus{}2\PYGZsq{},
 \PYGZsq{}flowers\PYGZsq{},
 \PYGZsq{}arts\PYGZus{}and\PYGZus{}craftmanship\PYGZsq{},
 \PYGZsq{}diapers\PYGZus{}and\PYGZus{}hygiene\PYGZsq{},
 \PYGZsq{}fashion\PYGZus{}sport\PYGZsq{},
 \PYGZsq{}party\PYGZus{}supplies\PYGZsq{},
 \PYGZsq{}music\PYGZsq{},
 \PYGZsq{}fashio\PYGZus{}female\PYGZus{}clothing\PYGZsq{},
 \PYGZsq{}furniture\PYGZus{}mattress\PYGZus{}and\PYGZus{}upholstery\PYGZsq{}]
\end{sphinxVerbatim}

\sphinxAtStartPar
We opted for a minimum of 50 items per category, let’s see how that improves our relative correlations:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pricing\PYGZus{}corr\PYGZus{}stacked}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{n}{small\PYGZus{}categories}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
product\PYGZus{}category\PYGZus{}name                                                          
fashion\PYGZus{}underwear\PYGZus{}beach  product\PYGZus{}photos\PYGZus{}qty          product\PYGZus{}height\PYGZus{}cm             0.849303
                         product\PYGZus{}height\PYGZus{}cm           product\PYGZus{}photos\PYGZus{}qty            0.849303
christmas\PYGZus{}supplies       product\PYGZus{}width\PYGZus{}cm            product\PYGZus{}description\PYGZus{}lenght    0.838264
                         product\PYGZus{}description\PYGZus{}lenght  product\PYGZus{}width\PYGZus{}cm              0.838264
                         product\PYGZus{}length\PYGZus{}cm           product\PYGZus{}description\PYGZus{}lenght    0.789431
                                                                                     ...   
fashion\PYGZus{}underwear\PYGZus{}beach  product\PYGZus{}weight\PYGZus{}g            product\PYGZus{}length\PYGZus{}cm            \PYGZhy{}1.037104
books\PYGZus{}imported           price                       product\PYGZus{}width\PYGZus{}cm             \PYGZhy{}1.136276
                         product\PYGZus{}width\PYGZus{}cm            price                        \PYGZhy{}1.136276
fashion\PYGZus{}shoes            product\PYGZus{}length\PYGZus{}cm           product\PYGZus{}width\PYGZus{}cm             \PYGZhy{}1.273546
                         product\PYGZus{}width\PYGZus{}cm            product\PYGZus{}length\PYGZus{}cm            \PYGZhy{}1.273546
Length: 4698, dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
Now we filtered out smaller categories that might have high fluctuations, however we are not interested into correlations between any 2 columns (keep your goals in mind!) so we are going to filter only the price. I even found a method (xs) which I never use myself, google is your friend!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pricing\PYGZus{}corr\PYGZus{}stacked}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{n}{small\PYGZus{}categories}\PYG{p}{)}\PYG{o}{.}\PYG{n}{xs}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{level}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{drop\PYGZus{}level}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
product\PYGZus{}category\PYGZus{}name                                                   
small\PYGZus{}appliances\PYGZus{}home\PYGZus{}oven\PYGZus{}and\PYGZus{}coffee  price  product\PYGZus{}photos\PYGZus{}qty            0.705878
computers                              price  product\PYGZus{}photos\PYGZus{}qty            0.703822
furniture\PYGZus{}bedroom                      price  product\PYGZus{}photos\PYGZus{}qty            0.638002
home\PYGZus{}confort                           price  product\PYGZus{}name\PYGZus{}lenght           0.631313
fashion\PYGZus{}shoes                          price  product\PYGZus{}name\PYGZus{}lenght           0.604471
                                                                              ...   
computers                              price  product\PYGZus{}height\PYGZus{}cm            \PYGZhy{}0.810059
construction\PYGZus{}tools\PYGZus{}lights              price  product\PYGZus{}description\PYGZus{}lenght   \PYGZhy{}0.812678
computers                              price  product\PYGZus{}length\PYGZus{}cm            \PYGZhy{}0.819188
                                              product\PYGZus{}weight\PYGZus{}g             \PYGZhy{}0.984040
books\PYGZus{}imported                         price  product\PYGZus{}width\PYGZus{}cm             \PYGZhy{}1.136276
Length: 522, dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
Ok, here I personally believe we have something we can work with! We can clearly see a relative change for correlation with certain columns. One thing that still remains is to filter per category the most important change compared to the average correlation

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pricing\PYGZus{}most\PYGZus{}important} \PYG{o}{=} \PYG{n}{pricing\PYGZus{}corr\PYGZus{}stacked}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{n}{small\PYGZus{}categories}\PYG{p}{)}\PYG{o}{.}\PYG{n}{xs}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{level}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{drop\PYGZus{}level}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{pricing\PYGZus{}most\PYGZus{}important}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{parameter}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relative\PYGZus{}correlation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{pricing\PYGZus{}most\PYGZus{}important}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                                        parameter  relative\PYGZus{}correlation
product\PYGZus{}category\PYGZus{}name                                                                  
small\PYGZus{}appliances\PYGZus{}home\PYGZus{}oven\PYGZus{}and\PYGZus{}coffee          product\PYGZus{}photos\PYGZus{}qty              0.705878
computers                                      product\PYGZus{}photos\PYGZus{}qty              0.703822
furniture\PYGZus{}bedroom                              product\PYGZus{}photos\PYGZus{}qty              0.638002
home\PYGZus{}confort                                  product\PYGZus{}name\PYGZus{}lenght              0.631313
fashion\PYGZus{}shoes                                 product\PYGZus{}name\PYGZus{}lenght              0.604471
fashion\PYGZus{}underwear\PYGZus{}beach                        product\PYGZus{}photos\PYGZus{}qty              0.577173
cine\PYGZus{}photo                             product\PYGZus{}description\PYGZus{}lenght              0.554634
electronics                                    product\PYGZus{}photos\PYGZus{}qty              0.547695
fixed\PYGZus{}telephony                        product\PYGZus{}description\PYGZus{}lenght              0.524150
christmas\PYGZus{}supplies                              product\PYGZus{}length\PYGZus{}cm              0.513941
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pricing\PYGZus{}least\PYGZus{}important} \PYG{o}{=} \PYG{n}{pricing\PYGZus{}corr\PYGZus{}stacked}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{n}{small\PYGZus{}categories}\PYG{p}{)}\PYG{o}{.}\PYG{n}{xs}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{level}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{drop\PYGZus{}level}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{keep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{last}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{pricing\PYGZus{}least\PYGZus{}important}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{parameter}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relative\PYGZus{}correlation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{pricing\PYGZus{}least\PYGZus{}important}\PYG{o}{.}\PYG{n}{tail}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                                 parameter  relative\PYGZus{}correlation
product\PYGZus{}category\PYGZus{}name                                                           
home\PYGZus{}confort                             product\PYGZus{}length\PYGZus{}cm             \PYGZhy{}0.444956
furniture\PYGZus{}bedroom                      product\PYGZus{}name\PYGZus{}lenght             \PYGZhy{}0.472171
fashion\PYGZus{}shoes                           product\PYGZus{}photos\PYGZus{}qty             \PYGZhy{}0.491136
furniture\PYGZus{}living\PYGZus{}room           product\PYGZus{}description\PYGZus{}lenght             \PYGZhy{}0.507893
audio                                  product\PYGZus{}name\PYGZus{}lenght             \PYGZhy{}0.530846
industry\PYGZus{}commerce\PYGZus{}and\PYGZus{}business           product\PYGZus{}height\PYGZus{}cm             \PYGZhy{}0.596886
fashion\PYGZus{}underwear\PYGZus{}beach                  product\PYGZus{}length\PYGZus{}cm             \PYGZhy{}0.629531
construction\PYGZus{}tools\PYGZus{}lights       product\PYGZus{}description\PYGZus{}lenght             \PYGZhy{}0.812678
computers                                 product\PYGZus{}weight\PYGZus{}g             \PYGZhy{}0.984040
books\PYGZus{}imported                            product\PYGZus{}width\PYGZus{}cm             \PYGZhy{}1.136276
\end{sphinxVerbatim}

\sphinxAtStartPar
What we can distill here:
\begin{itemize}
\item {} 
\sphinxAtStartPar
the quantity of photo’s is important for small applicances, computers, furniture,… which is to be expected because you are willing to pay more if you are sure it looks like you want it to look

\item {} 
\sphinxAtStartPar
the weight of fasion accessories and ‘industry commerce’ is not as important compared to other categories, as these things are always light, expensive or not

\end{itemize}

\sphinxAtStartPar
Anyway, now it is up to you to further interpret these values, but I think this should already give a nice idea on how we can estimate prices and how this changes per category.


\section{Visualization}
\label{\detokenize{c7_case_studies/Olist:visualization}}

\subsection{Product pricing}
\label{\detokenize{c7_case_studies/Olist:id2}}
\sphinxAtStartPar
Now that we done the exploration, we can back our hypothesi up with some visual representations, many plots you will make will not end up in the final product but are meant to give you a more clear view on the situation itself


\subsubsection{Normal distribution}
\label{\detokenize{c7_case_studies/Olist:id3}}
\sphinxAtStartPar
In the exploration we talked about the non normal distribution of our dataset, let us plot the numerical columns into histograms to verify this. Fortunately, pandas has a built\sphinxhyphen{}in hist method that works perfect.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{,} \PYG{n}{layout}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}Figure size 1152x576 with 10 Axes\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
ouch! this doesn’t look normally distributed at all, we can also put it into a boxplot and compare with a bar plot

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{(}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{dtypes} \PYG{o}{==} \PYG{n+nb}{float}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}\PYG{p}{]}\PYG{o}{.}\PYG{n}{stack}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{level\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yscale}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticklabels}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{get\PYGZus{}xticklabels}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{rotation}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{n}{horizontalalignment}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{left}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Output hidden; open in https://colab.research.google.com to view.
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{barplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{(}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{dtypes} \PYG{o}{==} \PYG{n+nb}{float}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}\PYG{p}{]}\PYG{o}{.}\PYG{n}{stack}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{level\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yscale}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticklabels}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{get\PYGZus{}xticklabels}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{rotation}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{n}{horizontalalignment}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{left}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}Figure size 432x288 with 1 Axes\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
These 2 plots look alike, but in my opinion the first clearly shows that the peak consists out of outliers, hence the non normal distribution. Can you find the column responsible for this peak using the histograms?


\subsubsection{Numerical correlation}
\label{\detokenize{c7_case_studies/Olist:id4}}
\sphinxAtStartPar
We saw there were some numerical correlations within the dataset, let us try to visualize these, the first thing that pops into my mind is the pairplot.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{}sns.pairplot(data=products\PYGZus{}w\PYGZus{}price.loc[:,(products\PYGZus{}w\PYGZus{}price.dtypes == float).values].dropna())}
\end{sphinxVerbatim}

\sphinxAtStartPar
hmm it seems that in this case the pairplot doesn’t seem to be that conclusive, but we already knew that the correlations werent that appearent. Let us keep it simple and make a heatmap of the correlation statistic!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{p}{(}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{dtypes} \PYG{o}{==} \PYG{n+nb}{float}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}\PYG{p}{]}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spearman}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{annot}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f5352774a10\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}Figure size 432x288 with 2 Axes\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
ok this is basically the same as in the exploration but with colors, these colors however give us a good way to group correlations, we can see that the width, height, length and weight create a nice block, and are also correlated with the price.


\subsubsection{Variance Inflation}
\label{\detokenize{c7_case_studies/Olist:id5}}
\sphinxAtStartPar
We looked into the inflation inbetween those correlated columns, because it might be that they are telling the same story. To illustrate this information we can use a bar chart.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{Series}\PYG{p}{(}\PYG{n}{vif\PYGZus{}price}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f53542a3410\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}Figure size 432x288 with 1 Axes\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
This way we can see that both the product length and width are highly correlated with other columns in the dataset i.e. their variation is explainable by other columns in the dataset. We opted to not remove any parameters here.


\subsubsection{Categorical correlation}
\label{\detokenize{c7_case_studies/Olist:id6}}
\sphinxAtStartPar
We performed anova tests to know if and how much variance there is between categories for each numerical column. we can use a bar plot to visualize.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{anova\PYGZus{}price}\PYG{o}{.}\PYG{n}{plot}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.axes.\PYGZus{}subplots.AxesSubplot at 0x7f5354828950\PYGZgt{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}Figure size 432x288 with 1 Axes\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
This might not be my best plot I ever made, but it quantifies the amount of variation of a numerical column compared to the category column ‘category’ item specs as size vary more whilst the name and description remain much more the same. In this plot, you can see I made a crucial mistake by using the same axis range for the test statistic and the p\sphinxhyphen{}value, which is much smaller (between 0\sphinxhyphen{}1). Don’t do this yourself! (I didn’t bother as all p\sphinxhyphen{}values are 0 except for order\_item\_id)


\subsubsection{Grouping by category}
\label{\detokenize{c7_case_studies/Olist:id7}}
\sphinxAtStartPar
As last we grouped by category and recalculated the numerical correlation for each category apart. Note that we removed lowly populated categories as the prediction of the price might be not representative. I will use a boxplot to show any variation

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{products\PYGZus{}w\PYGZus{}price\PYGZus{}sorted\PYGZus{}price} \PYG{o}{=} \PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{index}
\PYG{n}{products\PYGZus{}w\PYGZus{}price\PYGZus{}sorted\PYGZus{}price}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
CategoricalIndex([\PYGZsq{}home\PYGZus{}comfort\PYGZus{}2\PYGZsq{}, \PYGZsq{}dvds\PYGZus{}blu\PYGZus{}ray\PYGZsq{}, \PYGZsq{}electronics\PYGZsq{}, \PYGZsq{}flowers\PYGZsq{},
                  \PYGZsq{}telephony\PYGZsq{}, \PYGZsq{}portateis\PYGZus{}cozinha\PYGZus{}e\PYGZus{}preparadores\PYGZus{}de\PYGZus{}alimentos\PYGZsq{},
                  \PYGZsq{}diapers\PYGZus{}and\PYGZus{}hygiene\PYGZsq{}, \PYGZsq{}fixed\PYGZus{}telephony\PYGZsq{}, \PYGZsq{}food\PYGZus{}drink\PYGZsq{},
                  \PYGZsq{}books\PYGZus{}general\PYGZus{}interest\PYGZsq{}, \PYGZsq{}drinks\PYGZsq{}, \PYGZsq{}home\PYGZus{}appliances\PYGZsq{},
                  \PYGZsq{}fashion\PYGZus{}bags\PYGZus{}accessories\PYGZsq{}, \PYGZsq{}arts\PYGZus{}and\PYGZus{}craftmanship\PYGZsq{},
                  \PYGZsq{}fashio\PYGZus{}female\PYGZus{}clothing\PYGZsq{}, \PYGZsq{}cds\PYGZus{}dvds\PYGZus{}musicals\PYGZsq{}, \PYGZsq{}food\PYGZsq{},
                  \PYGZsq{}books\PYGZus{}technical\PYGZsq{}, \PYGZsq{}christmas\PYGZus{}supplies\PYGZsq{},
                  \PYGZsq{}costruction\PYGZus{}tools\PYGZus{}garden\PYGZsq{}, \PYGZsq{}fashion\PYGZus{}underwear\PYGZus{}beach\PYGZsq{},
                  \PYGZsq{}fashion\PYGZus{}male\PYGZus{}clothing\PYGZsq{}, \PYGZsq{}garden\PYGZus{}tools\PYGZsq{}, \PYGZsq{}fashion\PYGZus{}sport\PYGZsq{},
                  \PYGZsq{}books\PYGZus{}imported\PYGZsq{}, \PYGZsq{}music\PYGZsq{}, \PYGZsq{}housewares\PYGZsq{}, \PYGZsq{}market\PYGZus{}place\PYGZsq{},
                  \PYGZsq{}consoles\PYGZus{}games\PYGZsq{}, \PYGZsq{}party\PYGZus{}supplies\PYGZsq{}, \PYGZsq{}furniture\PYGZus{}decor\PYGZsq{},
                  \PYGZsq{}costruction\PYGZus{}tools\PYGZus{}tools\PYGZsq{}, \PYGZsq{}stationery\PYGZsq{}, \PYGZsq{}baby\PYGZsq{},
                  \PYGZsq{}signaling\PYGZus{}and\PYGZus{}security\PYGZsq{}, \PYGZsq{}computers\PYGZus{}accessories\PYGZsq{},
                  \PYGZsq{}bed\PYGZus{}bath\PYGZus{}table\PYGZsq{}, \PYGZsq{}auto\PYGZsq{}, \PYGZsq{}fashion\PYGZus{}shoes\PYGZsq{}, \PYGZsq{}cine\PYGZus{}photo\PYGZsq{},
                  \PYGZsq{}health\PYGZus{}beauty\PYGZsq{}, \PYGZsq{}furniture\PYGZus{}mattress\PYGZus{}and\PYGZus{}upholstery\PYGZsq{}, \PYGZsq{}toys\PYGZsq{},
                  \PYGZsq{}sports\PYGZus{}leisure\PYGZsq{}, \PYGZsq{}audio\PYGZsq{}, \PYGZsq{}home\PYGZus{}confort\PYGZsq{}, \PYGZsq{}perfumery\PYGZsq{},
                  \PYGZsq{}fashion\PYGZus{}childrens\PYGZus{}clothes\PYGZsq{}, \PYGZsq{}pet\PYGZus{}shop\PYGZsq{},
                  \PYGZsq{}construction\PYGZus{}tools\PYGZus{}construction\PYGZsq{}, \PYGZsq{}tablets\PYGZus{}printing\PYGZus{}image\PYGZsq{},
                  \PYGZsq{}art\PYGZsq{}, \PYGZsq{}musical\PYGZus{}instruments\PYGZsq{}, \PYGZsq{}luggage\PYGZus{}accessories\PYGZsq{},
                  \PYGZsq{}industry\PYGZus{}commerce\PYGZus{}and\PYGZus{}business\PYGZsq{}, \PYGZsq{}furniture\PYGZus{}living\PYGZus{}room\PYGZsq{},
                  \PYGZsq{}small\PYGZus{}appliances\PYGZsq{}, \PYGZsq{}home\PYGZus{}construction\PYGZsq{},
                  \PYGZsq{}kitchen\PYGZus{}dining\PYGZus{}laundry\PYGZus{}garden\PYGZus{}furniture\PYGZsq{},
                  \PYGZsq{}construction\PYGZus{}tools\PYGZus{}safety\PYGZsq{}, \PYGZsq{}pc\PYGZus{}gamer\PYGZsq{}, \PYGZsq{}cool\PYGZus{}stuff\PYGZsq{},
                  \PYGZsq{}la\PYGZus{}cuisine\PYGZsq{}, \PYGZsq{}air\PYGZus{}conditioning\PYGZsq{}, \PYGZsq{}watches\PYGZus{}gifts\PYGZsq{},
                  \PYGZsq{}security\PYGZus{}and\PYGZus{}services\PYGZsq{}, \PYGZsq{}office\PYGZus{}furniture\PYGZsq{},
                  \PYGZsq{}construction\PYGZus{}tools\PYGZus{}lights\PYGZsq{}, \PYGZsq{}furniture\PYGZus{}bedroom\PYGZsq{},
                  \PYGZsq{}home\PYGZus{}appliances\PYGZus{}2\PYGZsq{}, \PYGZsq{}agro\PYGZus{}industry\PYGZus{}and\PYGZus{}commerce\PYGZsq{},
                  \PYGZsq{}small\PYGZus{}appliances\PYGZus{}home\PYGZus{}oven\PYGZus{}and\PYGZus{}coffee\PYGZsq{}, \PYGZsq{}computers\PYGZsq{}],
                 categories=[\PYGZsq{}agro\PYGZus{}industry\PYGZus{}and\PYGZus{}commerce\PYGZsq{}, \PYGZsq{}food\PYGZsq{}, \PYGZsq{}food\PYGZus{}drink\PYGZsq{}, \PYGZsq{}art\PYGZsq{}, \PYGZsq{}arts\PYGZus{}and\PYGZus{}craftmanship\PYGZsq{}, \PYGZsq{}party\PYGZus{}supplies\PYGZsq{}, \PYGZsq{}christmas\PYGZus{}supplies\PYGZsq{}, \PYGZsq{}audio\PYGZsq{}, ...], ordered=False, name=\PYGZsq{}product\PYGZus{}category\PYGZus{}name\PYGZsq{}, dtype=\PYGZsq{}category\PYGZsq{})
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(}\PYG{n}{data}\PYG{o}{=}\PYG{n}{products\PYGZus{}w\PYGZus{}price}\PYG{p}{,} \PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{product\PYGZus{}category\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{order}\PYG{o}{=}\PYG{n}{products\PYGZus{}w\PYGZus{}price\PYGZus{}sorted\PYGZus{}price}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set}\PYG{p}{(}\PYG{n}{yscale}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{log}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticklabels}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{get\PYGZus{}xticklabels}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{rotation}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{n}{horizontalalignment}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{left}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}Figure size 432x288 with 1 Axes\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
cool! here we can see the variation in groups for the price column, this way we can deduce wich categories are highly priced and which are lowly priced. Our machine learning solution later will use this information to help decide the price (if we of course use it to train the model). We can conclude that while the variation in each category can be high, there is a trend in price between categories.

\sphinxAtStartPar
We also calculated relative changes of correlation between price and other numerical columns inbetween categories. Let’s see if we can visualize that information, my best guess would be a bar chart

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pricing\PYGZus{}most\PYGZus{}important}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                                 parameter  relative\PYGZus{}correlation
product\PYGZus{}category\PYGZus{}name                                                           
small\PYGZus{}appliances\PYGZus{}home\PYGZus{}oven\PYGZus{}and\PYGZus{}coffee   product\PYGZus{}photos\PYGZus{}qty              0.705878
computers                               product\PYGZus{}photos\PYGZus{}qty              0.703822
furniture\PYGZus{}bedroom                       product\PYGZus{}photos\PYGZus{}qty              0.638002
home\PYGZus{}confort                           product\PYGZus{}name\PYGZus{}lenght              0.631313
fashion\PYGZus{}shoes                          product\PYGZus{}name\PYGZus{}lenght              0.604471
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{top\PYGZus{}n} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{barplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{pricing\PYGZus{}most\PYGZus{}important}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{n}{top\PYGZus{}n}\PYG{p}{)}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{to\PYGZus{}list}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{pricing\PYGZus{}most\PYGZus{}important}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{n}{top\PYGZus{}n}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relative\PYGZus{}correlation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{n}{palette}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{colorblind}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{idx}\PYG{p}{,} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{patches}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{n}{pricing\PYGZus{}most\PYGZus{}important}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{n}{top\PYGZus{}n}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{parameter}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]}\PYG{p}{,} 
                   \PYG{p}{(}\PYG{n}{p}\PYG{o}{.}\PYG{n}{get\PYGZus{}x}\PYG{p}{(}\PYG{p}{)} \PYG{o}{+} \PYG{n}{p}\PYG{o}{.}\PYG{n}{get\PYGZus{}width}\PYG{p}{(}\PYG{p}{)} \PYG{o}{/} \PYG{l+m+mf}{2.}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} 
                   \PYG{n}{ha} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{va} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bottom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} 
                   \PYG{n}{xytext} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{,} 
                \PYG{n}{rotation} \PYG{o}{=} \PYG{l+m+mi}{90}\PYG{p}{,}
                  \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{white}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                   \PYG{n}{textcoords} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{offset points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{n+nb}{format}\PYG{p}{(}\PYG{n}{p}\PYG{o}{.}\PYG{n}{get\PYGZus{}height}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.1f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} 
                   \PYG{p}{(}\PYG{n}{p}\PYG{o}{.}\PYG{n}{get\PYGZus{}x}\PYG{p}{(}\PYG{p}{)} \PYG{o}{+} \PYG{n}{p}\PYG{o}{.}\PYG{n}{get\PYGZus{}width}\PYG{p}{(}\PYG{p}{)} \PYG{o}{/} \PYG{l+m+mf}{2.}\PYG{p}{,} \PYG{n}{p}\PYG{o}{.}\PYG{n}{get\PYGZus{}height}\PYG{p}{(}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mf}{0.9}\PYG{p}{)}\PYG{p}{,} 
                   \PYG{n}{ha} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{va} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} 
                   \PYG{n}{xytext} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{,} 
                   \PYG{n}{textcoords} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{offset points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticklabels}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{get\PYGZus{}xticklabels}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{rotation}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{n}{horizontalalignment}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{left}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}Figure size 432x288 with 1 Axes\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
You can see I put a little bit more effort in this last graph as I think this is the nice visualisation to show others. We can also make a similar plot but with the relatively least important features.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{top\PYGZus{}n} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{barplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{pricing\PYGZus{}least\PYGZus{}important}\PYG{o}{.}\PYG{n}{tail}\PYG{p}{(}\PYG{n}{top\PYGZus{}n}\PYG{p}{)}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{to\PYGZus{}list}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{pricing\PYGZus{}least\PYGZus{}important}\PYG{o}{.}\PYG{n}{tail}\PYG{p}{(}\PYG{n}{top\PYGZus{}n}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relative\PYGZus{}correlation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{n}{palette}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{colorblind}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{idx}\PYG{p}{,} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{patches}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{n}{pricing\PYGZus{}least\PYGZus{}important}\PYG{o}{.}\PYG{n}{tail}\PYG{p}{(}\PYG{n}{top\PYGZus{}n}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{parameter}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]}\PYG{p}{,} 
                   \PYG{p}{(}\PYG{n}{p}\PYG{o}{.}\PYG{n}{get\PYGZus{}x}\PYG{p}{(}\PYG{p}{)} \PYG{o}{+} \PYG{n}{p}\PYG{o}{.}\PYG{n}{get\PYGZus{}width}\PYG{p}{(}\PYG{p}{)} \PYG{o}{/} \PYG{l+m+mf}{2.}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} 
                   \PYG{n}{ha} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{va} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{top}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} 
                   \PYG{n}{xytext} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{,} 
                \PYG{n}{rotation} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{90}\PYG{p}{,}
                  \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{white}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                   \PYG{n}{textcoords} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{offset points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{n+nb}{format}\PYG{p}{(}\PYG{n}{p}\PYG{o}{.}\PYG{n}{get\PYGZus{}height}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{.1f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} 
                   \PYG{p}{(}\PYG{n}{p}\PYG{o}{.}\PYG{n}{get\PYGZus{}x}\PYG{p}{(}\PYG{p}{)} \PYG{o}{+} \PYG{n}{p}\PYG{o}{.}\PYG{n}{get\PYGZus{}width}\PYG{p}{(}\PYG{p}{)} \PYG{o}{/} \PYG{l+m+mf}{2.}\PYG{p}{,} \PYG{n}{p}\PYG{o}{.}\PYG{n}{get\PYGZus{}height}\PYG{p}{(}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mf}{0.9}\PYG{p}{)}\PYG{p}{,} 
                   \PYG{n}{ha} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{va} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} 
                   \PYG{n}{xytext} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{,} 
                   \PYG{n}{textcoords} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{offset points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticklabels}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{get\PYGZus{}xticklabels}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{rotation}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{n}{horizontalalignment}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{left}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}Figure size 432x288 with 1 Axes\PYGZgt{}
\end{sphinxVerbatim}


\section{Summary}
\label{\detokenize{c7_case_studies/Olist:summary}}

\subsection{Product pricing}
\label{\detokenize{c7_case_studies/Olist:id8}}
\sphinxAtStartPar
To conclude the product pricing analysis, we checked for normal distributions which werent present, so we had to opt for non\sphinxhyphen{}parametric/non\sphinxhyphen{}linear methods (although in many cases these will still do fine). We checked for numerical correlations but these were not really interesting, which led to the idea that perhaps per category our price could be predicted more accurate. This was proven by the fact that our price surely differs inbetween categories.

\sphinxAtStartPar
We split up our dataset by grouping per category and removing small categories, now we could see that a relative change in correlation \sphinxhyphen{} meaning that the correlation of a column in our dataset with the price was different in that category compared to the overall correlation of this column with the price \sphinxhyphen{} was present for all categories. For each category we selected both the highest increase in correlation \sphinxhyphen{} meaning a ‘spike’ in importance \sphinxhyphen{} for that category and the highest decrease \sphinxhyphen{} meaning a ‘drop’ in importance \sphinxhyphen{} for that category.

\sphinxAtStartPar
These plots hence show the most important and least important attributes for an item concerning the price e.g. if we want to increase the price of an item in the computers category, we need to make sure it has enough pictures and not try to decrease the weight value.


\section{Playground}
\label{\detokenize{c7_case_studies/Olist:playground}}
\sphinxAtStartPar
here I can experiment with new ideas, and so can you!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{order\PYGZus{}reviews}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                              review\PYGZus{}id  ... review\PYGZus{}answer\PYGZus{}timestamp
0      7bc2406110b926393aa56f80a40eba40  ...     2018\PYGZhy{}01\PYGZhy{}18 21:46:59
1      80e641a11e56f04c1ad469d5645fdfde  ...     2018\PYGZhy{}03\PYGZhy{}11 03:05:13
2      228ce5500dc1d8e020d8d1322874b6f0  ...     2018\PYGZhy{}02\PYGZhy{}18 14:36:24
3      e64fb393e7b32834bb789ff8bb30750e  ...     2017\PYGZhy{}04\PYGZhy{}21 22:02:06
4      f7c4243c7fe1938f181bec41a392bdeb  ...     2018\PYGZhy{}03\PYGZhy{}02 10:26:53
...                                 ...  ...                     ...
99219  574ed12dd733e5fa530cfd4bbf39d7c9  ...     2018\PYGZhy{}07\PYGZhy{}14 17:18:30
99220  f3897127253a9592a73be9bdfdf4ed7a  ...     2017\PYGZhy{}12\PYGZhy{}11 20:06:42
99221  b3de70c89b1510c4cd3d0649fd302472  ...     2018\PYGZhy{}03\PYGZhy{}23 09:10:43
99222  1adeb9d84d72fe4e337617733eb85149  ...     2018\PYGZhy{}07\PYGZhy{}02 12:59:13
99223  efe49f1d6f951dd88b51e6ccd4cc548f  ...     2017\PYGZhy{}07\PYGZhy{}03 21:01:49

[99224 rows x 7 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reviews\PYGZus{}per\PYGZus{}seller} \PYG{o}{=} \PYG{n}{order\PYGZus{}reviews}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{orders}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{left}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{left\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{order\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{right\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{av\PYGZus{}review} \PYG{o}{=} \PYG{n}{reviews\PYGZus{}per\PYGZus{}seller}\PYG{p}{[}\PYG{n}{reviews\PYGZus{}per\PYGZus{}seller}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{order\PYGZus{}item\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{seller\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{review\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{agg}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{total\PYGZus{}revenue} \PYG{o}{=} \PYG{n}{orders}\PYG{p}{[}\PYG{n}{orders}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{order\PYGZus{}item\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==}\PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{seller\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{total\PYGZus{}revenue}\PYG{o}{.}\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{revenue}\PYG{l+s+s1}{\PYGZsq{}}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reviews} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{av\PYGZus{}review}\PYG{p}{,} \PYG{n}{total\PYGZus{}revenue}\PYG{p}{,} \PYG{n}{left\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{right\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{reviews}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                  count      mean   revenue
seller\PYGZus{}id                                                  
0015a82c2db000af6aaaf3ae2ecb0532      3  3.666667   2685.00
001cca7ae9ae17fb1caed9dfb1094831    196  4.000000  20904.54
001e6ad469a905060d959994f1b41e4f      1  1.000000    250.00
002100f778ceb8431b7a1020ff7ab48f     51  3.882353   1151.00
003554e2dce176b5555353e4f3555ac8      1  5.000000    120.00
...                                 ...       ...       ...
ffcfefa19b08742c5d315f2791395ee5      1  1.000000     69.90
ffdd9f82b9a447f6f8d4b91554cc7dd3     18  4.333333   1823.60
ffeee66ac5d5a62fe688b9d26f83f534     14  4.214286   1839.86
fffd5413c0700ac820c7069d66d98c89     56  3.964286   8660.60
ffff564a4f9085cd26170f4732393726     20  2.100000   1426.30

[3083 rows x 3 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reviews}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spearman}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            count      mean   revenue
count    1.000000 \PYGZhy{}0.108698  0.850416
mean    \PYGZhy{}0.108698  1.000000 \PYGZhy{}0.109698
revenue  0.850416 \PYGZhy{}0.109698  1.000000
\end{sphinxVerbatim}


\chapter{Case Study: Olympic medals}
\label{\detokenize{c7_case_studies/Churn:case-study-olympic-medals}}\label{\detokenize{c7_case_studies/Churn::doc}}
\sphinxAtStartPar
In this case study we try to create an answer why customers have left our service, a telecom operator.

\sphinxAtStartPar
The case study is divided into several parts:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Goals

\item {} 
\sphinxAtStartPar
Parsing

\item {} 
\sphinxAtStartPar
Preparation (cleaning)

\item {} 
\sphinxAtStartPar
Processing

\item {} 
\sphinxAtStartPar
Exploration

\item {} 
\sphinxAtStartPar
Visualization

\item {} 
\sphinxAtStartPar
Conclusion

\end{itemize}


\section{Goals}
\label{\detokenize{c7_case_studies/Churn:goals}}
\sphinxAtStartPar
In this section we define questions that will be our guideline througout the case study
\begin{itemize}
\item {} 
\sphinxAtStartPar
Why are customers leaving us?

\item {} 
\sphinxAtStartPar
Can we cluster types of customers?

\end{itemize}

\sphinxAtStartPar
We’ll (try to) keep these question in mind when performing the case study.


\section{Parsing}
\label{\detokenize{c7_case_studies/Churn:parsing}}
\sphinxAtStartPar
we start out by importing all libraries

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{json}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{set\PYGZus{}matplotlib\PYGZus{}formats}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\PYG{n}{set\PYGZus{}matplotlib\PYGZus{}formats}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{svg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/tmp/ipykernel\PYGZus{}13857/4057771804.py:10: DeprecationWarning: `set\PYGZus{}matplotlib\PYGZus{}formats` is deprecated since IPython 7.23, directly use `matplotlib\PYGZus{}inline.backend\PYGZus{}inline.set\PYGZus{}matplotlib\PYGZus{}formats()`
  set\PYGZus{}matplotlib\PYGZus{}formats(\PYGZsq{}svg\PYGZsq{})
\end{sphinxVerbatim}

\sphinxAtStartPar
in order to download datasets from kaggle, we need an API key to access their API, we’ll make that here

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{mkdir}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/root/.kaggle/kaggle.json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}
        \PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{username}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lorenzf}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{7a44a9e99b27e796177d793a3d85b8cf}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{\PYGZcb{}}
        \PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{PermissionError}\PYG{g+gWhitespace}{                           }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}13857}\PYG{o}{/}\PYG{l+m+mf}{3113012040.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{1} \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{2}     \PYG{n}{os}\PYG{o}{.}\PYG{n}{mkdir}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{3} 
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{4} \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/root/.kaggle/kaggle.json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{5}     \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}

\PYG{n+ne}{PermissionError}: [Errno 13] Permission denied: \PYGZsq{}/root/.kaggle\PYGZsq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
now we can import kaggle too and download the datasets

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kaggle}
\PYG{n}{kaggle}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{dataset\PYGZus{}download\PYGZus{}files}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blastchar/telco\PYGZhy{}customer\PYGZhy{}churn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{unzip}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run \PYGZsq{}chmod 600 /root/.kaggle/kaggle.json\PYGZsq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
the csv files are now in the ‘./data’ folder, we can now read them using pandas, here is the list of all csv files in our folder

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{os}\PYG{o}{.}\PYG{n}{listdir}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}WA\PYGZus{}Fn\PYGZhy{}UseC\PYGZus{}\PYGZhy{}Telco\PYGZhy{}Customer\PYGZhy{}Churn.csv\PYGZsq{}]
\end{sphinxVerbatim}

\sphinxAtStartPar
This dataset only contains 1 file, in it each row has all the information about a single customer and which services he or she has or had before churning.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{churn\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/WA\PYGZus{}Fn\PYGZhy{}UseC\PYGZus{}\PYGZhy{}Telco\PYGZhy{}Customer\PYGZhy{}Churn.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
shape: (7043, 21)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   customerID  gender  SeniorCitizen  ... MonthlyCharges TotalCharges  Churn
0  7590\PYGZhy{}VHVEG  Female              0  ...          29.85        29.85     No
1  5575\PYGZhy{}GNVDE    Male              0  ...          56.95       1889.5     No
2  3668\PYGZhy{}QPYBK    Male              0  ...          53.85       108.15    Yes
3  7795\PYGZhy{}CFOCW    Male              0  ...          42.30      1840.75     No
4  9237\PYGZhy{}HQITU  Female              0  ...          70.70       151.65    Yes

[5 rows x 21 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Looks like there is some personal info and the configuration of the service, such as if they had an internet service, with or without options such as security, backup,…
By the lookds of it these Yes/No answers are not booleans (i.e. 2 options) but rather categories as they have a third option, ‘No … service’.


\section{Preparation}
\label{\detokenize{c7_case_studies/Churn:preparation}}
\sphinxAtStartPar
here we perform tasks to prepare the data in a more pleasing format.


\subsection{Data Types}
\label{\detokenize{c7_case_studies/Churn:data-types}}
\sphinxAtStartPar
Before we do anything with our data, it is good to see if our data types are in order

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 7043 entries, 0 to 7042
Data columns (total 21 columns):
 \PYGZsh{}   Column            Non\PYGZhy{}Null Count  Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}            \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   customerID        7043 non\PYGZhy{}null   object 
 1   gender            7043 non\PYGZhy{}null   object 
 2   SeniorCitizen     7043 non\PYGZhy{}null   int64  
 3   Partner           7043 non\PYGZhy{}null   object 
 4   Dependents        7043 non\PYGZhy{}null   object 
 5   tenure            7043 non\PYGZhy{}null   int64  
 6   PhoneService      7043 non\PYGZhy{}null   object 
 7   MultipleLines     7043 non\PYGZhy{}null   object 
 8   InternetService   7043 non\PYGZhy{}null   object 
 9   OnlineSecurity    7043 non\PYGZhy{}null   object 
 10  OnlineBackup      7043 non\PYGZhy{}null   object 
 11  DeviceProtection  7043 non\PYGZhy{}null   object 
 12  TechSupport       7043 non\PYGZhy{}null   object 
 13  StreamingTV       7043 non\PYGZhy{}null   object 
 14  StreamingMovies   7043 non\PYGZhy{}null   object 
 15  Contract          7043 non\PYGZhy{}null   object 
 16  PaperlessBilling  7043 non\PYGZhy{}null   object 
 17  PaymentMethod     7043 non\PYGZhy{}null   object 
 18  MonthlyCharges    7043 non\PYGZhy{}null   float64
 19  TotalCharges      7043 non\PYGZhy{}null   object 
 20  Churn             7043 non\PYGZhy{}null   object 
dtypes: float64(1), int64(2), object(18)
memory usage: 1.1+ MB
\end{sphinxVerbatim}

\sphinxAtStartPar
I am opting to change the sernior citizan from 0/1 to No/Yes and convert them all to categories, let’s do that right now.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{SeniorCitizen} \PYG{o}{=} \PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{SeniorCitizen}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{churn\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SeniorCitizen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Partner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dependents}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhoneService}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MultipleLines}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{InternetService}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OnlineSecurity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OnlineBackup}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DeviceProtection}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TechSupport}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StreamingTV}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StreamingMovies}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Contract}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PaperlessBilling}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PaymentMethod}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Churn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]} \PYG{o}{=} \PYG{n}{churn\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gender}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SeniorCitizen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Partner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dependents}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PhoneService}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MultipleLines}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{InternetService}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OnlineSecurity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OnlineBackup}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DeviceProtection}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TechSupport}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StreamingTV}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{StreamingMovies}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Contract}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PaperlessBilling}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PaymentMethod}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Churn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 7043 entries, 0 to 7042
Data columns (total 21 columns):
 \PYGZsh{}   Column            Non\PYGZhy{}Null Count  Dtype   
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}            \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   
 0   customerID        7043 non\PYGZhy{}null   object  
 1   gender            7043 non\PYGZhy{}null   category
 2   SeniorCitizen     7043 non\PYGZhy{}null   category
 3   Partner           7043 non\PYGZhy{}null   category
 4   Dependents        7043 non\PYGZhy{}null   category
 5   tenure            7043 non\PYGZhy{}null   int64   
 6   PhoneService      7043 non\PYGZhy{}null   category
 7   MultipleLines     7043 non\PYGZhy{}null   category
 8   InternetService   7043 non\PYGZhy{}null   category
 9   OnlineSecurity    7043 non\PYGZhy{}null   category
 10  OnlineBackup      7043 non\PYGZhy{}null   category
 11  DeviceProtection  7043 non\PYGZhy{}null   category
 12  TechSupport       7043 non\PYGZhy{}null   category
 13  StreamingTV       7043 non\PYGZhy{}null   category
 14  StreamingMovies   7043 non\PYGZhy{}null   category
 15  Contract          7043 non\PYGZhy{}null   category
 16  PaperlessBilling  7043 non\PYGZhy{}null   category
 17  PaymentMethod     7043 non\PYGZhy{}null   category
 18  MonthlyCharges    7043 non\PYGZhy{}null   float64 
 19  TotalCharges      7043 non\PYGZhy{}null   object  
 20  Churn             7043 non\PYGZhy{}null   category
dtypes: category(17), float64(1), int64(1), object(2)
memory usage: 338.9+ KB
\end{sphinxVerbatim}

\sphinxAtStartPar
Now our yes/no answers are configured as categories, for numbers we see that there are 2: ‘MontlyCharges’ and ‘TotalCharges’.
I’m going to make them floating numbers

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{churn\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MonthlyCharges}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TotalCharges}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]} \PYG{o}{=} \PYG{n}{churn\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MonthlyCharges}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TotalCharges}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{float}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ValueError}\PYG{g+gWhitespace}{                                }Traceback (most recent call last)
\PYG{n+nn}{\PYGZlt{}ipython\PYGZhy{}input\PYGZhy{}8\PYGZhy{}0ae90e86afc5\PYGZgt{}} in \PYG{n+ni}{\PYGZlt{}module\PYGZgt{}}\PYG{n+nt}{()}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{n}{churn\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MonthlyCharges}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TotalCharges}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]} \PYG{o}{=} \PYG{n}{churn\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MonthlyCharges}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TotalCharges}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{float}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{2} \PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}

\PYG{n+nn}{/usr/local/lib/python3.7/dist\PYGZhy{}packages/pandas/core/generic.py} in \PYG{n+ni}{astype}\PYG{n+nt}{(self, dtype, copy, errors)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{5546}         \PYG{k}{else}\PYG{p}{:}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{5547}             \PYG{c+c1}{\PYGZsh{} else, only a single dtype is given}
\PYG{n+ne}{\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{5548}             \PYG{n}{new\PYGZus{}data} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}mgr}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{dtype}\PYG{o}{=}\PYG{n}{dtype}\PYG{p}{,} \PYG{n}{copy}\PYG{o}{=}\PYG{n}{copy}\PYG{p}{,} \PYG{n}{errors}\PYG{o}{=}\PYG{n}{errors}\PYG{p}{,}\PYG{p}{)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{5549}             \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}constructor}\PYG{p}{(}\PYG{n}{new\PYGZus{}data}\PYG{p}{)}\PYG{o}{.}\PYG{n}{\PYGZus{}\PYGZus{}finalize\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{astype}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{5550} 

\PYG{n+nn}{/usr/local/lib/python3.7/dist\PYGZhy{}packages/pandas/core/internals/managers.py} in \PYG{n+ni}{astype}\PYG{n+nt}{(self, dtype, copy, errors)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{602}         \PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{dtype}\PYG{p}{,} \PYG{n}{copy}\PYG{p}{:} \PYG{n+nb}{bool} \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{errors}\PYG{p}{:} \PYG{n+nb}{str} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{raise}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{603}     \PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{BlockManager}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{604}         \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{astype}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{dtype}\PYG{p}{,} \PYG{n}{copy}\PYG{o}{=}\PYG{n}{copy}\PYG{p}{,} \PYG{n}{errors}\PYG{o}{=}\PYG{n}{errors}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{605} 
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{606}     \PYG{k}{def} \PYG{n+nf}{convert}\PYG{p}{(}

\PYG{n+nn}{/usr/local/lib/python3.7/dist\PYGZhy{}packages/pandas/core/internals/managers.py} in \PYG{n+ni}{apply}\PYG{n+nt}{(self, f, align\PYGZus{}keys, **kwargs)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{407}                 \PYG{n}{applied} \PYG{o}{=} \PYG{n}{b}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{n}{f}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{408}             \PYG{k}{else}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{409}                 \PYG{n}{applied} \PYG{o}{=} \PYG{n+nb}{getattr}\PYG{p}{(}\PYG{n}{b}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{410}             \PYG{n}{result\PYGZus{}blocks} \PYG{o}{=} \PYG{n}{\PYGZus{}extend\PYGZus{}blocks}\PYG{p}{(}\PYG{n}{applied}\PYG{p}{,} \PYG{n}{result\PYGZus{}blocks}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{411} 

\PYG{n+nn}{/usr/local/lib/python3.7/dist\PYGZhy{}packages/pandas/core/internals/blocks.py} in \PYG{n+ni}{astype}\PYG{n+nt}{(self, dtype, copy, errors)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{593}             \PYG{n}{vals1d} \PYG{o}{=} \PYG{n}{values}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{594}             \PYG{k}{try}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{595}                 \PYG{n}{values} \PYG{o}{=} \PYG{n}{astype\PYGZus{}nansafe}\PYG{p}{(}\PYG{n}{vals1d}\PYG{p}{,} \PYG{n}{dtype}\PYG{p}{,} \PYG{n}{copy}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{596}             \PYG{k}{except} \PYG{p}{(}\PYG{n+ne}{ValueError}\PYG{p}{,} \PYG{n+ne}{TypeError}\PYG{p}{)}\PYG{p}{:}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{597}                 \PYG{c+c1}{\PYGZsh{} e.g. astype\PYGZus{}nansafe can fail on object\PYGZhy{}dtype of strings}

\PYG{n+nn}{/usr/local/lib/python3.7/dist\PYGZhy{}packages/pandas/core/dtypes/cast.py} in \PYG{n+ni}{astype\PYGZus{}nansafe}\PYG{n+nt}{(arr, dtype, copy, skipna)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{995}     \PYG{k}{if} \PYG{n}{copy} \PYG{o+ow}{or} \PYG{n}{is\PYGZus{}object\PYGZus{}dtype}\PYG{p}{(}\PYG{n}{arr}\PYG{p}{)} \PYG{o+ow}{or} \PYG{n}{is\PYGZus{}object\PYGZus{}dtype}\PYG{p}{(}\PYG{n}{dtype}\PYG{p}{)}\PYG{p}{:}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{996}         \PYG{c+c1}{\PYGZsh{} Explicit copy, or required since NumPy can\PYGZsq{}t view from / to object.}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{997}         \PYG{k}{return} \PYG{n}{arr}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{dtype}\PYG{p}{,} \PYG{n}{copy}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{998} 
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{999}     \PYG{k}{return} \PYG{n}{arr}\PYG{o}{.}\PYG{n}{view}\PYG{p}{(}\PYG{n}{dtype}\PYG{p}{)}

\PYG{n+ne}{ValueError}: could not convert string to float: 
\end{sphinxVerbatim}

\sphinxAtStartPar
Looks like we have encountered some problems, there are strings in the Total charges that are not able to be converted to a decimal number.
We print out the rows that create an error and observe.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{churn\PYGZus{}df}\PYG{p}{[}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}numeric}\PYG{p}{(}\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{TotalCharges}\PYG{p}{,}\PYG{n}{errors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{coerce}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
      customerID  gender SeniorCitizen  ... MonthlyCharges TotalCharges  Churn
488   4472\PYGZhy{}LVYGI  Female            No  ...          52.55                  No
753   3115\PYGZhy{}CZMZD    Male            No  ...          20.25                  No
936   5709\PYGZhy{}LVOEQ  Female            No  ...          80.85                  No
1082  4367\PYGZhy{}NUYAO    Male            No  ...          25.75                  No
1340  1371\PYGZhy{}DWPAZ  Female            No  ...          56.05                  No
3331  7644\PYGZhy{}OMVMY    Male            No  ...          19.85                  No
3826  3213\PYGZhy{}VVOLG    Male            No  ...          25.35                  No
4380  2520\PYGZhy{}SGTTA  Female            No  ...          20.00                  No
5218  2923\PYGZhy{}ARZLG    Male            No  ...          19.70                  No
6670  4075\PYGZhy{}WKNIU  Female            No  ...          73.35                  No
6754  2775\PYGZhy{}SEFEE    Male            No  ...          61.90                  No

[11 rows x 21 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Seems that there are some customers being so new they have no total charges, for convenience i’m going to change the space to a 0.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{TotalCharges} \PYG{o}{=} \PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{TotalCharges}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{churn\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MonthlyCharges}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TotalCharges}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]} \PYG{o}{=} \PYG{n}{churn\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MonthlyCharges}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TotalCharges}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{float}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 7043 entries, 0 to 7042
Data columns (total 21 columns):
 \PYGZsh{}   Column            Non\PYGZhy{}Null Count  Dtype   
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}            \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   
 0   customerID        7043 non\PYGZhy{}null   object  
 1   gender            7043 non\PYGZhy{}null   category
 2   SeniorCitizen     7043 non\PYGZhy{}null   category
 3   Partner           7043 non\PYGZhy{}null   category
 4   Dependents        7043 non\PYGZhy{}null   category
 5   tenure            7043 non\PYGZhy{}null   int64   
 6   PhoneService      7043 non\PYGZhy{}null   category
 7   MultipleLines     7043 non\PYGZhy{}null   category
 8   InternetService   7043 non\PYGZhy{}null   category
 9   OnlineSecurity    7043 non\PYGZhy{}null   category
 10  OnlineBackup      7043 non\PYGZhy{}null   category
 11  DeviceProtection  7043 non\PYGZhy{}null   category
 12  TechSupport       7043 non\PYGZhy{}null   category
 13  StreamingTV       7043 non\PYGZhy{}null   category
 14  StreamingMovies   7043 non\PYGZhy{}null   category
 15  Contract          7043 non\PYGZhy{}null   category
 16  PaperlessBilling  7043 non\PYGZhy{}null   category
 17  PaymentMethod     7043 non\PYGZhy{}null   category
 18  MonthlyCharges    7043 non\PYGZhy{}null   float64 
 19  TotalCharges      7043 non\PYGZhy{}null   float64 
 20  Churn             7043 non\PYGZhy{}null   category
dtypes: category(17), float64(2), int64(1), object(1)
memory usage: 338.9+ KB
\end{sphinxVerbatim}


\subsection{Missing values}
\label{\detokenize{c7_case_studies/Churn:missing-values}}
\sphinxAtStartPar
for each dataframe we apply a few checks in order to see the quality of data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{o}{*}\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{/}\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
customerID          0.0
gender              0.0
SeniorCitizen       0.0
Partner             0.0
Dependents          0.0
tenure              0.0
PhoneService        0.0
MultipleLines       0.0
InternetService     0.0
OnlineSecurity      0.0
OnlineBackup        0.0
DeviceProtection    0.0
TechSupport         0.0
StreamingTV         0.0
StreamingMovies     0.0
Contract            0.0
PaperlessBilling    0.0
PaymentMethod       0.0
MonthlyCharges      0.0
TotalCharges        0.0
Churn               0.0
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
No missing values (if we do not count the ones we solved earlier), sometimes luck is on our side.


\subsection{Duplicates}
\label{\detokenize{c7_case_studies/Churn:duplicates}}
\sphinxAtStartPar
For any reason, our dataset might be containing duplicates that would be counted twice and will introduce a bias we would not want. On the other hand, duplicates can be subjected to interpretation, here we would say that if 2 records are completely the same they are duplicates.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
False
\end{sphinxVerbatim}


\subsection{Indexing}
\label{\detokenize{c7_case_studies/Churn:indexing}}
\sphinxAtStartPar
It is more convenient to work with an index, our dataset already contains an id which we can use as index

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{churn\PYGZus{}df} \PYG{o}{=} \PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{customerID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            gender SeniorCitizen Partner  ... MonthlyCharges  TotalCharges Churn
customerID                                ...                                   
7590\PYGZhy{}VHVEG  Female            No     Yes  ...          29.85         29.85    No
5575\PYGZhy{}GNVDE    Male            No      No  ...          56.95       1889.50    No
3668\PYGZhy{}QPYBK    Male            No      No  ...          53.85        108.15   Yes
7795\PYGZhy{}CFOCW    Male            No      No  ...          42.30       1840.75    No
9237\PYGZhy{}HQITU  Female            No      No  ...          70.70        151.65   Yes

[5 rows x 20 columns]
\end{sphinxVerbatim}


\section{Processing}
\label{\detokenize{c7_case_studies/Churn:processing}}

\subsection{Churn vs no churn}
\label{\detokenize{c7_case_studies/Churn:churn-vs-no-churn}}
\sphinxAtStartPar
I would like to compare between persons that have churned and others, therefore a function that calculates the counts between churn and a given column would be convenient.
By using functions I keep things dynamic without having to store a dataframe for each column, but static dataframes work equally well!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{count\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{col\PYGZus{}name}\PYG{p}{)}\PYG{p}{:}
  \PYG{k}{return} \PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Churn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{col\PYGZus{}name}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{count\PYGZus{}matrix}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DeviceProtection}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
DeviceProtection    No  No internet service   Yes
Churn                                            
No                1884                 1413  1877
Yes               1211                  113   545
\end{sphinxVerbatim}

\sphinxAtStartPar
aside from the counts I would also like to know the mean, as some groups have a smaller population yet their proportion of churned persons might be higher.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{mean\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{col\PYGZus{}name}\PYG{p}{)}\PYG{p}{:}
  \PYG{n}{df} \PYG{o}{=} \PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Churn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{col\PYGZus{}name}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(}\PYG{p}{)}
  \PYG{k}{return} \PYG{n}{df}\PYG{o}{.}\PYG{n}{divide}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{columns}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mean\PYGZus{}matrix}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DeviceProtection}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
DeviceProtection        No  No internet service       Yes
Churn                                                    
No                0.364128             0.273096  0.362775
Yes               0.647940             0.060460  0.291600
\end{sphinxVerbatim}

\sphinxAtStartPar
out of curiosity, let’s print all those ‘mean matrices’

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{col} \PYG{o+ow}{in} \PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Churn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
  \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{mean\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{col}\PYG{p}{)}\PYG{p}{)}
  \PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
gender    Female      Male
Churn                     
No      0.492656  0.507344
Yes     0.502408  0.497592

SeniorCitizen        No       Yes
Churn                            
No             0.871279  0.128721
Yes            0.745318  0.254682

Partner        No       Yes
Churn                      
No       0.471782  0.528218
Yes      0.642055  0.357945

Dependents        No       Yes
Churn                         
No          0.655199  0.344801
Yes         0.825575  0.174425

tenure        0         1         2   ...        70        71        72
Churn                                 ...                              
No      0.002126  0.045033  0.022227  ...  0.020874  0.031697  0.068806
Yes     0.000000  0.203317  0.065811  ...  0.005886  0.003210  0.003210

[2 rows x 73 columns]

PhoneService        No       Yes
Churn                           
No            0.098956  0.901044
Yes           0.090958  0.909042

MultipleLines        No  No phone service       Yes
Churn                                              
No             0.491109          0.098956  0.409934
Yes            0.454254          0.090958  0.454789

InternetService       DSL  Fiber optic        No
Churn                                           
No               0.379204     0.347700  0.273096
Yes              0.245586     0.693954  0.060460

OnlineSecurity        No  No internet service       Yes
Churn                                                  
No              0.393699             0.273096  0.333204
Yes             0.781701             0.060460  0.157838

OnlineBackup        No  No internet service       Yes
Churn                                                
No            0.358523             0.273096  0.368380
Yes           0.659711             0.060460  0.279829

DeviceProtection        No  No internet service       Yes
Churn                                                    
No                0.364128             0.273096  0.362775
Yes               0.647940             0.060460  0.291600

TechSupport        No  No internet service       Yes
Churn                                               
No           0.391767             0.273096  0.335137
Yes          0.773676             0.060460  0.165864

StreamingTV        No  No internet service       Yes
Churn                                               
No           0.361036             0.273096  0.365868
Yes          0.504013             0.060460  0.435527

StreamingMovies        No  No internet service       Yes
Churn                                                   
No               0.356977             0.273096  0.369927
Yes              0.501873             0.060460  0.437667

Contract  Month\PYGZhy{}to\PYGZhy{}month  One year  Two year
Churn                                       
No              0.429068  0.252609  0.318322
Yes             0.885500  0.088818  0.025682

PaperlessBilling        No       Yes
Churn                               
No                0.464438  0.535562
Yes               0.250936  0.749064

PaymentMethod  Bank transfer (automatic)  ...  Mailed check
Churn                                     ...              
No                              0.248550  ...      0.252029
Yes                             0.138042  ...      0.164794

[2 rows x 4 columns]

MonthlyCharges    18.25     18.40     18.55   ...    118.60    118.65    118.75
Churn                                         ...                              
No              0.000193  0.000193  0.000193  ...  0.000387  0.000193  0.000193
Yes             0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000

[2 rows x 1585 columns]

TotalCharges   0.00      18.80     18.85    ...   8670.10   8672.45   8684.80
Churn                                       ...                              
No            0.002126  0.000193  0.000193  ...  0.000193  0.000193  0.000000
Yes           0.000000  0.000000  0.000535  ...  0.000000  0.000000  0.000535

[2 rows x 6531 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
We already see some big differences between populations of churn and no churn for some of these features, promising!


\subsection{one hot encoding}
\label{\detokenize{c7_case_studies/Churn:one-hot-encoding}}
\sphinxAtStartPar
I would also like to run the data into an algorithm, yet computers don’t like categories, so I ‘one hot encode’ the categories and get a column/feature for each category in my categorical variables.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{churn\PYGZus{}ohe\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}
    \PYG{p}{[}
     \PYG{n}{pd}\PYG{o}{.}\PYG{n}{get\PYGZus{}dummies}\PYG{p}{(}\PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Churn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
     \PYG{n}{churn\PYGZus{}df}\PYG{o}{.}\PYG{n}{Churn}\PYG{o}{.}\PYG{n}{eq}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}
    \PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{columns}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{)}
\PYG{n}{churn\PYGZus{}ohe\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            tenure  MonthlyCharges  ...  PaymentMethod\PYGZus{}Mailed check  Churn
customerID                          ...                                   
7590\PYGZhy{}VHVEG       1           29.85  ...                           0      0
5575\PYGZhy{}GNVDE      34           56.95  ...                           1      0
3668\PYGZhy{}QPYBK       2           53.85  ...                           1      1
7795\PYGZhy{}CFOCW      45           42.30  ...                           0      0
9237\PYGZhy{}HQITU       2           70.70  ...                           0      1

[5 rows x 47 columns]
\end{sphinxVerbatim}


\subsection{correlation}
\label{\detokenize{c7_case_studies/Churn:correlation}}
\sphinxAtStartPar
I went ahead and already calculated the correlation matrix for this dataset, with the ohe version of the data we can figure out which categories are related.
In the next cell I printed out all correlations with the churn feature.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{churn\PYGZus{}corr\PYGZus{}df} \PYG{o}{=} \PYG{n}{churn\PYGZus{}ohe\PYGZus{}df}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{churn\PYGZus{}corr\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Churn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
tenure                                    \PYGZhy{}0.352229
MonthlyCharges                             0.193356
TotalCharges                              \PYGZhy{}0.198324
gender\PYGZus{}Female                              0.008612
gender\PYGZus{}Male                               \PYGZhy{}0.008612
SeniorCitizen\PYGZus{}No                          \PYGZhy{}0.150889
SeniorCitizen\PYGZus{}Yes                          0.150889
Partner\PYGZus{}No                                 0.150448
Partner\PYGZus{}Yes                               \PYGZhy{}0.150448
Dependents\PYGZus{}No                              0.164221
Dependents\PYGZus{}Yes                            \PYGZhy{}0.164221
PhoneService\PYGZus{}No                           \PYGZhy{}0.011942
PhoneService\PYGZus{}Yes                           0.011942
MultipleLines\PYGZus{}No                          \PYGZhy{}0.032569
MultipleLines\PYGZus{}No phone service            \PYGZhy{}0.011942
MultipleLines\PYGZus{}Yes                          0.040102
InternetService\PYGZus{}DSL                       \PYGZhy{}0.124214
InternetService\PYGZus{}Fiber optic                0.308020
InternetService\PYGZus{}No                        \PYGZhy{}0.227890
OnlineSecurity\PYGZus{}No                          0.342637
OnlineSecurity\PYGZus{}No internet service        \PYGZhy{}0.227890
OnlineSecurity\PYGZus{}Yes                        \PYGZhy{}0.171226
OnlineBackup\PYGZus{}No                            0.268005
OnlineBackup\PYGZus{}No internet service          \PYGZhy{}0.227890
OnlineBackup\PYGZus{}Yes                          \PYGZhy{}0.082255
DeviceProtection\PYGZus{}No                        0.252481
DeviceProtection\PYGZus{}No internet service      \PYGZhy{}0.227890
DeviceProtection\PYGZus{}Yes                      \PYGZhy{}0.066160
TechSupport\PYGZus{}No                             0.337281
TechSupport\PYGZus{}No internet service           \PYGZhy{}0.227890
TechSupport\PYGZus{}Yes                           \PYGZhy{}0.164674
StreamingTV\PYGZus{}No                             0.128916
StreamingTV\PYGZus{}No internet service           \PYGZhy{}0.227890
StreamingTV\PYGZus{}Yes                            0.063228
StreamingMovies\PYGZus{}No                         0.130845
StreamingMovies\PYGZus{}No internet service       \PYGZhy{}0.227890
StreamingMovies\PYGZus{}Yes                        0.061382
Contract\PYGZus{}Month\PYGZhy{}to\PYGZhy{}month                    0.405103
Contract\PYGZus{}One year                         \PYGZhy{}0.177820
Contract\PYGZus{}Two year                         \PYGZhy{}0.302253
PaperlessBilling\PYGZus{}No                       \PYGZhy{}0.191825
PaperlessBilling\PYGZus{}Yes                       0.191825
PaymentMethod\PYGZus{}Bank transfer (automatic)   \PYGZhy{}0.117937
PaymentMethod\PYGZus{}Credit card (automatic)     \PYGZhy{}0.134302
PaymentMethod\PYGZus{}Electronic check             0.301919
PaymentMethod\PYGZus{}Mailed check                \PYGZhy{}0.091683
Churn                                      1.000000
Name: Churn, dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
We can see that complementary categories show an inverse correlation, indicating that we are dealing with a excess of information.
Logical as when option A is not chosen, option B is.
However in this case, as some categoricals have 3 options I opt to keep all info, although it would be a good idea to remove 1 option for each category, this should become appearent in data exploration.


\chapter{Case Study: Olympic medals}
\label{\detokenize{c7_case_studies/Olympics:case-study-olympic-medals}}\label{\detokenize{c7_case_studies/Olympics::doc}}
\sphinxAtStartPar
In this case study we explore the history of medals in the summer and winter olympics

\sphinxAtStartPar
The case study is divided into several parts:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Goals

\item {} 
\sphinxAtStartPar
Parsing

\item {} 
\sphinxAtStartPar
Preparation (cleaning)

\item {} 
\sphinxAtStartPar
Processing

\item {} 
\sphinxAtStartPar
Exploration

\item {} 
\sphinxAtStartPar
Visualization

\item {} 
\sphinxAtStartPar
Conclusion

\end{itemize}


\section{Goals}
\label{\detokenize{c7_case_studies/Olympics:goals}}
\sphinxAtStartPar
In this section we define questions that will be our guideline througout the case study
\begin{itemize}
\item {} 
\sphinxAtStartPar
Which countries are over\sphinxhyphen{}/underperforming?

\item {} 
\sphinxAtStartPar
Are some countries exceptional in some sports?

\item {} 
\sphinxAtStartPar
Do physical traits have an influence on some sports?

\end{itemize}

\sphinxAtStartPar
We’ll (try to) keep these question in mind when performing the case study.


\section{Parsing}
\label{\detokenize{c7_case_studies/Olympics:parsing}}
\sphinxAtStartPar
we start out by importing all necessary libraries

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{json}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{set\PYGZus{}matplotlib\PYGZus{}formats}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\PYG{n}{set\PYGZus{}matplotlib\PYGZus{}formats}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{svg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/tmp/ipykernel\PYGZus{}13954/4057771804.py:10: DeprecationWarning: `set\PYGZus{}matplotlib\PYGZus{}formats` is deprecated since IPython 7.23, directly use `matplotlib\PYGZus{}inline.backend\PYGZus{}inline.set\PYGZus{}matplotlib\PYGZus{}formats()`
  set\PYGZus{}matplotlib\PYGZus{}formats(\PYGZsq{}svg\PYGZsq{})
\end{sphinxVerbatim}

\sphinxAtStartPar
in order to download datasets from kaggle, we need an API key to access their API, we’ll make that here

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{mkdir}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/root/.kaggle/kaggle.json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}
        \PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{username}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lorenzf}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{7a44a9e99b27e796177d793a3d85b8cf}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{\PYGZcb{}}
        \PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{PermissionError}\PYG{g+gWhitespace}{                           }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}13954}\PYG{o}{/}\PYG{l+m+mf}{3113012040.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{1} \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{2}     \PYG{n}{os}\PYG{o}{.}\PYG{n}{mkdir}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{3} 
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{4} \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/root/.kaggle/kaggle.json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{5}     \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}

\PYG{n+ne}{PermissionError}: [Errno 13] Permission denied: \PYGZsq{}/root/.kaggle\PYGZsq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
now we can import kaggle too and download the datasets

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kaggle}
\PYG{n}{kaggle}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{dataset\PYGZus{}download\PYGZus{}files}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{heesoo37/120\PYGZhy{}years\PYGZhy{}of\PYGZhy{}olympic\PYGZhy{}history\PYGZhy{}athletes\PYGZhy{}and\PYGZhy{}results}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{unzip}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run \PYGZsq{}chmod 600 /root/.kaggle/kaggle.json\PYGZsq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
the csv files are now in the ‘./data’ folder, we can now read them using pandas, here is the list of all csv files in our folder

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{os}\PYG{o}{.}\PYG{n}{listdir}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}noc\PYGZus{}regions.csv\PYGZsq{}, \PYGZsq{}athlete\PYGZus{}events.csv\PYGZsq{}]
\end{sphinxVerbatim}

\sphinxAtStartPar
The file of our interest is ‘athlete\_events.csv’, it contains every contestant in every sport since 1896. Let’s print out the top 5 events.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{athlete\PYGZus{}events} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/athlete\PYGZus{}events.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
shape: (271116, 15)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   ID                      Name  ...                             Event  Medal
0   1                 A Dijiang  ...       Basketball Men\PYGZsq{}s Basketball    NaN
1   2                  A Lamusi  ...      Judo Men\PYGZsq{}s Extra\PYGZhy{}Lightweight    NaN
2   3       Gunnar Nielsen Aaby  ...           Football Men\PYGZsq{}s Football    NaN
3   4      Edgar Lindenau Aabye  ...       Tug\PYGZhy{}Of\PYGZhy{}War Men\PYGZsq{}s Tug\PYGZhy{}Of\PYGZhy{}War   Gold
4   5  Christine Jacoba Aaftink  ...  Speed Skating Women\PYGZsq{}s 500 metres    NaN

[5 rows x 15 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Seems we have a name, gender, age, height and weight of the contestant, as wel as the country they represent, the games they attended located in which city. The last 3 columns specify the sport, event within the sport and a possible medal. Presumably the keeping of their score would have been difficult as different sports use different score metrics which would be hard to compare.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{noc\PYGZus{}regions} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/noc\PYGZus{}regions.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{noc\PYGZus{}regions}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{noc\PYGZus{}regions}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
shape: (230, 3)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   NOC       region                 notes
0  AFG  Afghanistan                   NaN
1  AHO      Curacao  Netherlands Antilles
2  ALB      Albania                   NaN
3  ALG      Algeria                   NaN
4  AND      Andorra                   NaN
\end{sphinxVerbatim}


\section{Preparation}
\label{\detokenize{c7_case_studies/Olympics:preparation}}
\sphinxAtStartPar
here we perform tasks to prepare the data in a more pleasing format.


\subsection{Data Types}
\label{\detokenize{c7_case_studies/Olympics:data-types}}
\sphinxAtStartPar
Before we do anything with our data, it is good to see if our data types are in order

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 271116 entries, 0 to 271115
Data columns (total 15 columns):
 \PYGZsh{}   Column  Non\PYGZhy{}Null Count   Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   ID      271116 non\PYGZhy{}null  int64  
 1   Name    271116 non\PYGZhy{}null  object 
 2   Sex     271116 non\PYGZhy{}null  object 
 3   Age     261642 non\PYGZhy{}null  float64
 4   Height  210945 non\PYGZhy{}null  float64
 5   Weight  208241 non\PYGZhy{}null  float64
 6   Team    271116 non\PYGZhy{}null  object 
 7   NOC     271116 non\PYGZhy{}null  object 
 8   Games   271116 non\PYGZhy{}null  object 
 9   Year    271116 non\PYGZhy{}null  int64  
 10  Season  271116 non\PYGZhy{}null  object 
 11  City    271116 non\PYGZhy{}null  object 
 12  Sport   271116 non\PYGZhy{}null  object 
 13  Event   271116 non\PYGZhy{}null  object 
 14  Medal   39783 non\PYGZhy{}null   object 
dtypes: float64(3), int64(2), object(10)
memory usage: 31.0+ MB
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{athlete\PYGZus{}events}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Team}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Season}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{City}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sport}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Event}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]} \PYG{o}{=} \PYG{n}{athlete\PYGZus{}events}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Team}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Season}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{City}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sport}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Event}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 271116 entries, 0 to 271115
Data columns (total 15 columns):
 \PYGZsh{}   Column  Non\PYGZhy{}Null Count   Dtype   
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   
 0   ID      271116 non\PYGZhy{}null  int64   
 1   Name    271116 non\PYGZhy{}null  object  
 2   Sex     271116 non\PYGZhy{}null  category
 3   Age     261642 non\PYGZhy{}null  float64 
 4   Height  210945 non\PYGZhy{}null  float64 
 5   Weight  208241 non\PYGZhy{}null  float64 
 6   Team    271116 non\PYGZhy{}null  category
 7   NOC     271116 non\PYGZhy{}null  object  
 8   Games   271116 non\PYGZhy{}null  object  
 9   Year    271116 non\PYGZhy{}null  int64   
 10  Season  271116 non\PYGZhy{}null  category
 11  City    271116 non\PYGZhy{}null  category
 12  Sport   271116 non\PYGZhy{}null  category
 13  Event   271116 non\PYGZhy{}null  category
 14  Medal   39783 non\PYGZhy{}null   object  
dtypes: category(6), float64(3), int64(2), object(4)
memory usage: 20.8+ MB
\end{sphinxVerbatim}


\subsection{Missing values}
\label{\detokenize{c7_case_studies/Olympics:missing-values}}
\sphinxAtStartPar
for each dataframe we apply a few checks in order to see the quality of data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{o}{*}\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{/}\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ID         0.000000
Name       0.000000
Sex        0.000000
Age        3.494445
Height    22.193821
Weight    23.191180
Team       0.000000
NOC        0.000000
Games      0.000000
Year       0.000000
Season     0.000000
City       0.000000
Sport      0.000000
Event      0.000000
Medal     85.326207
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
Age, 3.5\% missing:

\sphinxAtStartPar
Here we can’t do much about it, we could impute using mean or median by looking at other contestants from the same sport/event, however I  have a feeling that missing ages might be prevalent in the same sports.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{25}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Year
1948    1176
1924    1142
1928     963
1920     845
1900     790
1906     743
1908     649
1956     638
1932     330
1952     277
1904     274
1960     221
1984     216
1936     213
1980     187
1896     163
1912     156
1968     118
1988     110
1972      96
1964      56
1976      52
1992      44
1996       8
1994       2
Name: Age, dtype: int64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sport}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{25}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Sport
Gymnastics           1179
Athletics            1117
Shooting              821
Fencing               715
Cycling               678
Rowing                526
Swimming              524
Art Competitions      507
Wrestling             491
Football              375
Boxing                318
Sailing               285
Weightlifting         206
Hockey                204
Water Polo            200
Equestrianism         193
Basketball            186
Tennis                124
Diving                121
Archery                80
Alpine Skiing          78
Bobsleigh              72
Modern Pentathlon      53
Rugby                  48
Tug\PYGZhy{}Of\PYGZhy{}War             44
Name: Age, dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
Although some sports and years are more problematic, we cannot pinpoint a specific group where ages are missing. Imputing with mean or median would drasticly influence the distribution and standard deviation later on. I opt to leave the missing values as is and drop rows with NaN’s when using age in calculations.

\sphinxAtStartPar
Height \& Weight, 22 \& 23 \% missing:

\sphinxAtStartPar
Similar to the Age, yet much more are missing, to a point where dropping would become problematic. Let’s see if we can find a hotspot of missing data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Height}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Weight}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{by}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Height}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{25}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
      Height  Weight
Year                
1952    7170    7171
1948    6311    6329
1936    6209    6414
1924    4719    5003
1928    4599    4856
1956    3748    3754
1920    3525    3821
1912    3319    3444
1992    3175    3157
1908    2626    2618
1932    2108    2771
1996    1871    1821
1900    1820    1857
1906    1476    1528
1904    1088    1154
1960     961    1048
1988     933     928
1976     876     920
1964     681     708
1984     598     603
1980     588     596
1896     334     331
1972     301     389
1994     187     189
2016     176     223
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sport}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Height}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Weight}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{by}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Height}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{25}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                      Height  Weight
Sport                               
Gymnastics              8045    8372
Athletics               5717    6023
Swimming                4045    4391
Shooting                3779    4148
Fencing                 3773    4195
Art Competitions        3519    3523
Cycling                 2883    3029
Rowing                  2675    2662
Alpine Skiing           2435    2479
Football                2098    2212
Wrestling               1808    1849
Equestrianism           1742    1791
Sailing                 1647    1716
Boxing                  1469    1497
Cross Country Skiing    1464    1596
Hockey                  1102    1150
Speed Skating           1090    1192
Water Polo              1058    1122
Weightlifting            929     134
Ice Hockey               905     923
Tennis                   820     854
Bobsleigh                786     846
Diving                   764     828
Basketball               655     858
Figure Skating           631     786
\end{sphinxVerbatim}

\sphinxAtStartPar
Again, no hotspots. For the same reason (distribution) we will not be imputing values, although for machine learning reasons this might be useful to increase the training pool. We will drop the rows with missing values whenever we use the height/weight columns. It would be wise here to inform our audience that conclusions on this data might be skewed by a possible bias \sphinxhyphen{} there might be a reason the data is missing \sphinxhyphen{} which might in turn cause us to make a wrongful conclusion!

\sphinxAtStartPar
Medal, 85\% Missing:

\sphinxAtStartPar
Lastly we see that most are missing the medal, this is obviously that they did not win one. We could boldly assume that since each event has 3 medals, there must be an average of 20 contestants (17/20 = 85\%). But this might be deviating over time and sport.


\subsection{Duplicates}
\label{\detokenize{c7_case_studies/Olympics:duplicates}}
\sphinxAtStartPar
For any reason, our dataset might be containing duplicates that would be counted twice and will introduce a bias we would not want. On the other hand, duplicates can be subjected to interpretation, here we would say that if 2 records share a name, gender, NOC, Games and event, the rows would be identical.
This would mean that the person would have performed twice in the same event for the same games under the same flag. The illustration below demonstrates a duplicate.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{athlete\PYGZus{}events}\PYG{p}{[}\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{Name} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Jacques Doucet}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
          ID            Name Sex  ...    Sport                  Event   Medal
57956  29661  Jacques Doucet   M  ...  Sailing     Sailing Mixed Open     NaN
57957  29661  Jacques Doucet   M  ...  Sailing  Sailing Mixed 2\PYGZhy{}3 Ton  Silver
57958  29661  Jacques Doucet   M  ...  Sailing  Sailing Mixed 2\PYGZhy{}3 Ton  Silver

[3 rows x 15 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
We can se that Jacques for some reason is listed twice for the Sailing Mixed 2\sphinxhyphen{}3 Ton event. He won silver, but coming in second is no excused to be listed a second time! Perhaps we can find out where things went wrong by investigating in which year the duplicates appear.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{duplicate\PYGZus{}events} \PYG{o}{=} \PYG{n}{athlete\PYGZus{}events}\PYG{p}{[}\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NOC}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Games}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Event}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{duplicate\PYGZus{}events}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Year
1900    110
1908     35
1924    126
1928    347
1932    504
1936    258
1948    100
1968      2
1996      2
1998      3
2002      3
2012      1
Name: Name, dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
Seems most of them happen before 1948, perhaps due to errors in manual entries, it feels safe to delete them.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{athlete\PYGZus{}events} \PYG{o}{=} \PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NOC}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Games}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Event}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Indexing}
\label{\detokenize{c7_case_studies/Olympics:indexing}}
\sphinxAtStartPar
It is more convenient to work with an index, our dataset already contains an id which we can use as index

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{athlete\PYGZus{}events} \PYG{o}{=} \PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                        Name Sex  ...                             Event  Medal
ID                                ...                                         
1                  A Dijiang   M  ...       Basketball Men\PYGZsq{}s Basketball    NaN
2                   A Lamusi   M  ...      Judo Men\PYGZsq{}s Extra\PYGZhy{}Lightweight    NaN
3        Gunnar Nielsen Aaby   M  ...           Football Men\PYGZsq{}s Football    NaN
4       Edgar Lindenau Aabye   M  ...       Tug\PYGZhy{}Of\PYGZhy{}War Men\PYGZsq{}s Tug\PYGZhy{}Of\PYGZhy{}War   Gold
5   Christine Jacoba Aaftink   F  ...  Speed Skating Women\PYGZsq{}s 500 metres    NaN

[5 rows x 14 columns]
\end{sphinxVerbatim}


\section{Processing}
\label{\detokenize{c7_case_studies/Olympics:processing}}

\subsection{Medals per country per sport}
\label{\detokenize{c7_case_studies/Olympics:medals-per-country-per-sport}}
\sphinxAtStartPar
To find out which country (NOC) performs the best, we would like to have a dataframe with 3 columns {[}‘Gold’, ‘Silver’, ‘Bronze’{]} containing the count of each, as row index, we would have the games and the NOC, thus a multiindex.
An important detail is that team sports are given multiple medals, as indicated by the exampe below. Be careful as bias might not always as visible.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{athlete\PYGZus{}events}\PYG{p}{[}\PYG{p}{(}\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{Event} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Basketball Men}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{s Basketball}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{o}{\PYGZam{}}\PYG{p}{(}\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{Games}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1992 Summer}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{\PYGZam{}}\PYG{p}{(}\PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{Medal}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                   Name Sex  ...                        Event  Medal
ID                                           ...                                    
7901               Charles Wade Barkley   M  ...  Basketball Men\PYGZsq{}s Basketball   Gold
11668                    Larry Joe Bird   M  ...  Basketball Men\PYGZsq{}s Basketball   Gold
30009              Clyde Austin Drexler   M  ...  Basketball Men\PYGZsq{}s Basketball   Gold
33553            Patrick Aloysius Ewing   M  ...  Basketball Men\PYGZsq{}s Basketball   Gold
55424       Earvin \PYGZdq{}Magic\PYGZdq{} Johnson, Jr.   M  ...  Basketball Men\PYGZsq{}s Basketball   Gold
55881            Michael Jeffrey Jordan   M  ...  Basketball Men\PYGZsq{}s Basketball   Gold
65809         Christian Donald Laettner   M  ...  Basketball Men\PYGZsq{}s Basketball   Gold
74176                       Karl Malone   M  ...  Basketball Men\PYGZsq{}s Basketball   Gold
83179   Christopher Paul \PYGZdq{}Chris\PYGZdq{} Mullin   M  ...  Basketball Men\PYGZsq{}s Basketball   Gold
95105            Scottie Maurice Pippen   M  ...  Basketball Men\PYGZsq{}s Basketball   Gold
101428           David Maurice Robinson   M  ...  Basketball Men\PYGZsq{}s Basketball   Gold
115325            John Houston Stockton   M  ...  Basketball Men\PYGZsq{}s Basketball   Gold

[12 rows x 14 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
The preprocessing for this dataframe seem complex but is combination of several operations:
\begin{itemize}
\item {} 
\sphinxAtStartPar
drop all records with no medals

\item {} 
\sphinxAtStartPar
drop duplicates based on ‘Games’, ‘NOC’ , ‘Event’, ‘Medal’ to correct for team sports

\item {} 
\sphinxAtStartPar
group per ‘Games’, ‘NOC’ , ‘Medal’

\item {} 
\sphinxAtStartPar
aggregate groups by calculating their size

\end{itemize}

\sphinxAtStartPar
At this point, we have a single column containing the amount of medals and 3 indices: ‘Games’ , ‘NOC’ and ‘Medal’
\begin{itemize}
\item {} 
\sphinxAtStartPar
unstack the ‘Medal’ column to obtain 3 columns ‘Gold’, ‘Silver’, ‘Bronze’

\item {} 
\sphinxAtStartPar
make sure the order of columns is ‘Gold’, ‘Silver’, ‘Bronze’

\item {} 
\sphinxAtStartPar
drop rows where no medals are won, as we do not need those rows

\end{itemize}

\sphinxAtStartPar
This operation looks like the following:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{medals\PYGZus{}country\PYGZus{}df} \PYG{o}{=} \PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Medal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Games}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NOC}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Event}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Games}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NOC}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Medal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sport}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Medal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Silver}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bronze}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{c+c1}{\PYGZsh{}.dropna(how=\PYGZsq{}all\PYGZsq{})\PYGZsh{}.fillna(0)}
\PYG{n}{medals\PYGZus{}country\PYGZus{}df} \PYG{o}{=} \PYG{n}{medals\PYGZus{}country\PYGZus{}df}\PYG{p}{[}\PYG{n}{medals\PYGZus{}country\PYGZus{}df}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{columns}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{medals\PYGZus{}country\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Medal                      Gold  Silver  Bronze
Games       NOC Sport                          
1896 Summer AUS Athletics     2       0       0
                Tennis        0       0       1
            AUT Cycling       1       0       2
                Swimming      1       1       0
            DEN Fencing       0       0       1
...                         ...     ...     ...
2016 Summer UZB Wrestling     0       0       3
            VEN Athletics     0       1       0
                Boxing        0       0       1
                Cycling       0       0       1
            VIE Shooting      1       1       0

[6915 rows x 3 columns]
\end{sphinxVerbatim}


\subsection{average statistics per year, country and sport}
\label{\detokenize{c7_case_studies/Olympics:average-statistics-per-year-country-and-sport}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{avg\PYGZus{}stats\PYGZus{}df} \PYG{o}{=} \PYG{n}{athlete\PYGZus{}events}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{NOC}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Games}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sport}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Height}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Weight}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{avg\PYGZus{}stats\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                Age  Height  Weight
Sex NOC Games       Sport                          
F   AFG 2004 Summer Athletics  18.0   180.0    56.0
                    Judo       18.0   165.0    70.0
        2008 Summer Athletics  22.0   180.0    56.0
        2012 Summer Athletics  23.0   160.0    52.0
        2016 Summer Athletics  20.0   165.0    55.0
...                             ...     ...     ...
M   ZIM 2016 Summer Archery    37.0   186.0    78.0
                    Athletics  29.6   167.6    63.2
                    Rowing     27.0   191.0    87.0
                    Shooting   42.0   182.0    80.0
                    Swimming   22.0   181.0    84.0

[31329 rows x 3 columns]
\end{sphinxVerbatim}


\section{Exploration}
\label{\detokenize{c7_case_studies/Olympics:exploration}}

\section{Visualization}
\label{\detokenize{c7_case_studies/Olympics:visualization}}

\section{Summary}
\label{\detokenize{c7_case_studies/Olympics:summary}}

\chapter{Case Study: User survey}
\label{\detokenize{c7_case_studies/UserSurvey:case-study-user-survey}}\label{\detokenize{c7_case_studies/UserSurvey::doc}}
\sphinxAtStartPar
In this case study we figure out how to analyse the responses from a user survey form kaggle

\sphinxAtStartPar
The case study is divided into several parts:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Goals

\item {} 
\sphinxAtStartPar
Parsing

\item {} 
\sphinxAtStartPar
Preparation (cleaning)

\item {} 
\sphinxAtStartPar
Processing

\item {} 
\sphinxAtStartPar
Exploration

\item {} 
\sphinxAtStartPar
Visualization

\item {} 
\sphinxAtStartPar
Conclusion

\end{itemize}


\section{Goals}
\label{\detokenize{c7_case_studies/UserSurvey:goals}}
\sphinxAtStartPar
In this section we define questions that will be our guideline througout the case study
\begin{itemize}
\item {} 
\sphinxAtStartPar
What influences salary?

\item {} 
\sphinxAtStartPar
Can we deduce common skills for job titles?

\item {} 
\sphinxAtStartPar
Do higher paid jobs spend time differently?

\item {} 
\sphinxAtStartPar
Important: education or experience?

\end{itemize}

\sphinxAtStartPar
We’ll (try to) keep these question in mind when performing the case study.


\section{Parsing}
\label{\detokenize{c7_case_studies/UserSurvey:parsing}}
\sphinxAtStartPar
we start out by importing all necessary libraries

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{json}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{set\PYGZus{}matplotlib\PYGZus{}formats}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\PYG{n}{set\PYGZus{}matplotlib\PYGZus{}formats}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{svg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/tmp/ipykernel\PYGZus{}13986/4057771804.py:10: DeprecationWarning: `set\PYGZus{}matplotlib\PYGZus{}formats` is deprecated since IPython 7.23, directly use `matplotlib\PYGZus{}inline.backend\PYGZus{}inline.set\PYGZus{}matplotlib\PYGZus{}formats()`
  set\PYGZus{}matplotlib\PYGZus{}formats(\PYGZsq{}svg\PYGZsq{})
\end{sphinxVerbatim}

\sphinxAtStartPar
in order to download datasets from kaggle, we need an API key to access their API, we’ll make that here

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{mkdir}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/root/.kaggle/kaggle.json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}
        \PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{username}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lorenzf}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{7a44a9e99b27e796177d793a3d85b8cf}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{\PYGZcb{}}
        \PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{PermissionError}\PYG{g+gWhitespace}{                           }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}13986}\PYG{o}{/}\PYG{l+m+mf}{3113012040.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{1} \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{2}     \PYG{n}{os}\PYG{o}{.}\PYG{n}{mkdir}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{3} 
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{4} \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/root/.kaggle/kaggle.json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{5}     \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}

\PYG{n+ne}{PermissionError}: [Errno 13] Permission denied: \PYGZsq{}/root/.kaggle\PYGZsq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
now we can import kaggle too and download the datasets

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kaggle}
\PYG{n}{kaggle}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{dataset\PYGZus{}download\PYGZus{}files}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{kaggle/kaggle\PYGZhy{}survey\PYGZhy{}2018}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{unzip}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run \PYGZsq{}chmod 600 /root/.kaggle/kaggle.json\PYGZsq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
the csv files are now in the ‘./data’ folder, we can now read them using pandas, here is the list of all csv files in our folder

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{os}\PYG{o}{.}\PYG{n}{listdir}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}freeFormResponses.csv\PYGZsq{}, \PYGZsq{}multipleChoiceResponses.csv\PYGZsq{}, \PYGZsq{}SurveySchema.csv\PYGZsq{}]
\end{sphinxVerbatim}

\sphinxAtStartPar
The file of our interest is ‘athlete\_events.csv’, it contains every contestant in every sport since 1896. Let’s print out the top 5 events.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{choice\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/multipleChoiceResponses.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/usr/local/lib/python3.7/dist\PYGZhy{}packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,2,8,10,21,23,24,25,26,27,28,44,56,64,83,85,87,107,109,123,125,150,157,172,174,194,210,218,219,223,246,249,262,264,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,304,306,325,326,329,341,368,371,384,385,389,390,391,393,394) have mixed types.Specify dtype option on import or set low\PYGZus{}memory=False.
  interactivity=interactivity, compiler=compiler, result=result)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
shape: (23860, 395)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  Time from Start to Finish (seconds)  ...                                     Q50\PYGZus{}OTHER\PYGZus{}TEXT
0               Duration (in seconds)  ...  What barriers prevent you from making your wor...
1                                 710  ...                                                 \PYGZhy{}1
2                                 434  ...                                                 \PYGZhy{}1
3                                 718  ...                                                 \PYGZhy{}1
4                                 621  ...                                                 \PYGZhy{}1

[5 rows x 395 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{free\PYGZus{}form\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/freeFormResponses.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{free\PYGZus{}form\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{free\PYGZus{}form\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
shape: (23860, 35)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/usr/local/lib/python3.7/dist\PYGZhy{}packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (25) have mixed types.Specify dtype option on import or set low\PYGZus{}memory=False.
  interactivity=interactivity, compiler=compiler, result=result)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                      Q11\PYGZus{}OTHER\PYGZus{}TEXT  ...                                      Q7\PYGZus{}OTHER\PYGZus{}TEXT
0  Select any activities that make up an importan...  ...  In what industry is your current employer/cont...
1                                                NaN  ...                                                NaN
2                                                NaN  ...                                                NaN
3                                                NaN  ...                                                NaN
4                                                NaN  ...                                                NaN

[5 rows x 35 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
I saw that the first row of our choice dataframe contains the questions, to let’s extract that.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{questions} \PYG{o}{=} \PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{choice\PYGZus{}df} \PYG{o}{=} \PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{questions}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Time from Start to Finish (seconds)                                Duration (in seconds)
Q1                                                What is your gender? \PYGZhy{} Selected Choice
Q1\PYGZus{}OTHER\PYGZus{}TEXT                          What is your gender? \PYGZhy{} Prefer to self\PYGZhy{}describe...
Q2                                                           What is your age (\PYGZsh{} years)?
Q3                                             In which country do you currently reside?
Q4                                     What is the highest level of formal education ...
Q5                                     Which best describes your undergraduate major?...
Q6                                     Select the title most similar to your current ...
Q6\PYGZus{}OTHER\PYGZus{}TEXT                          Select the title most similar to your current ...
Q7                                     In what industry is your current employer/cont...
Q7\PYGZus{}OTHER\PYGZus{}TEXT                          In what industry is your current employer/cont...
Q8                                     How many years of experience do you have in yo...
Q9                                     What is your current yearly compensation (appr...
Q10                                    Does your current employer incorporate machine...
Q11\PYGZus{}Part\PYGZus{}1                             Select any activities that make up an importan...
Q11\PYGZus{}Part\PYGZus{}2                             Select any activities that make up an importan...
Q11\PYGZus{}Part\PYGZus{}3                             Select any activities that make up an importan...
Q11\PYGZus{}Part\PYGZus{}4                             Select any activities that make up an importan...
Q11\PYGZus{}Part\PYGZus{}5                             Select any activities that make up an importan...
Q11\PYGZus{}Part\PYGZus{}6                             Select any activities that make up an importan...
Name: 0, dtype: object
\end{sphinxVerbatim}


\section{Preparation}
\label{\detokenize{c7_case_studies/UserSurvey:preparation}}
\sphinxAtStartPar
here we perform tasks to prepare the data in a more pleasing format.


\subsection{Data Types}
\label{\detokenize{c7_case_studies/UserSurvey:data-types}}
\sphinxAtStartPar
Before we do anything with our data, it is good to see if our data types are in order

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
Int64Index: 23859 entries, 1 to 23859
Columns: 395 entries, Time from Start to Finish (seconds) to Q50\PYGZus{}OTHER\PYGZus{}TEXT
dtypes: object(395)
memory usage: 72.1+ MB
\end{sphinxVerbatim}

\sphinxAtStartPar
Seems there are to many too show, so we have to do some manual work, The first 10 questions seem to be about personal info, where the first one is about gender

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q1}\PYG{p}{)}
\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q1}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
What is your gender? \PYGZhy{} Selected Choice
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Male                       19430
Female                      4010
Prefer not to say            340
Prefer to self\PYGZhy{}describe       79
Name: Q1, dtype: int64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q1\PYGZus{}OTHER\PYGZus{}TEXT}\PYG{p}{)}
\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q1\PYGZus{}OTHER\PYGZus{}TEXT}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
What is your gender? \PYGZhy{} Prefer to self\PYGZhy{}describe \PYGZhy{} Text
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([\PYGZsq{}\PYGZhy{}1\PYGZsq{}, \PYGZsq{}2\PYGZsq{}, \PYGZsq{}3\PYGZsq{}, \PYGZsq{}4\PYGZsq{}, \PYGZsq{}5\PYGZsq{}, \PYGZsq{}6\PYGZsq{}, \PYGZhy{}1, 7, 8, 9, 10, 11, 12, 13, 14, 15,
       16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 4,
       32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48,
       49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65,
       66, 67], dtype=object)
\end{sphinxVerbatim}

\sphinxAtStartPar
Hmm the self\sphinxhyphen{}describe seems to already been encoded, as there are so many different answers I would opt to ignore those results as they only take up 79 answers of all 24k.
For the second question I am going to convert it to an ordinal value, this way we know the order of the categories.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q2} \PYG{o}{=} \PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q2}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{types}\PYG{o}{.}\PYG{n}{CategoricalDtype}\PYG{p}{(}\PYG{n}{categories}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{18\PYGZhy{}21}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{22\PYGZhy{}24}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{25\PYGZhy{}29}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{30\PYGZhy{}34}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{35\PYGZhy{}39}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{40\PYGZhy{}44}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{45\PYGZhy{}49}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{50\PYGZhy{}54}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{55\PYGZhy{}59}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{60\PYGZhy{}69}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{70\PYGZhy{}79}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{80+}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ordered}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q2}\PYG{p}{)}
\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q2}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
What is your age (\PYGZsh{} years)?
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1        45\PYGZhy{}49
2        30\PYGZhy{}34
3        30\PYGZhy{}34
4        35\PYGZhy{}39
5        22\PYGZhy{}24
         ...  
23855    45\PYGZhy{}49
23856    25\PYGZhy{}29
23857    22\PYGZhy{}24
23858    25\PYGZhy{}29
23859    25\PYGZhy{}29
Name: Q2, Length: 23859, dtype: category
Categories (12, object): [\PYGZsq{}18\PYGZhy{}21\PYGZsq{} \PYGZlt{} \PYGZsq{}22\PYGZhy{}24\PYGZsq{} \PYGZlt{} \PYGZsq{}25\PYGZhy{}29\PYGZsq{} \PYGZlt{} \PYGZsq{}30\PYGZhy{}34\PYGZsq{} ... \PYGZsq{}55\PYGZhy{}59\PYGZsq{} \PYGZlt{} \PYGZsq{}60\PYGZhy{}69\PYGZsq{} \PYGZlt{} \PYGZsq{}70\PYGZhy{}79\PYGZsq{} \PYGZlt{} \PYGZsq{}80+\PYGZsq{}]
\end{sphinxVerbatim}

\sphinxAtStartPar
Next we have a few very important questions that signify the situation of each user in our survey. I chose for nominal categories as I don’t want to be biased.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q6}\PYG{p}{)}
\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q6}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Select the title most similar to your current role (or most recent title if retired): \PYGZhy{} Selected Choice
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Student                    5253
Data Scientist             4137
Software Engineer          3130
Data Analyst               1922
Other                      1322
Research Scientist         1189
Not employed                842
Consultant                  785
Business Analyst            772
Data Engineer               737
Research Assistant          600
Manager                     590
Product/Project Manager     428
Chief Officer               360
Statistician                237
DBA/Database Engineer       145
Developer Advocate          117
Marketing Analyst           115
Salesperson                 102
Principal Investigator       97
Data Journalist              20
Name: Q6, dtype: int64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q6}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q7}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{choice\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q6}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q7}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]} \PYG{o}{=} \PYG{n}{choice\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q6}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q7}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Q3            In which country do you currently reside?
Q4    What is the highest level of formal education ...
Q5    Which best describes your undergraduate major?...
Q6    Select the title most similar to your current ...
Q7    In what industry is your current employer/cont...
Name: 0, dtype: object
\end{sphinxVerbatim}

\sphinxAtStartPar
Question 8 is about experience, or as they call it tenure. Not as a numerical value but in categories, so again I create an ordinal category from it.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q8}\PYG{p}{)}
\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q8}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
How many years of experience do you have in your current role?
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0\PYGZhy{}1      5898
1\PYGZhy{}2      3745
2\PYGZhy{}3      2577
5\PYGZhy{}10     2524
3\PYGZhy{}4      1751
10\PYGZhy{}15    1512
4\PYGZhy{}5      1488
15\PYGZhy{}20     854
20\PYGZhy{}25     384
30 +      197
25\PYGZhy{}30     171
Name: Q8, dtype: int64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q8} \PYG{o}{=} \PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q8}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{types}\PYG{o}{.}\PYG{n}{CategoricalDtype}\PYG{p}{(}\PYG{n}{categories}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{0\PYGZhy{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1\PYGZhy{}2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2\PYGZhy{}3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3\PYGZhy{}4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{4\PYGZhy{}50}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{5\PYGZhy{}10}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{10\PYGZhy{}15}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{15\PYGZhy{}20}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{20\PYGZhy{}25}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{25\PYGZhy{}30}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{30+}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ordered}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q8}\PYG{p}{)}
\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q8}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
How many years of experience do you have in your current role?
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1         NaN
2        5\PYGZhy{}10
3         0\PYGZhy{}1
4         NaN
5         0\PYGZhy{}1
         ... 
23855    5\PYGZhy{}10
23856     NaN
23857     0\PYGZhy{}1
23858     NaN
23859     NaN
Name: Q8, Length: 23859, dtype: category
Categories (11, object): [\PYGZsq{}0\PYGZhy{}1\PYGZsq{} \PYGZlt{} \PYGZsq{}1\PYGZhy{}2\PYGZsq{} \PYGZlt{} \PYGZsq{}2\PYGZhy{}3\PYGZsq{} \PYGZlt{} \PYGZsq{}3\PYGZhy{}4\PYGZsq{} ... \PYGZsq{}15\PYGZhy{}20\PYGZsq{} \PYGZlt{} \PYGZsq{}20\PYGZhy{}25\PYGZsq{} \PYGZlt{} \PYGZsq{}25\PYGZhy{}30\PYGZsq{} \PYGZlt{} \PYGZsq{}30+\PYGZsq{}]
\end{sphinxVerbatim}

\sphinxAtStartPar
And not to forget we have the salary, again as a category, which is unfortunate since we could have been able to create a more accurate prediction in the end.
Here I opt for an ordinal category.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q9}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
I do not wish to disclose my approximate yearly compensation    4756
0\PYGZhy{}10,000                                                        4398
10\PYGZhy{}20,000                                                       1937
20\PYGZhy{}30,000                                                       1395
30\PYGZhy{}40,000                                                       1119
40\PYGZhy{}50,000                                                        965
50\PYGZhy{}60,000                                                        919
100\PYGZhy{}125,000                                                      843
60\PYGZhy{}70,000                                                        729
70\PYGZhy{}80,000                                                        677
90\PYGZhy{}100,000                                                       566
125\PYGZhy{}150,000                                                      533
80\PYGZhy{}90,000                                                        506
150\PYGZhy{}200,000                                                      457
200\PYGZhy{}250,000                                                      172
250\PYGZhy{}300,000                                                       75
500,000+                                                          63
300\PYGZhy{}400,000                                                       52
400\PYGZhy{}500,000                                                       23
Name: Q9, dtype: int64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q9} \PYG{o}{=} \PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q9}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{pd}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{types}\PYG{o}{.}\PYG{n}{CategoricalDtype}\PYG{p}{(}\PYG{n}{categories}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{0\PYGZhy{}10,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{10\PYGZhy{}20,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{20\PYGZhy{}30,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{30\PYGZhy{}40,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{40\PYGZhy{}50,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{50\PYGZhy{}60,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{60\PYGZhy{}70,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{70\PYGZhy{}80,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{80\PYGZhy{}90,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{90\PYGZhy{}100,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{100\PYGZhy{}125,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{125\PYGZhy{}150,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{150\PYGZhy{}200,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{200\PYGZhy{}250,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{250\PYGZhy{}300,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{300\PYGZhy{}400,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{400\PYGZhy{}500,000}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{500,000+}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ordered}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q9}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1                NaN
2          10\PYGZhy{}20,000
3           0\PYGZhy{}10,000
4                NaN
5           0\PYGZhy{}10,000
            ...     
23855    250\PYGZhy{}300,000
23856            NaN
23857      10\PYGZhy{}20,000
23858            NaN
23859            NaN
Name: Q9, Length: 23859, dtype: category
Categories (18, object): [\PYGZsq{}0\PYGZhy{}10,000\PYGZsq{} \PYGZlt{} \PYGZsq{}10\PYGZhy{}20,000\PYGZsq{} \PYGZlt{} \PYGZsq{}20\PYGZhy{}30,000\PYGZsq{} \PYGZlt{} \PYGZsq{}30\PYGZhy{}40,000\PYGZsq{} ... \PYGZsq{}250\PYGZhy{}300,000\PYGZsq{} \PYGZlt{}
                          \PYGZsq{}300\PYGZhy{}400,000\PYGZsq{} \PYGZlt{} \PYGZsq{}400\PYGZhy{}500,000\PYGZsq{} \PYGZlt{} \PYGZsq{}500,000+\PYGZsq{}]
\end{sphinxVerbatim}


\subsection{Missing values}
\label{\detokenize{c7_case_studies/UserSurvey:missing-values}}
\sphinxAtStartPar
for each dataframe we apply a few checks in order to see the quality of data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{o}{*}\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{)}\PYG{o}{/}\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Time from Start to Finish (seconds)     0.000000
Q1                                      0.000000
Q1\PYGZus{}OTHER\PYGZus{}TEXT                           0.000000
Q2                                      0.000000
Q3                                      0.000000
Q4                                      1.764533
Q5                                      3.822457
Q6                                      4.019448
Q6\PYGZus{}OTHER\PYGZus{}TEXT                           0.000000
Q7                                      9.111866
Q7\PYGZus{}OTHER\PYGZus{}TEXT                           0.000000
Q8                                     18.621904
Q9                                     35.332579
Q10                                    13.370217
Q11\PYGZus{}Part\PYGZus{}1                             60.048619
Q11\PYGZus{}Part\PYGZus{}2                             77.027537
Q11\PYGZus{}Part\PYGZus{}3                             78.066977
Q11\PYGZus{}Part\PYGZus{}4                             69.684396
Q11\PYGZus{}Part\PYGZus{}5                             79.320173
Q11\PYGZus{}Part\PYGZus{}6                             85.452031
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
You can clearly see that there are a lot of missing values, for questions 11 and onwards this is just because they did not check that answer on a question, but for 1\sphinxhyphen{}10 this is a problem as these are ‘mandatory’ questions. I have no idea how to fill this in and salary is missing about 35\%, pretty disastrous, but this is to be expected with user surveys.

\sphinxAtStartPar
Another problem we have here is trolls, there might have been persons that would just fill this in to mess with our data collection, I thought they might have been funny and answered a high salary.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{choice\PYGZus{}df}\PYG{p}{[}\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q9}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{500,000+}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{Q2}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
25\PYGZhy{}29    13
35\PYGZhy{}39    10
80+       7
30\PYGZhy{}34     7
50\PYGZhy{}54     6
45\PYGZhy{}49     5
55\PYGZhy{}59     4
22\PYGZhy{}24     4
18\PYGZhy{}21     4
60\PYGZhy{}69     3
70\PYGZhy{}79     0
40\PYGZhy{}44     0
Name: Q2, dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
you can see there are 13 persons between 25\sphinxhyphen{}29 that earn more than 500k annually, which i think is near impossible. Let us see what they are upto.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{choice\PYGZus{}df}\PYG{p}{[}\PYG{p}{(}\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q9}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{500,000+}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q2}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{25\PYGZhy{}29}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
      Time from Start to Finish (seconds)  ... Q50\PYGZus{}OTHER\PYGZus{}TEXT
2322                                  561  ...             \PYGZhy{}1
8899                                 2116  ...            260
12092                                1607  ...             \PYGZhy{}1
13468                                5487  ...             \PYGZhy{}1
14367                              359331  ...             \PYGZhy{}1
15469                                  68  ...             \PYGZhy{}1
15825                                  94  ...             \PYGZhy{}1
16404                                  78  ...             \PYGZhy{}1
18120                                 281  ...             \PYGZhy{}1
20576                                 197  ...             \PYGZhy{}1
21122                                 183  ...             \PYGZhy{}1
22264                                 520  ...             \PYGZhy{}1
22591                                 426  ...             \PYGZhy{}1

[13 rows x 395 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
No way they are this succesfull, i’m not yet going to remove them, but i’m definitely going to keep this in mind, this might break our predictions!

\sphinxAtStartPar
Later on I will remove the entries without salaries, but im going to keep them in a prediction dataframe, so we could perhaps predict their salary, we don’t have a reference but still might be interesting. For the rest of the preparation im going to keep them in here so the final format of both train and prediction are the same.


\subsection{Duplicates}
\label{\detokenize{c7_case_studies/UserSurvey:duplicates}}
\sphinxAtStartPar
It is very highly unlikely but just to check if no one has entered the same survey twice, we check for duplicates

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{choice\PYGZus{}df}\PYG{p}{[}\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
      Time from Start to Finish (seconds)    Q1  ... Q50\PYGZus{}Part\PYGZus{}8 Q50\PYGZus{}OTHER\PYGZus{}TEXT
15278                                  36  Male  ...        NaN             \PYGZhy{}1
15865                                  23  Male  ...        NaN             \PYGZhy{}1
17521                                  36  Male  ...        NaN             \PYGZhy{}1
18257                                  27  Male  ...        NaN             \PYGZhy{}1
18320                                  46  Male  ...        NaN             \PYGZhy{}1
18966                                  43  Male  ...        NaN             \PYGZhy{}1
21214                                 106  Male  ...        NaN             \PYGZhy{}1
21916                                  45  Male  ...        NaN             \PYGZhy{}1
22049                                  46  Male  ...        NaN             \PYGZhy{}1
22638                                  60  Male  ...        NaN             \PYGZhy{}1
22816                                  41  Male  ...        NaN             \PYGZhy{}1
23683                                  70  Male  ...        NaN             \PYGZhy{}1

[12 rows x 395 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
I take back my words, seems there are some faulty entries, perhaps we should even improve our bad entry detection? For now im just going to remove duplicates

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{choice\PYGZus{}df} \PYG{o}{=} \PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
At this point im going to seperate the non salary entries from our training dataframe. resulting in 2 partitions:
\begin{itemize}
\item {} 
\sphinxAtStartPar
train\_df

\item {} 
\sphinxAtStartPar
prediction\_df

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{prediction\PYGZus{}df} \PYG{o}{=} \PYG{n}{choice\PYGZus{}df}\PYG{p}{[}\PYG{p}{(}\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q9}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{o}{|} \PYG{p}{(}\PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{Q9}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{I do not wish to disclose my approximate yearly compensation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{train\PYGZus{}df} \PYG{o}{=} \PYG{n}{choice\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{prediction\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{prediction shape:}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{prediction\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{remaining shape:}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{train\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
prediction shape:(8418, 395)
remaining shape:(15429, 395)
\end{sphinxVerbatim}


\section{Processing}
\label{\detokenize{c7_case_studies/UserSurvey:processing}}
\sphinxAtStartPar
For other questions I selected a few that caught my interest, here is the list that made it. Notice that I did not perform any preparation on these question as they mostly are checkmarks on a survey, yet in processing I am going to create a more convenient method to store them.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q11\PYGZus{}Part\PYGZus{}1}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{}print(questions.Q12\PYGZus{}Part\PYGZus{}1\PYGZus{}TEXT)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q13\PYGZus{}Part\PYGZus{}1}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q16\PYGZus{}Part\PYGZus{}1}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q17}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q19\PYGZus{}Part\PYGZus{}1}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q21\PYGZus{}Part\PYGZus{}1}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q31\PYGZus{}Part\PYGZus{}1}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q34\PYGZus{}Part\PYGZus{}1}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q42\PYGZus{}Part\PYGZus{}1}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{Q49\PYGZus{}Part\PYGZus{}1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Select any activities that make up an important part of your role at work: (Select all that apply) \PYGZhy{} Selected Choice \PYGZhy{} Analyze and understand data to influence product or business decisions
Which of the following integrated development environments (IDE\PYGZsq{}s) have you used at work or school in the last 5 years? (Select all that apply) \PYGZhy{} Selected Choice \PYGZhy{} Jupyter/IPython
What programming languages do you use on a regular basis? (Select all that apply) \PYGZhy{} Selected Choice \PYGZhy{} Python
What specific programming language do you use most often? \PYGZhy{} Selected Choice
What machine learning frameworks have you used in the past 5 years? (Select all that apply) \PYGZhy{} Selected Choice \PYGZhy{} Scikit\PYGZhy{}Learn
What data visualization libraries or tools have you used in the past 5 years? (Select all that apply) \PYGZhy{} Selected Choice \PYGZhy{} ggplot2
Which types of data do you currently interact with most often at work or school? (Select all that apply) \PYGZhy{} Selected Choice \PYGZhy{} Audio Data
During a typical data science project at work or school, approximately what proportion of your time is devoted to the following? (Answers must add up to 100\PYGZpc{}) \PYGZhy{} Gathering data
What metrics do you or your organization use to determine whether or not your models were successful? (Select all that apply) \PYGZhy{} Selected Choice \PYGZhy{} Revenue and/or business goals
What tools and methods do you use to make your work easy to reproduce? (Select all that apply) \PYGZhy{} Selected Choice \PYGZhy{} Share code on Github or a similar code\PYGZhy{}sharing repository
\end{sphinxVerbatim}


\subsection{One hot encoding questions}
\label{\detokenize{c7_case_studies/UserSurvey:one-hot-encoding-questions}}
\sphinxAtStartPar
What I will do here is create a makeshift database, not in SQL as usually just to keep it simple, but in a dictionary of dataframes. For each question I will take the answers and create a one hot encoded table from them, for each user we will know which checkmarks they marked and which they didn’t. This view makes it easier to apply statistics and machine learning to the data.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{answer\PYGZus{}dfs} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{k}{for} \PYG{n}{question} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q11}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q13}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q16}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q19}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q21}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q31}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q34}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q42}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q49}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{:}
  
  \PYG{n}{choices} \PYG{o}{=} \PYG{n}{train\PYGZus{}df}\PYG{p}{[}\PYG{n}{train\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{[}\PYG{n}{train\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(}\PYG{n}{question}\PYG{p}{)}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{notnull}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}
  \PYG{n}{choices}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{n}{questions}\PYG{p}{[}\PYG{n}{questions}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(}\PYG{n}{question}\PYG{p}{)}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ \PYGZhy{} }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{x}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}
  \PYG{n}{answer\PYGZus{}dfs}\PYG{p}{[}\PYG{n}{question}\PYG{p}{]} \PYG{o}{=} \PYG{n}{choices}
\end{sphinxVerbatim}

\sphinxAtStartPar
an example of a question, Q13: Which IDE’s have you used in the last 5 years?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{answer\PYGZus{}dfs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q13}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       Jupyter/IPython  RStudio  PyCharm  ...  Spyder  None  Other
2                    0        0        0  ...       0     1      0
3                    0        0        0  ...       0     0      0
5                    0        1        0  ...       0     0      0
7                    0        1        0  ...       0     0      0
8                    1        0        1  ...       1     0      0
...                ...      ...      ...  ...     ...   ...    ...
23844                1        0        1  ...       0     0      0
23845                0        0        0  ...       0     0      0
23854                0        0        0  ...       0     0      0
23855                1        1        1  ...       0     0      0
23857                0        0        0  ...       0     0      0

[15429 rows x 15 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
for some reason they did Q17 differently, so we have to one hot encode it in another method.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{answer\PYGZus{}dfs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q17}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{get\PYGZus{}dummies}\PYG{p}{(}\PYG{n}{train\PYGZus{}df}\PYG{p}{[}\PYG{n}{train\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{[}\PYG{n}{train\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q17}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{answer\PYGZus{}dfs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q17}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       Q17\PYGZus{}Bash  Q17\PYGZus{}C\PYGZsh{}/.NET  ...  Q17\PYGZus{}Scala  Q17\PYGZus{}Visual Basic/VBA
2             0            0  ...          0                     0
3             0            0  ...          0                     0
5             0            0  ...          0                     0
7             0            0  ...          0                     0
8             0            0  ...          0                     0
...         ...          ...  ...        ...                   ...
23844         0            0  ...          0                     0
23845         0            0  ...          0                     0
23854         0            0  ...          0                     0
23855         0            0  ...          0                     0
23857         0            0  ...          0                     0

[15429 rows x 17 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
That was for our choices data, where the questions are based on choices, for generic info we do it a bit different, we create a general dataframe containing all info.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{info\PYGZus{}df} \PYG{o}{=} \PYG{n}{train\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q6}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q7}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q8}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q9}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q10}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{info\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{n}{questions}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q6}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q7}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q8}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q9}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q10}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{info\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0     What is your gender? \PYGZhy{} Selected Choice  ... Does your current employer incorporate machine learning methods into their business?
2                                       Male  ...                      No (we do not use ML methods)                                  
3                                     Female  ...                                      I do not know                                  
5                                       Male  ...                                      I do not know                                  
7                                       Male  ...                      No (we do not use ML methods)                                  
8                                       Male  ...  We recently started using ML methods (i.e., mo...                                  
...                                      ...  ...                                                ...                                  
23844                                   Male  ...  We are exploring ML methods (and may one day p...                                  
23845                                   Male  ...                                                NaN                                  
23854                                   Male  ...                                                NaN                                  
23855                                   Male  ...  We recently started using ML methods (i.e., mo...                                  
23857                                   Male  ...  We recently started using ML methods (i.e., mo...                                  

[15429 rows x 10 columns]
\end{sphinxVerbatim}


\subsection{Mean choice Matrix}
\label{\detokenize{c7_case_studies/UserSurvey:mean-choice-matrix}}
\sphinxAtStartPar
As we have so much information to process, I opted to keep it dynamic, the following function helps in that, it calculates for a question from our choice database the mean occurence for each group in a feature of the info dataframe.
Let’s say we want to know the average amount of persons that know a specific language for each role/job title. We would have to match Q16 (known languages) with Q6 (job description). This is performed below, notice how it both performs a merge (join) and a groupby to get the result.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{mean\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{info}\PYG{p}{,} \PYG{n}{question}\PYG{p}{)}\PYG{p}{:}
  \PYG{k}{return} \PYG{n}{info\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{n}{questions}\PYG{p}{[}\PYG{n}{info}\PYG{p}{]}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{n}{answer\PYGZus{}dfs}\PYG{p}{[}\PYG{n}{question}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{questions}\PYG{p}{[}\PYG{n}{info}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{mean\PYGZus{}matrix}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q6}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q16}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                                      Python  ...     Other
Select the title most similar to your current r...            ...          
Business Analyst                                    0.605085  ...  0.027119
Chief Officer                                       0.717131  ...  0.055777
Consultant                                          0.692845  ...  0.041885
DBA/Database Engineer                               0.666667  ...  0.017094
Data Analyst                                        0.647059  ...  0.018908
Data Engineer                                       0.801418  ...  0.028369
Data Journalist                                     0.600000  ...  0.000000
Data Scientist                                      0.860265  ...  0.023084
Developer Advocate                                  0.611765  ...  0.058824
Manager                                             0.681416  ...  0.028761
Marketing Analyst                                   0.505747  ...  0.000000
Not employed                                             NaN  ...       NaN
Other                                               0.682041  ...  0.037291
Principal Investigator                              0.759036  ...  0.072289
Product/Project Manager                             0.720365  ...  0.027356
Research Assistant                                  0.776286  ...  0.031320
Research Scientist                                  0.780541  ...  0.043243
Salesperson                                         0.612500  ...  0.037500
Software Engineer                                   0.737179  ...  0.051282
Statistician                                        0.500000  ...  0.022222
Student                                             0.697710  ...  0.015267

[21 rows x 18 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
We can see that for each combination of job title and programming language an average between 0 and 1 persons have checked this option, e.g. the combination of data scientist and python equals 0.86, meaning that 86\% of data scientists know python.

\sphinxAtStartPar
Similarly we can also calculate correlation between choices from our choice database, here we did it again for Question 16.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{answer\PYGZus{}dfs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q16}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{corr}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                         Python         R  ...      None     Other
Python                 1.000000  0.077293  ... \PYGZhy{}0.206520  0.009641
R                      0.077293  1.000000  ... \PYGZhy{}0.085536 \PYGZhy{}0.001240
SQL                    0.191304  0.223527  ... \PYGZhy{}0.101752  0.001674
Bash                   0.188435  0.032511  ... \PYGZhy{}0.049637  0.056457
Java                   0.141813 \PYGZhy{}0.034205  ... \PYGZhy{}0.056888  0.026739
Javascript/Typescript  0.125164 \PYGZhy{}0.039002  ... \PYGZhy{}0.052961  0.034497
Visual Basic/VBA       0.007550  0.098949  ... \PYGZhy{}0.032002 \PYGZhy{}0.004965
C/C++                  0.183621 \PYGZhy{}0.049046  ... \PYGZhy{}0.058636  0.019235
MATLAB                 0.131117  0.030446  ... \PYGZhy{}0.044376 \PYGZhy{}0.000043
Scala                  0.095374  0.033991  ... \PYGZhy{}0.025780  0.014499
Julia                  0.039096  0.063129  ... \PYGZhy{}0.013031  0.006897
Go                     0.053180 \PYGZhy{}0.016114  ... \PYGZhy{}0.017048  0.012141
C\PYGZsh{}/.NET                0.046182 \PYGZhy{}0.039415  ... \PYGZhy{}0.035878  0.006953
PHP                    0.048012 \PYGZhy{}0.006318  ... \PYGZhy{}0.029713  0.031095
Ruby                   0.031308  0.014921  ... \PYGZhy{}0.015859  0.034500
SAS/STATA             \PYGZhy{}0.009036  0.198183  ... \PYGZhy{}0.029014 \PYGZhy{}0.005546
None                  \PYGZhy{}0.206520 \PYGZhy{}0.085536  ...  1.000000 \PYGZhy{}0.021852
Other                  0.009641 \PYGZhy{}0.001240  ... \PYGZhy{}0.021852  1.000000

[18 rows x 18 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we see thich answers are checked usually together or not, as an example we see that python and SQL have a correlation of 19\% whilst Python and R have a correlation of 7.7\% which is logical as Python and R have a similar purpose and SQL is complementary. Obviously None is always negatively correlated, a good example of obsolete information!


\subsection{Count matrix}
\label{\detokenize{c7_case_studies/UserSurvey:count-matrix}}
\sphinxAtStartPar
to correlate information between 2 questions of the info dataframe, we create a function that counts the occurence of each combination. An example is given for question 2 (age) and Question 7 (industry). With this information we can find out if there is a correlation between information of our users in the survey, not specifically their choices on the multiple choice answers.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{count\PYGZus{}matrix}\PYG{p}{(}\PYG{n}{q1}\PYG{p}{,} \PYG{n}{q2}\PYG{p}{)}\PYG{p}{:}
  \PYG{k}{return} \PYG{n}{info\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{n}{questions}\PYG{p}{[}\PYG{n}{q1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{questions}\PYG{p}{[}\PYG{n}{q2}\PYG{p}{]}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{n}{questions}\PYG{p}{[}\PYG{n}{q1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{questions}\PYG{p}{[}\PYG{n}{q2}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{count\PYGZus{}matrix}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q7}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
In what industry is your current employer/contract (or your most recent employer if retired)? \PYGZhy{} Selected Choice  Academics/Education  ...  Shipping/Transportation
What is your age (\PYGZsh{} years)?                                                                                                           ...                         
18\PYGZhy{}21                                                                                                                            117  ...                        7
22\PYGZhy{}24                                                                                                                            322  ...                       26
25\PYGZhy{}29                                                                                                                            539  ...                       67
30\PYGZhy{}34                                                                                                                            364  ...                       59
35\PYGZhy{}39                                                                                                                            243  ...                       36
40\PYGZhy{}44                                                                                                                            146  ...                       20
45\PYGZhy{}49                                                                                                                             91  ...                        6
50\PYGZhy{}54                                                                                                                             71  ...                        9
55\PYGZhy{}59                                                                                                                             35  ...                        3
60\PYGZhy{}69                                                                                                                             40  ...                        4
70\PYGZhy{}79                                                                                                                             11  ...                        0
80+                                                                                                                                2  ...                        0

[12 rows x 19 columns]
\end{sphinxVerbatim}


\chapter{Case Study: Jokes}
\label{\detokenize{c7_case_studies/Jokes:case-study-jokes}}\label{\detokenize{c7_case_studies/Jokes::doc}}
\sphinxAtStartPar
In this case study we find out if we can make ourselves funnier by analysing jokes from a database.

\sphinxAtStartPar
The case study is divided into several parts:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Goals

\item {} 
\sphinxAtStartPar
Parsing

\item {} 
\sphinxAtStartPar
Preparation (cleaning)

\item {} 
\sphinxAtStartPar
Processing

\item {} 
\sphinxAtStartPar
Exploration

\item {} 
\sphinxAtStartPar
Visualization

\item {} 
\sphinxAtStartPar
Conclusion

\end{itemize}


\section{Goals}
\label{\detokenize{c7_case_studies/Jokes:goals}}
\sphinxAtStartPar
In this section we define questions that will be our guideline througout the case study
\begin{itemize}
\item {} 
\sphinxAtStartPar
What jokes are funny?

\item {} 
\sphinxAtStartPar
Can we find types of jokes?

\item {} 
\sphinxAtStartPar
Would a joke recommender work?

\end{itemize}

\sphinxAtStartPar
We’ll (try to) keep these question in mind when performing the case study.


\section{Parsing}
\label{\detokenize{c7_case_studies/Jokes:parsing}}
\sphinxAtStartPar
we start out by importing all necessary libraries

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{json}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{set\PYGZus{}matplotlib\PYGZus{}formats}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\PYG{n}{set\PYGZus{}matplotlib\PYGZus{}formats}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{svg}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/tmp/ipykernel\PYGZus{}13889/4057771804.py:10: DeprecationWarning: `set\PYGZus{}matplotlib\PYGZus{}formats` is deprecated since IPython 7.23, directly use `matplotlib\PYGZus{}inline.backend\PYGZus{}inline.set\PYGZus{}matplotlib\PYGZus{}formats()`
  set\PYGZus{}matplotlib\PYGZus{}formats(\PYGZsq{}svg\PYGZsq{})
\end{sphinxVerbatim}

\sphinxAtStartPar
in order to download datasets from kaggle, we need an API key to access their API, we’ll make that here

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{mkdir}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/root/.kaggle/kaggle.json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}
        \PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{username}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lorenzf}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{7a44a9e99b27e796177d793a3d85b8cf}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{\PYGZcb{}}
        \PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{PermissionError}\PYG{g+gWhitespace}{                           }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}13889}\PYG{o}{/}\PYG{l+m+mf}{3113012040.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{1} \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{2}     \PYG{n}{os}\PYG{o}{.}\PYG{n}{mkdir}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/root/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{3} 
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{4} \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{/root/.kaggle/kaggle.json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{5}     \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}

\PYG{n+ne}{PermissionError}: [Errno 13] Permission denied: \PYGZsq{}/root/.kaggle\PYGZsq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
now we can import kaggle too and download the datasets

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kaggle}
\PYG{n}{kaggle}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{dataset\PYGZus{}download\PYGZus{}files}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pavellexyr/one\PYGZhy{}million\PYGZhy{}reddit\PYGZhy{}jokes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{unzip}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run \PYGZsq{}chmod 600 /root/.kaggle/kaggle.json\PYGZsq{}
\end{sphinxVerbatim}

\sphinxAtStartPar
the csv files are now in the ‘./data’ folder, we can now read them using pandas, here is the list of all csv files in our folder

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{os}\PYG{o}{.}\PYG{n}{listdir}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}one\PYGZhy{}million\PYGZhy{}reddit\PYGZhy{}jokes.csv\PYGZsq{}]
\end{sphinxVerbatim}

\sphinxAtStartPar
With only one file in the dataset, we import it.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/one\PYGZhy{}million\PYGZhy{}reddit\PYGZhy{}jokes.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
shape: (1000000, 12)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   type      id  ...                                              title score
0  post  ftbp1i  ...               I am soooo glad I\PYGZsq{}m not circumcised!     2
1  post  ftboup  ...  Did you know Google now has a platform for rec...     9
2  post  ftbopj  ...  What is the difference between my wife and my ...    15
3  post  ftbnxh  ...                              My last joke for now.     9
4  post  ftbjpg  ...              The Nintendo 64 turns 18 this week...   134

[5 rows x 12 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Already we can see a lot of unnecessary information, so cleanup is important. It seems the joke is divided in a title and selftext where often the punchline is present.


\section{Preparation}
\label{\detokenize{c7_case_studies/Jokes:preparation}}
\sphinxAtStartPar
here we perform tasks to prepare the data in a more pleasing format.


\subsection{Cleanup}
\label{\detokenize{c7_case_studies/Jokes:cleanup}}
\sphinxAtStartPar
First thing I would like to do see which columns are useless, by printing the amount of unique values

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{col} \PYG{o+ow}{in} \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{:}
  \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{col}\PYG{p}{)}
  \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{p}{[}\PYG{n}{col}\PYG{p}{]}\PYG{o}{.}\PYG{n}{nunique}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
  \PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
type
1

id
1000000

subreddit.id
1

subreddit.name
1

subreddit.nsfw
1

created\PYGZus{}utc
996373

permalink
1000000

domain
364

url
4410

selftext
520567

title
861254

score
8913
\end{sphinxVerbatim}

\sphinxAtStartPar
a few columns only have 1 value, also the links are not important for our case, so we drop them too.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df} \PYG{o}{=} \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{type}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{subreddit.id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{subreddit.name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{subreddit.nsfw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{permalink}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{url}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   created\PYGZus{}utc  ... score
0   1585785543  ...     2
1   1585785522  ...     9
2   1585785508  ...    15
3   1585785428  ...     9
4   1585785009  ...   134

[5 rows x 5 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
much cleaner already!


\subsection{Data Types}
\label{\detokenize{c7_case_studies/Jokes:data-types}}
\sphinxAtStartPar
Before we do anything with our data, it is good to see if our data types are in order

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 1000000 entries, 0 to 999999
Data columns (total 5 columns):
 \PYGZsh{}   Column       Non\PYGZhy{}Null Count    Dtype 
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}       \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}    \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{} 
 0   created\PYGZus{}utc  1000000 non\PYGZhy{}null  int64 
 1   domain       1000000 non\PYGZhy{}null  object
 2   selftext     995525 non\PYGZhy{}null   object
 3   title        1000000 non\PYGZhy{}null  object
 4   score        1000000 non\PYGZhy{}null  int64 
dtypes: int64(2), object(3)
memory usage: 38.1+ MB
\end{sphinxVerbatim}

\sphinxAtStartPar
the created\_utc feature is encoded in an unix timestamp, it would be more usefull to transform it to a timestamp

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{created}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{created\PYGZus{}utc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{unit}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{s}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{del} \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{created\PYGZus{}utc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       domain  ...             created
0  self.jokes  ... 2020\PYGZhy{}04\PYGZhy{}01 23:59:03
1  self.jokes  ... 2020\PYGZhy{}04\PYGZhy{}01 23:58:42
2  self.jokes  ... 2020\PYGZhy{}04\PYGZhy{}01 23:58:28
3  self.jokes  ... 2020\PYGZhy{}04\PYGZhy{}01 23:57:08
4  self.jokes  ... 2020\PYGZhy{}04\PYGZhy{}01 23:50:09

[5 rows x 5 columns]
\end{sphinxVerbatim}


\subsection{Missing values}
\label{\detokenize{c7_case_studies/Jokes:missing-values}}
\sphinxAtStartPar
for each dataframe we apply a few checks in order to see the quality of data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{o}{*}\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{/}\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
domain      0.0000
selftext    0.4475
title       0.0000
score       0.0000
created     0.0000
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
it looks like some jokes are missing the selftext field, we show a few here.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{p}{[}\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{selftext}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{by}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            domain selftext  ...  score             created
625315   imgur.com      NaN  ...  67950 2017\PYGZhy{}05\PYGZhy{}20 15:41:28
971313  self.jokes      NaN  ...  36918 2015\PYGZhy{}07\PYGZhy{}03 15:41:05
942471  self.jokes      NaN  ...  17486 2015\PYGZhy{}10\PYGZhy{}05 16:09:09
926550  self.jokes      NaN  ...  17456 2015\PYGZhy{}11\PYGZhy{}18 04:29:54
919422  self.jokes      NaN  ...  12580 2015\PYGZhy{}12\PYGZhy{}07 18:55:27
...            ...      ...  ...    ...                 ...
929807  self.jokes      NaN  ...      0 2015\PYGZhy{}11\PYGZhy{}09 03:33:22
959394  self.jokes      NaN  ...      0 2015\PYGZhy{}08\PYGZhy{}14 13:40:21
929809  self.jokes      NaN  ...      0 2015\PYGZhy{}11\PYGZhy{}09 03:26:55
959338  self.jokes      NaN  ...      0 2015\PYGZhy{}08\PYGZhy{}14 17:03:55
999984  self.jokes      NaN  ...      0 2015\PYGZhy{}03\PYGZhy{}26 19:57:54

[4475 rows x 5 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
as far as I can see here the jokes are so short they are only one sentence, so we can fill in the missing values with an empty text.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{selftext} \PYG{o}{=} \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{selftext}\PYG{o}{.}\PYG{n}{fillna}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
This does not mean we are done, earlier I noticed the words {[}removed{]} and {[}deleted{]} in the selftext feature, indicating the joke was removed or deleted, these are missing values!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{p}{[}\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{selftext}\PYG{o}{.}\PYG{n}{isin}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[removed]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[deleted]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       domain   selftext  ... score             created
3  self.jokes  [removed]  ...     9 2020\PYGZhy{}04\PYGZhy{}01 23:57:08
4  self.jokes  [removed]  ...   134 2020\PYGZhy{}04\PYGZhy{}01 23:50:09
5  self.jokes  [removed]  ...     1 2020\PYGZhy{}04\PYGZhy{}01 23:49:55
6  self.jokes  [removed]  ...     8 2020\PYGZhy{}04\PYGZhy{}01 23:44:11
8  self.jokes  [removed]  ...    88 2020\PYGZhy{}04\PYGZhy{}01 23:39:27

[5 rows x 5 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
I am going to remove these jokes as they are not complete anymore, it might have been that these jokes have been removed as they have already been posted.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df} \PYG{o}{=} \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{p}{[}\PYG{o}{\PYGZti{}}\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{selftext}\PYG{o}{.}\PYG{n}{isin}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[removed]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[deleted]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(578637, 5)
\end{sphinxVerbatim}

\sphinxAtStartPar
seems we have kept about 578k jokes, not bad!


\subsection{Duplicates}
\label{\detokenize{c7_case_studies/Jokes:duplicates}}
\sphinxAtStartPar
As formatting of text might be different i’m not expecting a lot of duplicates, let’s see what we can find.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{p}{[}\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{title}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{selftext}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            domain  ...             created
211     self.jokes  ... 2020\PYGZhy{}04\PYGZhy{}01 18:54:06
4452    self.jokes  ... 2020\PYGZhy{}03\PYGZhy{}27 09:16:20
6349    self.jokes  ... 2020\PYGZhy{}03\PYGZhy{}25 00:48:09
6881    self.jokes  ... 2020\PYGZhy{}03\PYGZhy{}24 09:40:14
8299    self.jokes  ... 2020\PYGZhy{}03\PYGZhy{}22 07:49:45
...            ...  ...                 ...
999779  self.jokes  ... 2015\PYGZhy{}03\PYGZhy{}27 10:33:12
999851  self.jokes  ... 2015\PYGZhy{}03\PYGZhy{}27 02:42:29
999882  self.jokes  ... 2015\PYGZhy{}03\PYGZhy{}27 00:48:36
999936  self.jokes  ... 2015\PYGZhy{}03\PYGZhy{}26 22:00:06
999979  self.jokes  ... 2015\PYGZhy{}03\PYGZhy{}26 20:16:34

[12867 rows x 5 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
A fair amount of jokes are reposted, so we keep the ones with the highest score.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
 \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df} \PYG{o}{=} \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{title}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{selftext}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{keep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{last}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{Text formatting}
\label{\detokenize{c7_case_studies/Jokes:text-formatting}}
\sphinxAtStartPar
Before we can analyze the text in the jokes we have to format it. We can do this by removing all special character and changing it all to lowercase

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{col} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{selftext}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{title}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{:}
  \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{p}{[}\PYG{n}{col}\PYG{p}{]} \PYG{o}{=} \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{p}{[}\PYG{n}{col}\PYG{p}{]}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{n}{to\PYGZus{}replace}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{[\PYGZca{}a\PYGZhy{}zA\PYGZhy{}Z,.!? ]}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{value}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{regex}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{lower}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    index      domain  ... score             created
0  630580  self.jokes  ...     0 2017\PYGZhy{}05\PYGZhy{}12 17:01:44
1  187066  self.jokes  ...     0 2019\PYGZhy{}05\PYGZhy{}28 00:30:46
2  437464  self.jokes  ...     0 2018\PYGZhy{}03\PYGZhy{}28 10:17:26
3  714598  self.jokes  ...     0 2017\PYGZhy{}01\PYGZhy{}13 02:37:59
4  187072  self.jokes  ...     0 2019\PYGZhy{}05\PYGZhy{}28 00:20:01

[5 rows x 6 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Next we create a single joke by combining the title and selftext, this makes it easier to operate.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{joke}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{title} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{selftext}
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df} \PYG{o}{=} \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{title}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{selftext}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    index  ...                                               joke
0  630580  ...  there are two kinds of people in the world. th...
1  187066  ...  set your wifi password to  so when someone ask...
2  437464  ...  at what time do you see your dentist? tooth hu...
3  714598  ...  john and juan are on lunch break when juans ph...
4  187072  ...  a guy is handing out free fake mustaches on th...

[5 rows x 5 columns]
\end{sphinxVerbatim}


\section{Processing}
\label{\detokenize{c7_case_studies/Jokes:processing}}

\subsection{Timing of joke}
\label{\detokenize{c7_case_studies/Jokes:timing-of-joke}}
\sphinxAtStartPar
I would like to know if the timing of the jokes makes an impact on how funny the joke is, so i grouped based on both the weekday as well as the hour of day.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}weekday} \PYG{o}{=} \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{created}\PYG{o}{.}\PYG{n}{dt}\PYG{o}{.}\PYG{n}{weekday}\PYG{p}{)}\PYG{o}{.}\PYG{n}{score}\PYG{o}{.}\PYG{n}{agg}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}weekday}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
               mean  count
created                   
0        226.871773  79866
1        228.808886  82940
2        222.802165  84793
3        215.771594  84932
4        222.888666  82634
5        232.752534  75089
6        241.322581  75516
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}hour} \PYG{o}{=} \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{created}\PYG{o}{.}\PYG{n}{dt}\PYG{o}{.}\PYG{n}{hour}\PYG{p}{)}\PYG{o}{.}\PYG{n}{score}\PYG{o}{.}\PYG{n}{agg}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}hour}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
               mean  count
created                   
0        189.177767  25646
1        189.383726  25440
2        172.406772  25368
3        140.741126  23637
4        144.066960  21162
5        137.355467  19006
6        168.542319  16671
7        214.903014  15198
8        271.710558  14217
9        398.431366  14009
10       456.262600  15952
11       446.946555  18056
12       404.759640  21447
13       318.348451  24342
14       263.100078  26899
15       227.382529  28322
16       204.701879  29327
17       185.274719  29623
18       210.557595  29369
19       194.320871  29342
20       194.809965  29063
21       179.245679  29099
22       198.723083  27817
23       179.602399  26758
\end{sphinxVerbatim}


\subsection{Bag of words}
\label{\detokenize{c7_case_studies/Jokes:bag-of-words}}
\sphinxAtStartPar
To be able to work with the words in our joke, we create a bag of words dataframe, where for each word and joke combination a count is kept of how many times the word is present in that joke. Notice that stopwords are removed.

\sphinxAtStartPar
First we split each joke up in words

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{joke\PYGZus{}words} \PYG{o}{=} \PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{joke}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{joke\PYGZus{}words}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0    [there, are, two, kinds, of, people, in, the, ...
1    [set, your, wifi, password, to, , so, when, so...
2    [at, what, time, do, you, see, your, dentist?,...
3    [john, and, juan, are, on, lunch, break, when,...
4    [a, guy, is, handing, out, free, fake, mustach...
Name: joke, dtype: object
\end{sphinxVerbatim}

\sphinxAtStartPar
Next we use the nltk toolkit to get a list of english stopwords.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{nltk}
\PYG{n}{nltk}\PYG{o}{.}\PYG{n}{download}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{stopwords}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k+kn}{from} \PYG{n+nn}{nltk}\PYG{n+nn}{.}\PYG{n+nn}{corpus} \PYG{k+kn}{import} \PYG{n}{stopwords}
\PYG{n}{stopwords}\PYG{o}{.}\PYG{n}{words}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{english}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[nltk\PYGZus{}data] Downloading package stopwords to /root/nltk\PYGZus{}data...
[nltk\PYGZus{}data]   Package stopwords is already up\PYGZhy{}to\PYGZhy{}date!
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}i\PYGZsq{}, \PYGZsq{}me\PYGZsq{}, \PYGZsq{}my\PYGZsq{}, \PYGZsq{}myself\PYGZsq{}, \PYGZsq{}we\PYGZsq{}]
\end{sphinxVerbatim}

\sphinxAtStartPar
We remove all the stopwords from the jokes, now the jokes have a handicapped grammar.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{joke\PYGZus{}words} \PYG{o}{=} \PYG{n}{joke\PYGZus{}words}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{apply}\PYG{p}{(}\PYG{k}{lambda} \PYG{n}{x} \PYG{p}{:} \PYG{p}{[}\PYG{n}{word} \PYG{k}{for} \PYG{n}{word} \PYG{o+ow}{in} \PYG{n}{x} \PYG{k}{if} \PYG{n}{word} \PYG{o+ow}{not} \PYG{o+ow}{in} \PYG{n}{stopwords}\PYG{o}{.}\PYG{n}{words}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{english}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{joke\PYGZus{}words}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0         [two, kinds, people, world., need, closure,]
1       [set, wifi, password, , someone, asks, say, .]
2                 [time, see, dentist?, tooth, hurty!]
3    [john, juan, lunch, break, juans, phone, rings...
4    [guy, handing, free, fake, mustaches, street, ...
Name: joke, dtype: object
\end{sphinxVerbatim}

\sphinxAtStartPar
Finally we are going to use sklearn and the CountVectorizer to create the BoW vector, this is a sparse matrix as most words are not appearing in most jokes.
This means we cannot visualise the matrix, or our computer would explode.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}extraction}\PYG{n+nn}{.}\PYG{n+nn}{text} \PYG{k+kn}{import} \PYG{n}{CountVectorizer}

\PYG{n}{cnt\PYGZus{}vect} \PYG{o}{=} \PYG{n}{CountVectorizer}\PYG{p}{(}\PYG{n}{analyzer}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{word}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{stop\PYGZus{}words}\PYG{o}{=}\PYG{n}{stopwords}\PYG{o}{.}\PYG{n}{words}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{english}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{max\PYGZus{}features}\PYG{o}{=}\PYG{l+m+mi}{20000}\PYG{p}{)} 

\PYG{n}{bow\PYGZus{}jokes} \PYG{o}{=} \PYG{n}{cnt\PYGZus{}vect}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{joke}\PYG{o}{.}\PYG{n}{values}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{bow\PYGZus{}jokes}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}565770x20000 sparse matrix of type \PYGZsq{}\PYGZlt{}class \PYGZsq{}numpy.int64\PYGZsq{}\PYGZgt{}\PYGZsq{}
	with 9101120 stored elements in Compressed Sparse Row format\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
But we can fetch the vocabulary of our bag, which starts with a lot of weird words, indicating we might have chosen too many features

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cnt\PYGZus{}vect}\PYG{o}{.}\PYG{n}{get\PYGZus{}feature\PYGZus{}names\PYGZus{}out}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{10}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([\PYGZsq{}aa\PYGZsq{}, \PYGZsq{}aaa\PYGZsq{}, \PYGZsq{}aaah\PYGZsq{}, \PYGZsq{}aah\PYGZsq{}, \PYGZsq{}aardvark\PYGZsq{}, \PYGZsq{}aaron\PYGZsq{}, \PYGZsq{}ab\PYGZsq{}, \PYGZsq{}aback\PYGZsq{},
       \PYGZsq{}abacus\PYGZsq{}, \PYGZsq{}abandon\PYGZsq{}], dtype=object)
\end{sphinxVerbatim}


\subsection{Term Frequency \sphinxhyphen{} Inverse Document Frequency}
\label{\detokenize{c7_case_studies/Jokes:term-frequency-inverse-document-frequency}}
\sphinxAtStartPar
Another interesting method is the tf\sphinxhyphen{}idf matrix, where each occurence is weighted by the overall frequency of that word. If a word is used often over all jokes, it won’t be as important, but if a word is used infrequent it is more important.

\sphinxAtStartPar
Again we use sklearn to vectorize our jokes

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{feature\PYGZus{}extraction}\PYG{n+nn}{.}\PYG{n+nn}{text} \PYG{k+kn}{import} \PYG{n}{TfidfVectorizer}
\PYG{n}{tfidf\PYGZus{}vect} \PYG{o}{=} \PYG{n}{TfidfVectorizer}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{tfidf\PYGZus{}jokes} \PYG{o}{=} \PYG{n}{tfidf\PYGZus{}vect}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{reddit\PYGZus{}jokes\PYGZus{}df}\PYG{o}{.}\PYG{n}{joke}\PYG{o}{.}\PYG{n}{values}\PYG{p}{)}
\PYG{n}{tfidf\PYGZus{}jokes}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}565770x196601 sparse matrix of type \PYGZsq{}\PYGZlt{}class \PYGZsq{}numpy.float64\PYGZsq{}\PYGZgt{}\PYGZsq{}
	with 15153814 stored elements in Compressed Sparse Row format\PYGZgt{}
\end{sphinxVerbatim}

\sphinxAtStartPar
we can create a quick dataframe to interpret the result, for each word in our dataset we retrieve the inverse document frequency, a high idf means a unique word.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{idf} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}
    \PYG{p}{\PYGZob{}}
      \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{term}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tfidf\PYGZus{}vect}\PYG{o}{.}\PYG{n}{get\PYGZus{}feature\PYGZus{}names\PYGZus{}out}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
      \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{idf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{tfidf\PYGZus{}vect}\PYG{o}{.}\PYG{n}{idf\PYGZus{}}\PYG{p}{,}
    \PYG{p}{\PYGZcb{}}
\PYG{p}{)}
\PYG{n}{idf}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     term        idf
0      aa  10.026437
1     aaa  10.275653
2    aaaa  12.454185
3   aaaaa  13.147332
4  aaaaaa  13.552798
\end{sphinxVerbatim}

\sphinxAtStartPar
When we sort them by idf we can find the most unique words, yet it doesn’t seem to be useful at the moment.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{idf}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{n}{by}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{idf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ascending}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                                                     term        idf
196600  zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzthe  13.552798
110080                                misterunderstanding  13.552798
110074                                       misterectomy  13.552798
110075                                         misterious  13.552798
110076                                       misterjmyers  13.552798
110077                                          misterlee  13.552798
110078                                        misterogyny  13.552798
110079                                            misters  13.552798
110081                                   misterunderstood  13.552798
110072                                  misterapproximate  13.552798
\end{sphinxVerbatim}







\renewcommand{\indexname}{Index}
\printindex
\end{document}