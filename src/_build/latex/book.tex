%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=0,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{1. Introduction}}

\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Data Science - A practical Approach}
\date{Oct 08, 2021}
\release{}
\author{Lorenz Feyen}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{foreword::doc}}


\sphinxAtStartPar
pdf version can be found \sphinxhref{https://github.com/LorenzF/data-science-practical-approach/raw/main/src/\_build/latex/book.pdf}{here}.


\part{1. Introduction}


\chapter{Introduction}
\label{\detokenize{c1_introduction/introduction:introduction}}\label{\detokenize{c1_introduction/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\section{Structured vs Unstructured}
\label{\detokenize{c1_introduction/introduction:structured-vs-unstructured}}
\sphinxAtStartPar
When performing data preparation an important aspect is to consider with the type of data we are working with.
In general there are 2 types of data, but you could consider a third.


\subsection{Structured data}
\label{\detokenize{c1_introduction/introduction:structured-data}}
\sphinxAtStartPar
Structured data is data that adheres to a pre\sphinxhyphen{}defined data model and is therefore straightforward to analyze.
This data model is the description of our data, each record has to be conform to the model.
A table in a spreadsheet is a good example of the concept of structured data however often no data types are enforced, meaning a column can contain e.g. both numbers and text.
Later we will see that a mixture of data types is often problematic therefor the need of a data model.


\subsection{Unstructured data}
\label{\detokenize{c1_introduction/introduction:unstructured-data}}
\sphinxAtStartPar
In contrast to structured data, there is no apparent data model but this does not mean the data is unusable or cluttered.
Usually it means either no data model has yet been applied or we are dealing with data that is difficult to confine in a model.
A great example of this would be images, or more general (binary) files.
These obviously are hard to sort yet often data structures also contain metadata from these files, with data describing things as when the file was uploaded, what is shown in the file, …
In turn the metadata can be structured and a data model can be related to the unstructured data.


\subsection{Semi\sphinxhyphen{}structured data}
\label{\detokenize{c1_introduction/introduction:semi-structured-data}}
\sphinxAtStartPar
As an intermediate option, we have what is called semi\sphinxhyphen{}structured data.
The reasoning behind this is that the concept of tables is not always applicable, in some occasions e.g. data lakes there is no complex structure present compared to a database.
In a data lake files are stored similar to the folder structure in your computer, with no fancy infrastructure behind it, thus reducing operation costs.
This implies that a data model can not be enforced and the data is stored in generic files.


\section{Data Structures}
\label{\detokenize{c1_introduction/introduction:data-structures}}
\sphinxAtStartPar
There are several structures in which data can be stored and accessed, here we cover the 3 most important.


\subsection{Data Lake}
\label{\detokenize{c1_introduction/introduction:data-lake}}
\sphinxAtStartPar
As mentioned earlier a data lake would be the most cost efficient method as it relies on the least infrastructure and can be serverless.
The concept behind a data lake is straight\sphinxhyphen{}forward, the data is stored in simple files with a specific notation e.g. parquet, csv, xml,…
What is important when designing a data lake would be partitioning, this can be achieved by using subfolders and saving parts of the data in different files.
To make this more tangible, take a look at this symbolic \sphinxhref{https://github.com/LorenzF/data-science-practical-approach/tree/main/src/c2\_data\_preparation/data/temperatures}{example} I provided.
Instead of putting all data in one csv file, subfolder divide the data in Country, City and then the year.
We could even further partition yet the data is here in daily frequency so that would create many small partitions.
The difficulty for a data lake lies in the method of interacting, when adding new data one has to adhere to a agreed upon data model that is not enforced, meaning you could create incorrect data which then need to be cleaned.
On the other hand efficiency of you data lake depends on good partitioning, as the order of divisioning of your folders. We could have also divided first on year and then on country and city.
As a data scientist seeing the data lake might not be as common, as this is rather an engineering task, however using the concepts of a data lake in experimental projects can make a big difference.


\subsection{Database}
\label{\detokenize{c1_introduction/introduction:database}}
\sphinxAtStartPar
Another interesting data structure is the database, widely used for exceptional speeds and ease of use, yet costly in storage.
Numerous implementations of servers using the SQL language are developed over the years with each their own dialect and advantages.
The important take home message here is that you can easily perform queries on the database that pre\sphinxhyphen{}handles the data to retrieve the information you need.
these operations include filtering, grouping categories, joining tables, ordering and much more, as SQL is a complete language on its own.
As a data scientist these databases are much more common, so SQL is a good asset to learn!


\subsection{Data Warehouse}
\label{\detokenize{c1_introduction/introduction:data-warehouse}}
\sphinxAtStartPar
A next step towards data analysis is the data warehouse, where a database is composed of the most pragmatic method of storing your data a data warehouse consist of multiple views on your data.
Based upon the data of a dataset the data warehouse transforms this data into a new format that displays the data in a new way.
Let me illustrate with with a simple example, we have a database with a table that contains the rentals of books from multiple libraries.
This table has a few columns: a timestamp, the library, the action (rent, return, …), the client\_id and the book\_id.
If you would want to know if a book is available this database is perfect for your needs as you just have to find the last event for that book and if its a return the book is (or should be) there.
Now image we would want to know how many books are being rented per month this database is insufficient, yet our data warehouse might contain such a view!
It is up to the data engineer/scientist to create a computation that displays the amount of books rented per month.
If they also would like to subdivided it per category of books, you would need to incorporate another table of the database where information of the books is stored.
More on these operations of a data warehouse will be seen in the data preprocessing chapter.
One last remark about data warehousing, it is important to optimize between memory and computation.
Tables in our data warehouse compared to database can be computed in place reducing memory costs yet increasing computation costs.
If a visualization tool often queries a table in your warehouse it is favorable to create it as a table in your database.


\section{OLTP and OLAP}
\label{\detokenize{c1_introduction/introduction:oltp-and-olap}}
\sphinxAtStartPar
From the previous section you might have deduced that a database and Data Warehouse serve 2 different purposes.
These are denoted as OnLine Transaction Processing and OnLine Analytical Processing, as the names suggest these are used for transactional and analytical processes.


\subsection{OLTP}
\label{\detokenize{c1_introduction/introduction:oltp}}
\sphinxAtStartPar
For this method the database structure is optimal, let us review the example where we have libraries renting out books.
Renting out a book would send a message to our OLTP system creating a new record stating that specific book is at this moment rented out from our library.
OLTP handles day\sphinxhyphen{}to\sphinxhyphen{}day operational data that can be both written and read from our database.


\subsection{OLAP}
\label{\detokenize{c1_introduction/introduction:olap}}
\sphinxAtStartPar
In the case we would like to analyse data from the libraries we would use the OLAP method, creating multi\sphinxhyphen{}dimensional views from our transactional data.
Our dimensions would be the date (aggregated per month), the library and the category of book, the chapter of data preprocessing will use these operations practically.
I could write a whole chapter on OLAP operations however they are well described in \sphinxhref{https://en.wikipedia.org/wiki/OLAP\_cube}{this} wikipedia page.


\part{2. Data Preparation}


\chapter{Introduction}
\label{\detokenize{c2_data_preparation/introduction:introduction}}\label{\detokenize{c2_data_preparation/introduction::doc}}
\sphinxAtStartPar
When performing data science, we often do not elaborate about the preparation that went into the dataset.
It is considered tedious and irrelevant to the story of the analysis,
however it is often the most important part of data analysis.
Data Preparation is the metaphorical foundation of your construction, if you fail to prepare data, you prepare to fail your analysis.
\begin{quote}

\sphinxAtStartPar
Good data beats a fancy algorithm
\end{quote}

\sphinxAtStartPar
If you would perform an analysis and insert unprepared data, you will mostly be disappointed with the result.


\section{why Data Preparation?}
\label{\detokenize{c2_data_preparation/introduction:why-data-preparation}}
\sphinxAtStartPar
Aside from metaphors let us make the reasoning behind this step more tangile, to explain the relevance of this step, we partitioned the answer into a few key points.


\subsection{Accuracy}
\label{\detokenize{c2_data_preparation/introduction:accuracy}}
\sphinxAtStartPar
There is no excuse for incorrect data and accuracy is the most important attribute.
Let us assume that we have a dataset where for some reason the result are not accurate.
This would led us to an analysis where we conclude a result that contains a bias.
An example would be a dataset of sold cars, where the listed price is that of the stock car without options.
Options are not incorporated in the price and we are perhaps training an algorithm that predicts the stock price.
If you as a data scientist fail to report/correct this, your predictions are making sense, but always underestimate!


\subsection{Consistency}
\label{\detokenize{c2_data_preparation/introduction:consistency}}
\sphinxAtStartPar
They usually say something such as ‘consistency is key’ and with data preparation that is likewise true.
A dataset where we do not have consistent results will never converge towards a particular answer.
Note however that it might not be a problem of consistency but rather you are missing crucial information.
If we would have a dataset where local temperatures are logged, we would like to see a consistency each 24 hours.
However we do see there are day to day fluctuations, so perhaps we need to keep track of cloud and rain data to make the dataset more complete.
We could then see that the results are more consistent yet the possibility of outliers is still present.
Equally possible would be that our temperature sensor is not sensitive enough or has large fluctuations in readings, it is the task of the data scientist to figure this out.

\sphinxAtStartPar
To get a visual  about accuracy and consistency this picture might help:

\sphinxAtStartPar
\sphinxincludegraphics{{accuracy}.png}


\subsection{Completeness}
\label{\detokenize{c2_data_preparation/introduction:completeness}}
\sphinxAtStartPar
As hinted in the previous point, completeness is something you have to be aware of.
Having ‘complete’ data is crucial for you narrative to give a correct answer, as you might otherwise lose detail.
Note that you never will know if your data is complete as there might always be more data to mine.
Yet you have to make a consideration between collecting more data and the effort required.
This collecting can happen in multiple methods, as an example we use a survey where we asked several people 10 different questions, we could:
\begin{itemize}
\item {} 
\sphinxAtStartPar
gather new data, here our data grows ‘longer’ by asking the 10 question to more people.
It might be that our sample of people were only students at a campus, so our data was not complete.

\item {} 
\sphinxAtStartPar
gather new feature, by asking more questions to the same people (in case we could still find them).
By doing this we get a better understanding of their opinion, again making our data more complete.

\item {} 
\sphinxAtStartPar
fill missing values, by imputing the abstained questions with answers of similar records.
When someone answered they did not want to answer we could figure out what they would have answered by looking at what persons answered that reply in a similar way.

\end{itemize}


\subsection{Timeliness}
\label{\detokenize{c2_data_preparation/introduction:timeliness}}
\sphinxAtStartPar
For some datasets we are dealing with data that is time related.
It can happen that data at specific timepoints is missing or delayed, resulting in a failure to use machine learning algorithms.
A well\sphinxhyphen{}organised data pipeline utilises techniques of data preparation to circumvent these outages, usually this would be to retain the last successful datapoint.
However in hindsight we could use more complex strategies to fill in these gaps or correct datetimes in our dataset,

\sphinxAtStartPar
In this example the data stream is interrupted and data preparation is there to handle these outages before we can perform analysis.

\sphinxAtStartPar
\sphinxincludegraphics{{timeliness}.png}


\subsection{Believability}
\label{\detokenize{c2_data_preparation/introduction:believability}}
\sphinxAtStartPar
You could collect the most intricate dataset possible, but if the narrative that you are conducting contradicts itself, you will end up nowhere.
During the process of data analytics it is important to apply a critical mind to what your dataset is telling you.
Obviously this is not a reason to mask or mold the data so it agrees with your opinion.
Rather you should be wary when conflicts happen and act accordingly, unfortunately it is impossible to write a generic tactic for this.
As a data scientist your experience of the underlying subject should help create understanding of the topic, remember, gathering information from experts in the field is crucial here!


\subsection{Interpretability}
\label{\detokenize{c2_data_preparation/introduction:interpretability}}
\sphinxAtStartPar
Another problem that might arise when you are diving deep into the data might be that you have created something no human could ever interpret.
The Machine Learning algorithms outputs plausible and believable results, but it is impossible to understand the reasoning behind.
For some this is perfectly acceptible, for some this is undesirable.
It is your task as a data scientist to cater the wishes of the product operator and if they desire understanding as they would like to learn from the data driven process you need to unfold the process.
Usually this comes down to which data transformations are used as some do produce an output that only makes mathematical sense.


\subsection{In conclusion}
\label{\detokenize{c2_data_preparation/introduction:in-conclusion}}
\sphinxAtStartPar
There are multiple ways to deteriorate the quality of your data and raw formats of data often contain multiple.
Before we can do anything with it these problems need to be resolved, if you fail to do so, the final output fails too.




\section{Further reading}
\label{\detokenize{c2_data_preparation/introduction:further-reading}}
\sphinxAtStartPar
\sphinxhref{https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4}{Towards Data Science}


\chapter{Missing Data}
\label{\detokenize{c2_data_preparation/missing_data:missing-data}}\label{\detokenize{c2_data_preparation/missing_data::doc}}
\sphinxAtStartPar
In this notebook we will look at a few datasets where values from columns are missing.
It is crucial for data science and machine learning to have a dataset where no values are missing as algorithms are usually not able to handle data with information missing.

\sphinxAtStartPar
For python, we will be using the pandas library to handle our dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}


\section{Kamyr digester}
\label{\detokenize{c2_data_preparation/missing_data:kamyr-digester}}
\sphinxAtStartPar
The first dataset we will be looking at is taken from a physical device equiped with numerous sensors, each timepoint (1 hour) these sensors are read out and the data is collected. Let’s have a look at the general structure

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/kamyr\PYGZhy{}digester.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  Observation  Y\PYGZhy{}Kappa  ChipRate  BF\PYGZhy{}CMratio  BlowFlow  ChipLevel4   \PYGZbs{}
0    31\PYGZhy{}00:00    23.10    16.520     121.717  1177.607      169.805   
1    31\PYGZhy{}01:00    27.60    16.810      79.022  1328.360      341.327   
2    31\PYGZhy{}02:00    23.19    16.709      79.562  1329.407      239.161   
3    31\PYGZhy{}03:00    23.60    16.478      81.011  1334.877      213.527   
4    31\PYGZhy{}04:00    22.90    15.618      93.244  1334.168      243.131   

   T\PYGZhy{}upperExt\PYGZhy{}2   T\PYGZhy{}lowerExt\PYGZhy{}2    UCZAA  WhiteFlow\PYGZhy{}4   ...  SteamFlow\PYGZhy{}4   \PYGZbs{}
0        358.282         329.545  1.443       599.253  ...        67.122   
1        351.050         329.067  1.549       537.201  ...        60.012   
2        350.022         329.260  1.600       549.611  ...        61.304   
3        350.938         331.142  1.604       623.362  ...        68.496   
4        351.640         332.709    NaN       638.672  ...        70.022   

   Lower\PYGZhy{}HeatT\PYGZhy{}3  Upper\PYGZhy{}HeatT\PYGZhy{}3   ChipMass\PYGZhy{}4   WeakLiquorF   BlackFlow\PYGZhy{}2   \PYGZbs{}
0        329.432         303.099      175.964      1127.197      1319.039   
1        330.823         304.879      163.202       665.975      1297.317   
2        329.140         303.383      164.013       677.534      1327.072   
3        328.875         302.254      181.487       767.853      1324.461   
4        328.352         300.954      183.929       888.448      1343.424   

   WeakWashF   SteamHeatF\PYGZhy{}3   T\PYGZhy{}Top\PYGZhy{}Chips\PYGZhy{}4   SulphidityL\PYGZhy{}4   
0     257.325         54.612         252.077             NaN  
1     241.182         46.603         251.406           29.11  
2     237.272         51.795         251.335             NaN  
3     239.478         54.846         250.312           29.02  
4     215.372         54.186         249.916           29.01  

[5 rows x 23 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Interesting, there seem to be 22 sensor values and 1 timestamp for each record. As mechanical devices are prone to noise and dropouts of sensors we would be foolish to assume no missing values are present.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{divide}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{kamyr\PYGZus{}df}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{100}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Observation         0.00
Y\PYGZhy{}Kappa             0.00
ChipRate            1.33
BF\PYGZhy{}CMratio          4.65
BlowFlow            4.32
ChipLevel4          0.33
T\PYGZhy{}upperExt\PYGZhy{}2        0.33
T\PYGZhy{}lowerExt\PYGZhy{}2        0.33
UCZAA               7.97
WhiteFlow\PYGZhy{}4         0.33
AAWhiteSt\PYGZhy{}4        46.84
AA\PYGZhy{}Wood\PYGZhy{}4           0.33
ChipMoisture\PYGZhy{}4      0.33
SteamFlow\PYGZhy{}4         0.33
Lower\PYGZhy{}HeatT\PYGZhy{}3       0.33
Upper\PYGZhy{}HeatT\PYGZhy{}3       0.33
ChipMass\PYGZhy{}4          0.33
WeakLiquorF         0.33
BlackFlow\PYGZhy{}2         0.33
WeakWashF           0.33
SteamHeatF\PYGZhy{}3        0.33
T\PYGZhy{}Top\PYGZhy{}Chips\PYGZhy{}4       0.33
SulphidityL\PYGZhy{}4      46.84
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
As expected, the datapoint ‘AAWhiteSt\sphinxhyphen{}4’ even has 46\% of data missing!
It seems we only have 300 datapoints and presumably these missing values occur in different records our dataset will be decimated if we just drop all rows with missing values.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(301, 23)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(131, 23)
\end{sphinxVerbatim}

\sphinxAtStartPar
As we drop all rows with missing values, we are left with only 131 records.
Whilst this might be good enough for some purposes, there are more viable options.

\sphinxAtStartPar
Perhaps we can first remove the column with the most missing values and then drop all remaining

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AAWhiteSt\PYGZhy{}4 }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SulphidityL\PYGZhy{}4 }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(263, 21)
\end{sphinxVerbatim}

\sphinxAtStartPar
Significantly better, although we lost the information of 2 sensors we now have a complete dataset with 263 records. For purposes where those 2 sensors are irrelevant this is a viable option, keep in mind that this dataset is still 100\% truthful, as we have not imputed any values.

\sphinxAtStartPar
Another option, where we retain all our records would be using the timely nature of our dataset, each record is a measurement with an interval of 1 hour. I have no knowledge of this dataset but one might make the assumption that the interval of 1 hour is taken as the state of the machine does not alter much in 1 hour. Therefore we could do what is called a forward fill, where we fill in the missing values with the same value of the sensor for the previous measurement.

\sphinxAtStartPar
This would solve nearly all nan values as there might be a problem where the first value is missing. This is shown below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{fillna}\PYG{p}{(}\PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ffill}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SulphidityL\PYGZhy{}4 }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0        NaN
1      29.11
2      29.11
3      29.02
4      29.01
       ...  
296    30.43
297    30.29
298    30.47
299    30.47
300    30.46
Name: SulphidityL\PYGZhy{}4 , Length: 301, dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
Although our dataset is not fully the truth, we can see that little to no changes occur in the sensor and using a forward fill is arguably the most suitable option.


\section{Travel times}
\label{\detokenize{c2_data_preparation/missing_data:travel-times}}
\sphinxAtStartPar
Another dataset from the same source contains a collection of recorded travel times and specific information about the travel itself as e.g.: the day of the week, where they were going, …

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/travel\PYGZhy{}times.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{travel\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
          Date StartTime  DayOfWeek GoingTo  Distance  MaxSpeed  AvgSpeed  \PYGZbs{}
0     1/6/2012     16:37     Friday    Home     51.29     127.4      78.3   
1     1/6/2012     08:20     Friday     GSK     51.63     130.3      81.8   
2     1/4/2012     16:17  Wednesday    Home     51.27     127.4      82.0   
3     1/4/2012     07:53  Wednesday     GSK     49.17     132.3      74.2   
4     1/3/2012     18:57    Tuesday    Home     51.15     136.2      83.4   
..         ...       ...        ...     ...       ...       ...       ...   
200  7/18/2011     08:09     Monday     GSK     54.52     125.6      49.9   
201  7/14/2011     08:03   Thursday     GSK     50.90     123.7      76.2   
202  7/13/2011     17:08  Wednesday    Home     51.96     132.6      57.5   
203  7/12/2011     17:51    Tuesday    Home     53.28     125.8      61.6   
204  7/11/2011     16:56     Monday    Home     51.73     125.0      62.8   

     AvgMovingSpeed FuelEconomy  TotalTime  MovingTime Take407All Comments  
0              84.8         NaN       39.3        36.3         No      NaN  
1              88.9         NaN       37.9        34.9         No      NaN  
2              85.8         NaN       37.5        35.9         No      NaN  
3              82.9         NaN       39.8        35.6         No      NaN  
4              88.1         NaN       36.8        34.8         No      NaN  
..              ...         ...        ...         ...        ...      ...  
200            82.4        7.89       65.5        39.7         No      NaN  
201            95.1        7.89       40.1        32.1        Yes      NaN  
202            76.7         NaN       54.2        40.6        Yes      NaN  
203            87.6         NaN       51.9        36.5        Yes      NaN  
204            92.5         NaN       49.5        33.6        Yes      NaN  

[205 rows x 13 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
we have a total of 205 records and we can already see that the FuelEconomy column seems pretty bad, let’s quantify that.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{divide}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{travel\PYGZus{}df}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{100}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Date               0.00
StartTime          0.00
DayOfWeek          0.00
GoingTo            0.00
Distance           0.00
MaxSpeed           0.00
AvgSpeed           0.00
AvgMovingSpeed     0.00
FuelEconomy        8.29
TotalTime          0.00
MovingTime         0.00
Take407All         0.00
Comments          88.29
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
In the end, it doesn’t seem that bad, but there are comments and nearly none of them are filled in. Which in perspective is understandable. Let’s see what the comments look like

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{o}{\PYGZti{}}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{Comments}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
15                                  Put snow tires on
39                                         Heavy rain
49                                Huge traffic backup
50      Pumped tires up: check fuel economy improved?
52                                Backed up at Bronte
54                                Backed up at Bronte
60                                              Rainy
78                                   Rain, rain, rain
91                                   Rain, rain, rain
92         Accident: backup from Hamilton to 407 ramp
110                                           Raining
132                           Back to school traffic?
133                Took 407 all the way (to McMaster)
150                             Heavy volume on Derry
156                        Start early to run a batch
158    Accident at 403/highway 6; detour along Dundas
165                                      Detour taken
166                                    Must be Friday
172                             Medium amount of rain
174                                         New tires
182                              Turn around on Derry
184                                       Empty roads
187                            Police slowdown on 403
189                         Accident blocked 407 exit
Name: Comments, dtype: object
\end{sphinxVerbatim}

\sphinxAtStartPar
As you would expect, these comments are text based. Now imagine we would like to run some Natural Language Processing (NLP) on these, it would be a pain to perform string operations on it when it is riddled with missing values.

\sphinxAtStartPar
Here a simple example where we select all records containing the word ‘rain’, with no avail.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{lower}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ValueError}\PYG{g+gWhitespace}{                                }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}6376}\PYG{o}{/}\PYG{l+m+mf}{1298831137.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{lower}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/frame.py} in \PYG{n+ni}{\PYGZus{}\PYGZus{}getitem\PYGZus{}\PYGZus{}}\PYG{n+nt}{(self, key)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{3446} 
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{3447}         \PYG{c+c1}{\PYGZsh{} Do we have a (boolean) 1d indexer?}
\PYG{n+ne}{\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{3448}         \PYG{k}{if} \PYG{n}{com}\PYG{o}{.}\PYG{n}{is\PYGZus{}bool\PYGZus{}indexer}\PYG{p}{(}\PYG{n}{key}\PYG{p}{)}\PYG{p}{:}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{3449}             \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}getitem\PYGZus{}bool\PYGZus{}array}\PYG{p}{(}\PYG{n}{key}\PYG{p}{)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{3450} 

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/common.py} in \PYG{n+ni}{is\PYGZus{}bool\PYGZus{}indexer}\PYG{n+nt}{(key)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{137}                     \PYG{c+c1}{\PYGZsh{} Don\PYGZsq{}t raise on e.g. [\PYGZdq{}A\PYGZdq{}, \PYGZdq{}B\PYGZdq{}, np.nan], see}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{138}                     \PYG{c+c1}{\PYGZsh{}  test\PYGZus{}loc\PYGZus{}getitem\PYGZus{}list\PYGZus{}of\PYGZus{}labels\PYGZus{}categoricalindex\PYGZus{}with\PYGZus{}na}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{139}                     \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{n}{na\PYGZus{}msg}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{140}                 \PYG{k}{return} \PYG{k+kc}{False}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{141}             \PYG{k}{return} \PYG{k+kc}{True}

\PYG{n+ne}{ValueError}: Cannot mask with non\PYGZhy{}boolean array containing NA / NaN values
\end{sphinxVerbatim}

\sphinxAtStartPar
The last line of the python error traceback gives us the reason it failed, because there were NaN values present.

\sphinxAtStartPar
Luckily the string variable has more or less it’s on ‘null’ value, being an empty string, this way these operations are still possible, most of the comments will just contain nothing.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments} \PYG{o}{=} \PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{fillna}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{lower}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date StartTime  DayOfWeek GoingTo  Distance  MaxSpeed  AvgSpeed  \PYGZbs{}
39   11/29/2011     07:23    Tuesday     GSK     51.74     112.2      55.3   
60    11/9/2011     16:15  Wednesday    Home     51.28     121.4      65.9   
78   10/25/2011     17:24    Tuesday    Home     52.87     123.5      65.1   
91   10/12/2011     17:47  Wednesday    Home     51.40     114.4      59.7   
110   9/27/2011     07:36    Tuesday     GSK     50.65     128.1      86.3   
172    8/9/2011     08:15    Tuesday     GSK     49.08     134.8      60.5   

     AvgMovingSpeed FuelEconomy  TotalTime  MovingTime Take407All  \PYGZbs{}
39             61.0         NaN       56.2        50.9         No   
60             71.8        9.35       46.7        42.1         No   
78             72.4        8.97       48.7        43.8         No   
91             65.8        8.75       51.7        46.9         No   
110            88.6        8.31       35.2        34.3        Yes   
172            67.2        8.54       48.7        43.8         No   

                  Comments  
39              Heavy rain  
60                   Rainy  
78        Rain, rain, rain  
91        Rain, rain, rain  
110                Raining  
172  Medium amount of rain  
\end{sphinxVerbatim}

\sphinxAtStartPar
Fixed! now we can use the comments for analysis.

\sphinxAtStartPar
We still have to fix the FuelEconomy, let us take a look at the non NaN values

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{o}{\PYGZti{}}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{FuelEconomy}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date StartTime  DayOfWeek GoingTo  Distance  MaxSpeed  AvgSpeed  \PYGZbs{}
6      1/2/2012     17:31     Monday    Home     51.37     123.2      82.9   
7      1/2/2012     07:34     Monday     GSK     49.01     128.3      77.5   
8    12/23/2011     08:01     Friday     GSK     52.91     130.3      80.9   
9    12/22/2011     17:19   Thursday    Home     51.17     122.3      70.6   
10   12/22/2011     08:16   Thursday     GSK     49.15     129.4      74.0   
..          ...       ...        ...     ...       ...       ...       ...   
197   7/20/2011     08:24  Wednesday     GSK     48.50     125.8      75.7   
198   7/19/2011     17:17    Tuesday    Home     51.16     126.7      92.2   
199   7/19/2011     08:11    Tuesday     GSK     50.96     124.3      82.3   
200   7/18/2011     08:09     Monday     GSK     54.52     125.6      49.9   
201   7/14/2011     08:03   Thursday     GSK     50.90     123.7      76.2   

     AvgMovingSpeed FuelEconomy  TotalTime  MovingTime Take407All Comments  
6              87.3           \PYGZhy{}       37.2        35.3         No           
7              85.9           \PYGZhy{}       37.9        34.3         No           
8              88.3        8.89       39.3        36.0         No           
9              78.1        8.89       43.5        39.3         No           
10             81.4        8.89       39.8        36.2         No           
..              ...         ...        ...         ...        ...      ...  
197            87.3        7.89       38.5        33.3        Yes           
198           102.6        7.89       33.3        29.9        Yes           
199            96.4        7.89       37.2        31.7        Yes           
200            82.4        7.89       65.5        39.7         No           
201            95.1        7.89       40.1        32.1        Yes           

[188 rows x 13 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems that aside NaN values there are also other intruders, a quick check on the data type (Dtype) reveils it is not recognised as a number!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 205 entries, 0 to 204
Data columns (total 13 columns):
 \PYGZsh{}   Column          Non\PYGZhy{}Null Count  Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}          \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   Date            205 non\PYGZhy{}null    object 
 1   StartTime       205 non\PYGZhy{}null    object 
 2   DayOfWeek       205 non\PYGZhy{}null    object 
 3   GoingTo         205 non\PYGZhy{}null    object 
 4   Distance        205 non\PYGZhy{}null    float64
 5   MaxSpeed        205 non\PYGZhy{}null    float64
 6   AvgSpeed        205 non\PYGZhy{}null    float64
 7   AvgMovingSpeed  205 non\PYGZhy{}null    float64
 8   FuelEconomy     188 non\PYGZhy{}null    object 
 9   TotalTime       205 non\PYGZhy{}null    float64
 10  MovingTime      205 non\PYGZhy{}null    float64
 11  Take407All      205 non\PYGZhy{}null    object 
 12  Comments        205 non\PYGZhy{}null    object 
dtypes: float64(6), object(7)
memory usage: 20.9+ KB
\end{sphinxVerbatim}

\sphinxAtStartPar
The column is noted as an object or string type, meaning that these numbers are given as ‘9.24’ instead of 9.24 and numerical operations are not possible.
We can cast them to numeric but have to warn pandas to coerce errors, meaning errors will be converted to NaN values.
Later we’ll handle the NaN’s.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{FuelEconomy} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}numeric}\PYG{p}{(}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{FuelEconomy}\PYG{p}{,} \PYG{n}{errors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{coerce}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 205 entries, 0 to 204
Data columns (total 13 columns):
 \PYGZsh{}   Column          Non\PYGZhy{}Null Count  Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}          \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   Date            205 non\PYGZhy{}null    object 
 1   StartTime       205 non\PYGZhy{}null    object 
 2   DayOfWeek       205 non\PYGZhy{}null    object 
 3   GoingTo         205 non\PYGZhy{}null    object 
 4   Distance        205 non\PYGZhy{}null    float64
 5   MaxSpeed        205 non\PYGZhy{}null    float64
 6   AvgSpeed        205 non\PYGZhy{}null    float64
 7   AvgMovingSpeed  205 non\PYGZhy{}null    float64
 8   FuelEconomy     186 non\PYGZhy{}null    float64
 9   TotalTime       205 non\PYGZhy{}null    float64
 10  MovingTime      205 non\PYGZhy{}null    float64
 11  Take407All      205 non\PYGZhy{}null    object 
 12  Comments        205 non\PYGZhy{}null    object 
dtypes: float64(7), object(6)
memory usage: 20.9+ KB
\end{sphinxVerbatim}

\sphinxAtStartPar
Wonderful, now the column is numerical and we can see 2 more missing values have popped up!
We could easily drop these 19 records and have a complete dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date StartTime  DayOfWeek GoingTo  Distance  MaxSpeed  AvgSpeed  \PYGZbs{}
8    12/23/2011     08:01     Friday     GSK     52.91     130.3      80.9   
9    12/22/2011     17:19   Thursday    Home     51.17     122.3      70.6   
10   12/22/2011     08:16   Thursday     GSK     49.15     129.4      74.0   
11   12/21/2011     07:45  Wednesday     GSK     51.77     124.8      71.7   
12   12/20/2011     16:05    Tuesday    Home     51.45     130.1      75.2   
..          ...       ...        ...     ...       ...       ...       ...   
197   7/20/2011     08:24  Wednesday     GSK     48.50     125.8      75.7   
198   7/19/2011     17:17    Tuesday    Home     51.16     126.7      92.2   
199   7/19/2011     08:11    Tuesday     GSK     50.96     124.3      82.3   
200   7/18/2011     08:09     Monday     GSK     54.52     125.6      49.9   
201   7/14/2011     08:03   Thursday     GSK     50.90     123.7      76.2   

     AvgMovingSpeed  FuelEconomy  TotalTime  MovingTime Take407All Comments  
8              88.3         8.89       39.3        36.0         No           
9              78.1         8.89       43.5        39.3         No           
10             81.4         8.89       39.8        36.2         No           
11             78.9         8.89       43.3        39.4         No           
12             82.7         8.89       41.1        37.3         No           
..              ...          ...        ...         ...        ...      ...  
197            87.3         7.89       38.5        33.3        Yes           
198           102.6         7.89       33.3        29.9        Yes           
199            96.4         7.89       37.2        31.7        Yes           
200            82.4         7.89       65.5        39.7         No           
201            95.1         7.89       40.1        32.1        Yes           

[186 rows x 13 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
However im leaving them as an excercise for you to apply a technique we will see in the next part


\section{Material properties}
\label{\detokenize{c2_data_preparation/missing_data:material-properties}}
\sphinxAtStartPar
Another dataset from the same source contains the material properties from 30 samples, this time there is not timestamp as the samples are not related in time with each other.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{material\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/raw\PYGZhy{}material\PYGZhy{}properties.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{material\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    Sample  size1  size2  size3  density1  density2  density3
0   X12558  0.696   2.69   6.38      41.8     17.18      3.90
1   X14728  0.636   2.30   5.14      38.1     12.73      3.89
2   X15468  0.841   2.85   5.20      37.6     13.58      3.98
3   X21364  0.609   2.13   4.62      34.2     11.12      4.02
4   X23671  0.684   2.16   4.87      36.4     12.24      3.92
5   X24055  0.762   2.81   6.36      38.1     13.28      3.89
6   X24905  0.552   2.34   5.03      41.3     16.71      3.86
7   X25917  0.501   2.17   5.09       NaN       NaN       NaN
8   X27871  0.619   2.11   5.13       NaN       NaN       NaN
9   X28690  0.610   2.10   4.18      35.0     12.15      3.86
10  X31385  0.532   2.09   4.93       NaN       NaN       NaN
11  X31813  0.738   2.29   5.47       NaN       NaN       NaN
12  X32807  0.779   2.62   5.59       NaN       NaN       NaN
13  X33943  0.537   2.23   5.41      35.2     11.34      3.99
14  X35035  0.702   2.05   5.10      34.2     10.54      4.02
15  X39223  0.768   2.51   5.09      34.9     12.55      3.90
16  X40503  0.714   2.56   6.03      35.6     12.20      4.02
17  X41400  0.621   2.42   5.10      38.7     14.27      3.98
18  X42988  0.726   2.11   4.69      37.1     13.14      3.98
19  X44749  0.698   2.36   5.40      36.6     12.16      4.01
20  X45295    NaN    NaN    NaN      38.1     13.34      3.89
21  X46965  0.759   2.47   4.83      38.7     14.83      3.89
22  X49666  0.535   2.13   5.23       NaN       NaN       NaN
23  X50678  0.716   2.29   5.45      37.3     13.70      3.92
24  X52894  0.635   2.08   4.94       NaN       NaN       NaN
25  X53925  0.598   2.12   4.69      37.9     13.45      3.78
26  X54254  0.700   2.47   5.22      38.8     14.72      3.92
27  X54272  0.957   2.96   7.37      36.2     13.38      4.20
28  X54394  0.759   2.66   5.36      35.2     12.19      3.98
29  X55408  0.661   2.10   4.27       NaN       NaN       NaN
30  X56952  0.646   2.38   4.51      40.1     15.68      3.86
31  X57095  0.662   2.34   4.71      35.0     12.37      3.90
32  X57128  0.749   2.43   5.16      37.3     13.04      3.92
33  X61870  0.598   2.21   4.90       NaN       NaN       NaN
34  X61888  0.619   2.59   5.81       NaN       NaN       NaN
35  X72736  0.693   2.05   5.02      39.6     15.55      3.94
\end{sphinxVerbatim}

\sphinxAtStartPar
let us quantify the amount of missing data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{material\PYGZus{}df}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{divide}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{material\PYGZus{}df}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{100}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Sample       0.00
size1        2.78
size2        2.78
size3        2.78
density1    27.78
density2    27.78
density3    27.78
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
Unfortunately that is a lot of missing data, covered in all records, dropping here seems almost impossible if we want to keep a healthy amount of records.

\sphinxAtStartPar
Here it would be wise to go for a more elaborate method of imputation, I opted for the K\sphinxhyphen{}nearest neighbours method, which looks at the K most similar records in the dataset to make an educated guess on what the missing value could be, this because we can assume that records with similar data are also similar over all the properties (columns).

\sphinxAtStartPar
Im using the sklearn library for this, which has more imputation techniques such as MICE.
More info can be found \sphinxhref{https://scikit-learn.org/stable/modules/impute.html}{here}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{impute} \PYG{k+kn}{import} \PYG{n}{KNNImputer}
\end{sphinxVerbatim}

\sphinxAtStartPar
im creating an imputer object and specify that i want to use the 5 most similar records and weigh them by distance from the to imputed record, meaning closer neighbours are more important.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{imputer} \PYG{o}{=} \PYG{n}{KNNImputer}\PYG{p}{(}\PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{weights}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{distance}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
As the imputer only takes numerical values I had to do some pandas magic and drop the first column, which I then added again. The result is a fully filled dataset, you can recognise the new values as they are not rounded.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}
    \PYG{n}{imputer}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{material\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} 
    \PYG{n}{columns}\PYG{o}{=}\PYG{n}{material\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       size1     size2     size3   density1   density2  density3
0   0.696000  2.690000  6.380000  41.800000  17.180000  3.900000
1   0.636000  2.300000  5.140000  38.100000  12.730000  3.890000
2   0.841000  2.850000  5.200000  37.600000  13.580000  3.980000
3   0.609000  2.130000  4.620000  34.200000  11.120000  4.020000
4   0.684000  2.160000  4.870000  36.400000  12.240000  3.920000
5   0.762000  2.810000  6.360000  38.100000  13.280000  3.890000
6   0.552000  2.340000  5.030000  41.300000  16.710000  3.860000
7   0.501000  2.170000  5.090000  38.495282  14.029399  3.931180
8   0.619000  2.110000  5.130000  37.405275  13.157346  3.943667
9   0.610000  2.100000  4.180000  35.000000  12.150000  3.860000
10  0.532000  2.090000  4.930000  37.811132  13.646072  3.908364
11  0.738000  2.290000  5.470000  37.088833  13.255412  3.941654
12  0.779000  2.620000  5.590000  36.540567  12.889902  3.970973
13  0.537000  2.230000  5.410000  35.200000  11.340000  3.990000
14  0.702000  2.050000  5.100000  34.200000  10.540000  4.020000
15  0.768000  2.510000  5.090000  34.900000  12.550000  3.900000
16  0.714000  2.560000  6.030000  35.600000  12.200000  4.020000
17  0.621000  2.420000  5.100000  38.700000  14.270000  3.980000
18  0.726000  2.110000  4.690000  37.100000  13.140000  3.980000
19  0.698000  2.360000  5.400000  36.600000  12.160000  4.010000
20  0.733097  2.653959  5.881504  38.100000  13.340000  3.890000
21  0.759000  2.470000  4.830000  38.700000  14.830000  3.890000
22  0.535000  2.130000  5.230000  37.391815  13.089536  3.944335
23  0.716000  2.290000  5.450000  37.300000  13.700000  3.920000
24  0.635000  2.080000  4.940000  37.254724  13.206262  3.933904
25  0.598000  2.120000  4.690000  37.900000  13.450000  3.780000
26  0.700000  2.470000  5.220000  38.800000  14.720000  3.920000
27  0.957000  2.960000  7.370000  36.200000  13.380000  4.200000
28  0.759000  2.660000  5.360000  35.200000  12.190000  3.980000
29  0.661000  2.100000  4.270000  36.172345  12.755632  3.887375
30  0.646000  2.380000  4.510000  40.100000  15.680000  3.860000
31  0.662000  2.340000  4.710000  35.000000  12.370000  3.900000
32  0.749000  2.430000  5.160000  37.300000  13.040000  3.920000
33  0.598000  2.210000  4.900000  37.865882  13.826029  3.887021
34  0.619000  2.590000  5.810000  35.932339  12.318210  3.989911
35  0.693000  2.050000  5.020000  39.600000  15.550000  3.940000
\end{sphinxVerbatim}

\sphinxAtStartPar
This concludes the part of missing values, perhaps you can try yourself and impute the missing values for the FuelEconomy using the SimpleImputer or even the IterativeImputer.


\chapter{Concatenation and deduplication}
\label{\detokenize{c2_data_preparation/concatenation_deduplication:concatenation-and-deduplication}}\label{\detokenize{c2_data_preparation/concatenation_deduplication::doc}}
\sphinxAtStartPar
In this notebook we are going to investigate the concepts of stitching data files (concatenation) and verifying the integrity of our data concercing duplicates


\section{Concatenation}
\label{\detokenize{c2_data_preparation/concatenation_deduplication:concatenation}}
\sphinxAtStartPar
When dealing with large amounts of data, fractioning is often the only solution.
Not only does this tidy up your data space, but it also benefits computation.
Aside from that, appending new data to your data lake is independent of the historical data.
However if you want to perform historical analysis this means you will need to perform additional operations.

\sphinxAtStartPar
In this notebook we have a setup of a very small data lake containing daily minimal temperatures.
If you would look closely in the url you would see the following structure.
\begin{quote}

\sphinxAtStartPar
data/temperature/australia/melbourne/1981.csv
\end{quote}

\sphinxAtStartPar
This is a straight\sphinxhyphen{}forward but perfect example on how fragmentation works, in our data lake we have:
temperatures data fractioned by country, city and year. As we are working with daily temperatures further fractioning would not be interesting, but you could fraction e.g. per month.

\sphinxAtStartPar
In the cells below, we read our both 1981 and 1982 data and concatenate them using python.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{melbourne\PYGZus{}1981\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/temperatures/australia/melbourne/1981.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{melbourne\PYGZus{}1982\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/temperatures/australia/melbourne/1982.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}
    \PYG{p}{[}
        \PYG{n}{melbourne\PYGZus{}1981\PYGZus{}df}\PYG{p}{,}
        \PYG{n}{melbourne\PYGZus{}1982\PYGZus{}df}\PYG{p}{,}
    \PYG{p}{]}
\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date  Temp
0    1981\PYGZhy{}01\PYGZhy{}01  20.7
1    1981\PYGZhy{}01\PYGZhy{}02  17.9
2    1981\PYGZhy{}01\PYGZhy{}03  18.8
3    1981\PYGZhy{}01\PYGZhy{}04  14.6
4    1981\PYGZhy{}01\PYGZhy{}05  15.8
..          ...   ...
360  1982\PYGZhy{}12\PYGZhy{}27  15.3
361  1982\PYGZhy{}12\PYGZhy{}28  16.3
362  1982\PYGZhy{}12\PYGZhy{}29  15.8
363  1982\PYGZhy{}12\PYGZhy{}30  17.7
364  1982\PYGZhy{}12\PYGZhy{}31  16.3

[730 rows x 2 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
And there you have it! we now have a dataframe containing both data from 1981 as 1982.
Can you figure out what I calculated in the next cell? Do you think there might be a more ‘clean’ solution?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{Date}\PYG{o}{.}\PYG{n}{str}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{:}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{Temp}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
17.140322580645158
\end{sphinxVerbatim}

\sphinxAtStartPar
As an exercise I would ask you now to create a small python script that given a begin and end year (between 1981 and 1990) can automatically concatenate all the necessary data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1982}\PYG{p}{,}\PYG{l+m+mi}{1987}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{i}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1982
1983
1984
1985
1986
\end{sphinxVerbatim}


\section{Deduplication}
\label{\detokenize{c2_data_preparation/concatenation_deduplication:deduplication}}
\sphinxAtStartPar
Another important aspect of data cleaning is the removal of duplicates.
Here we fragment of a dataset from activity on a popular games platform.
We can see which user has either bought or played specific games and how often.
Unfortunately for some reason, entries might have duplicates which we have to deal with as otherwise users might have e.g. bought a game twice.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/steam.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game    action  freq
0      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
1      11373749                   Sid Meier\PYGZsq{}s Civilization IV      play   0.1
2      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
3      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
4      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
...         ...                                           ...       ...   ...
1834  112845094                                        Arma 2  purchase   1.0
1835  112845094                  Grand Theft Auto San Andreas  purchase   1.0
1836  112845094                    Grand Theft Auto Vice City  purchase   1.0
1837  112845094                    Grand Theft Auto Vice City  purchase   1.0
1838  112845094                          Grand Theft Auto III  purchase   1.0

[1839 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
We have a dataframe with 1839 interactions, you can see that the freq either notes the amount they bought (which always 1 as there is not use in buying it more) or the amount in hours they played.

\sphinxAtStartPar
Let us straightforward ask pandas to remove all rows that have an exact duplicate

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game    action  freq
0      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
1      11373749                   Sid Meier\PYGZsq{}s Civilization IV      play   0.1
3      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
5      11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
7      56038151                       Tom Clancy\PYGZsq{}s H.A.W.X. 2  purchase   1.0
...         ...                                           ...       ...   ...
1831  112845094                  Grand Theft Auto San Andreas  purchase   1.0
1832  112845094                  Grand Theft Auto San Andreas      play   0.2
1833  112845094                          Grand Theft Auto III  purchase   1.0
1834  112845094                                        Arma 2  purchase   1.0
1836  112845094                    Grand Theft Auto Vice City  purchase   1.0

[1132 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Alright! this seemed to have dropped 707 rows from our dataset, but we would like to know more about those.
Let’s ask which rows the algorithm has dropped:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game    action  freq
2      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
4      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
6      11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
10     56038151                  Grand Theft Auto San Andreas  purchase   1.0
12     56038151                    Grand Theft Auto Vice City  purchase   1.0
...         ...                                           ...       ...   ...
1827   39146470          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
1830   48666962                                      Crysis 2  purchase   1.0
1835  112845094                  Grand Theft Auto San Andreas  purchase   1.0
1837  112845094                    Grand Theft Auto Vice City  purchase   1.0
1838  112845094                          Grand Theft Auto III  purchase   1.0

[707 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we can see the duplicates, no particular pattern seems to be present, we could just for curiosity count the games that are duplicated

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{game}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Grand Theft Auto San Andreas                    172
Grand Theft Auto Vice City                      103
Sid Meier\PYGZsq{}s Civilization IV                      98
Grand Theft Auto III                             90
Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword     80
Sid Meier\PYGZsq{}s Civilization IV Warlords             79
Sid Meier\PYGZsq{}s Civilization IV Colonization         75
Crysis 2                                          7
Arma 2                                            1
Tom Clancy\PYGZsq{}s H.A.W.X. 2                           1
TERA                                              1
Name: game, dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems there are some games which are very prone to being duplicated, at this point we could go and ask the IT department why these games are acting weird.

\sphinxAtStartPar
Another thing im interested about is the perspective of a single gamer, here we took a single user\_id and printed all his games

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{user\PYGZus{}id} \PYG{o}{==} \PYG{l+m+mi}{11373749}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    user\PYGZus{}id                                          game    action  freq
0  11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
1  11373749                   Sid Meier\PYGZsq{}s Civilization IV      play   0.1
2  11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
3  11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
4  11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
5  11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
6  11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
\end{sphinxVerbatim}

\sphinxAtStartPar
Ah, you can see all of his three games are somehow duplicated in purchase, also it seems he only played one of them for only 0.1 hours.
Looks like he fell to the bait of a tempting summer sale but didn’t realise he had no time to actually play it.

\sphinxAtStartPar
Another thing I would like to mention here is that this dataset would make a fine recommender system as it contains user ids and hours played.
Add game metadata (description) and reviews to the mix and your data preparation is done!

\sphinxAtStartPar
We can remove all duplicates now by overwriting our dataframe

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
One thing still bothers me, as hours played can change over time it might be that different snapshots have produced different values, therefore more duplicates might be present with different hours\_played.

\sphinxAtStartPar
Time to investigate this by using a subset of columns in the drop\_duplicates algorithm

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{game}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{action}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game    action  freq
0      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
1      11373749                   Sid Meier\PYGZsq{}s Civilization IV      play   0.1
3      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
5      11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
7      56038151                       Tom Clancy\PYGZsq{}s H.A.W.X. 2  purchase   1.0
...         ...                                           ...       ...   ...
1831  112845094                  Grand Theft Auto San Andreas  purchase   1.0
1832  112845094                  Grand Theft Auto San Andreas      play   0.2
1833  112845094                          Grand Theft Auto III  purchase   1.0
1834  112845094                                        Arma 2  purchase   1.0
1836  112845094                    Grand Theft Auto Vice City  purchase   1.0

[1120 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Seems we have shaved off another 12 records, so our intuition was right, again lets see which the duplicates are:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{game}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{action}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game action  freq
118   118664413                  Grand Theft Auto San Andreas   play   0.2
458    50769696                  Grand Theft Auto San Andreas   play   3.1
521    71411882                          Grand Theft Auto III   play   0.2
607    33865373                   Sid Meier\PYGZsq{}s Civilization IV   play   2.0
898    71510748                  Grand Theft Auto San Andreas   play   0.2
908    28472068                    Grand Theft Auto Vice City   play   0.4
910    28472068                  Grand Theft Auto San Andreas   play   0.2
912    28472068                          Grand Theft Auto III   play   0.1
1506   59925638                       Tom Clancy\PYGZsq{}s H.A.W.X. 2   play   0.3
1553  148362155                  Grand Theft Auto San Andreas   play  12.5
1709  176261926  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword   play   0.4
1711  176261926                   Sid Meier\PYGZsq{}s Civilization IV   play   0.2
\end{sphinxVerbatim}

\sphinxAtStartPar
As expected the duplicates are all in the ‘play’ action, to complete our view we extract the data of a single user

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{user\PYGZus{}id}\PYG{o}{==}\PYG{l+m+mi}{118664413}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       user\PYGZus{}id                          game    action  freq
115  118664413  Grand Theft Auto San Andreas  purchase   1.0
116  118664413  Grand Theft Auto San Andreas      play   1.9
118  118664413  Grand Theft Auto San Andreas      play   0.2
\end{sphinxVerbatim}

\sphinxAtStartPar
It looks like we have a problem now, we know these are duplicates and should be removed, but which one?
Personally I would argue here that we keep the highest value, as it is impossible to ‘unplay’ hours on the game.
I will leave this as an exercise for you, but the solution is pretty tricky so i’ll give a hint:

\sphinxAtStartPar
The algorithm always keeps the first record in case of duplicates, so you could sort the rows making sure the higher value is always encountered first, good luck!


\chapter{Outliers and validity}
\label{\detokenize{c2_data_preparation/outliers:outliers-and-validity}}\label{\detokenize{c2_data_preparation/outliers::doc}}
\sphinxAtStartPar
When preparing data we have to be cautious with the accuracy of our set.
Outliers and invalid data points are difficult to detect but should be handled with caution.

\sphinxAtStartPar
we start out by importing our most important library.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}


\section{Silicon wafer thickness}
\label{\detokenize{c2_data_preparation/outliers:silicon-wafer-thickness}}
\sphinxAtStartPar
Our first dataset contains information about the production of silicon wafers, each wafers thickness is measure on 9 different spots.
More information on the dataset can be found \sphinxhref{https://openmv.net/info/silicon-wafer-thickness}{here}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{wafer\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/silicon\PYGZhy{}wafer\PYGZhy{}thickness.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{wafer\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
      G1     G2     G3     G4     G5     G6     G7     G8     G9
0  0.175  0.188 \PYGZhy{}0.159  0.095  0.374 \PYGZhy{}0.238 \PYGZhy{}0.800  0.158 \PYGZhy{}0.211
1  0.102  0.075  0.141  0.180  0.138 \PYGZhy{}0.057 \PYGZhy{}0.075  0.072  0.072
2  0.607  0.711  0.879  0.765  0.592  0.187  0.431  0.345  0.187
3  0.774  0.823  0.619  0.370  0.725  0.439 \PYGZhy{}0.025 \PYGZhy{}0.259  0.496
4  0.504  0.644  0.845  0.681  0.502  0.151  0.404  0.296  0.260
\end{sphinxVerbatim}

\sphinxAtStartPar
we would like to investigate the distribution of measurements here, as we are early in this course using visualisation techniques would be too soon.
This does not mean we can’t use simple mathematics, introducing the InterQuartile Range.
A reason for using IQR over standard deviation is that with IQR we do not assume a normal distribution.
The IQR calculates the range between the bottom ‘quart’ or 25\% and the top 25\%, giving us an indication of the spread of our results, we calculate this IQR for each of the 9 measurements independently.
For more info about IQR you can visit \sphinxhref{https://en.wikipedia.org/wiki/Interquartile\_range}{wikipedia}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{iqr} \PYG{o}{=} \PYG{n}{wafer\PYGZus{}df}\PYG{o}{.}\PYG{n}{quantile}\PYG{p}{(}\PYG{l+m+mf}{0.75}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{wafer\PYGZus{}df}\PYG{o}{.}\PYG{n}{quantile}\PYG{p}{(}\PYG{l+m+mf}{0.25}\PYG{p}{)}
\PYG{n}{iqr}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
G1    0.54425
G2    0.61000
G3    0.54075
G4    0.52475
G5    0.61175
G6    0.86750
G7    0.76175
G8    0.87225
G9    0.86300
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
you can see that the IQR spread for each measurement lays between 0.5 and 1 unit indicating that the 9 measurements of the wafer have a similar spread.
With these IQR’s we could calculate for each point relative to the spread of the measurement how far it is from the median.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{relative\PYGZus{}spread\PYGZus{}df} \PYG{o}{=} \PYG{p}{(}\PYG{n}{wafer\PYGZus{}df}\PYG{o}{\PYGZhy{}}\PYG{n}{wafer\PYGZus{}df}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{iqr}
\PYG{n}{relative\PYGZus{}spread\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
         G1        G2        G3        G4        G5        G6        G7  \PYGZbs{}
0 \PYGZhy{}0.011024 \PYGZhy{}0.077869 \PYGZhy{}0.819233 \PYGZhy{}0.367794  0.176543 \PYGZhy{}0.352738 \PYGZhy{}1.029865   
1 \PYGZhy{}0.145154 \PYGZhy{}0.263115 \PYGZhy{}0.264448 \PYGZhy{}0.205812 \PYGZhy{}0.209236 \PYGZhy{}0.144092 \PYGZhy{}0.078110   
2  0.782729  0.779508  1.100324  0.909004  0.532897  0.137176  0.586150   
3  1.089573  0.963115  0.619510  0.156265  0.750306  0.427666 \PYGZhy{}0.012471   
4  0.593477  0.669672  1.037448  0.748928  0.385779  0.095677  0.550706   

         G8        G9  
0 \PYGZhy{}0.130696 \PYGZhy{}0.254925  
1 \PYGZhy{}0.229292  0.073001  
2  0.083692  0.206257  
3 \PYGZhy{}0.608770  0.564311  
4  0.027515  0.290846  
\end{sphinxVerbatim}

\sphinxAtStartPar
You can now see that some points are close to the median, whilst others are much higher, both positive as negative.
By defining a threshold, we quantify what deviation has to be there to flag a reading as an outlier.
The high outliers are seperated, note that only a single measurement of the 9 can trigger and render the total measurement as an outlier.
Yet judging from the setup where we would want to find wafers with varying thickness that approach is desirable.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{relative\PYGZus{}spread\PYGZus{}df}\PYG{p}{[}\PYG{p}{(}\PYG{n}{relative\PYGZus{}spread\PYGZus{}df}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{columns}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            G1         G2         G3         G4         G5        G6  \PYGZbs{}
8     2.232430   2.009016   1.956542   1.589328   1.843890  1.544669   
38   12.891135  12.827049  12.832178  13.913292  11.429506  9.500865   
39    3.691318   3.981148   3.774387   4.081944   3.248059  3.729107   
61    2.010106   2.153279   1.987980   1.863745   1.858602  1.274928   
110   3.678457   2.841803   3.204808   3.180562   2.669391  0.518732   
112   2.361047   2.086066   2.363384   2.107670   1.925623  1.238040   
117   1.475425   1.043443   2.154415   2.582182   0.653862  1.823631   
120   1.791456   1.484426   2.583449   1.440686   2.085819  0.990202   
121   1.791456   1.484426   2.583449   1.440686   2.085819  0.990202   
152   2.610932   2.102459   2.387425   2.549786   2.169187  1.730259   
154  \PYGZhy{}0.529169  \PYGZhy{}0.538525  \PYGZhy{}0.404993  \PYGZhy{}0.331586  \PYGZhy{}0.552513  4.565994   

            G7        G8        G9  
8     1.233344  0.419604  1.582851  
38   10.305875  9.927200  9.055620  
39    3.304890  3.846374  3.149479  
61    1.237283  0.825451  0.955968  
110   0.700361  0.176555  0.727694  
112   1.766328  0.890800  1.377752  
117   1.581227  0.857552  1.188876  
120   1.782081  1.034107  1.822711  
121   1.782081  1.034107  1.822711  
152   2.241549  1.713958  1.592121  
154  \PYGZhy{}0.051854 \PYGZhy{}0.382918 \PYGZhy{}0.536501  
\end{sphinxVerbatim}

\sphinxAtStartPar
seems we have a few high outliers, you can clearly see the measurements are mostly all across the board high, but in some cases (e.g. id 154) only one measurement was an outlier.
We can do the same for the low outliers.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{relative\PYGZus{}spread\PYGZus{}df}\PYG{p}{[}\PYG{p}{(}\PYG{n}{relative\PYGZus{}spread\PYGZus{}df}\PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{columns}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           G1        G2        G3        G4        G5        G6        G7  \PYGZbs{}
54  \PYGZhy{}1.550758 \PYGZhy{}1.525410 \PYGZhy{}1.843736 \PYGZhy{}2.082897 \PYGZhy{}1.659174 \PYGZhy{}1.203458 \PYGZhy{}1.184772   
56  \PYGZhy{}1.732660 \PYGZhy{}1.510656 \PYGZhy{}2.121128 \PYGZhy{}2.122916 \PYGZhy{}1.781774 \PYGZhy{}1.521614 \PYGZhy{}1.909419   
59  \PYGZhy{}1.971520 \PYGZhy{}1.310656 \PYGZhy{}2.328248 \PYGZhy{}1.175798 \PYGZhy{}2.067838 \PYGZhy{}0.915274 \PYGZhy{}1.783394   
64  \PYGZhy{}1.234727 \PYGZhy{}1.361475 \PYGZhy{}0.736015 \PYGZhy{}1.055741 \PYGZhy{}2.224765 \PYGZhy{}0.839193 \PYGZhy{}0.679357   
65  \PYGZhy{}2.226918 \PYGZhy{}1.194262 \PYGZhy{}2.117429 \PYGZhy{}2.161029 \PYGZhy{}2.043318 \PYGZhy{}0.190202 \PYGZhy{}1.004923   
102 \PYGZhy{}2.484153 \PYGZhy{}2.330328 \PYGZhy{}1.568192 \PYGZhy{}2.808957 \PYGZhy{}1.945239 \PYGZhy{}1.340634 \PYGZhy{}0.846078   

           G8        G9  
54  \PYGZhy{}1.650903 \PYGZhy{}1.245655  
56  \PYGZhy{}1.782746 \PYGZhy{}1.159907  
59  \PYGZhy{}1.304672 \PYGZhy{}1.514484  
64  \PYGZhy{}0.865578 \PYGZhy{}0.663963  
65  \PYGZhy{}0.270565 \PYGZhy{}0.794902  
102 \PYGZhy{}1.691029 \PYGZhy{}0.887601  
\end{sphinxVerbatim}

\sphinxAtStartPar
For a simple mathematical equation these result look promising, yet it can always be more sophisticated.
Not going to deep into the subject we could perform some Machine Learning, using a unsupervised method.
Here we use the sklearn library which contains the Isolation forest algorithm.
More info about the algorithm \sphinxhref{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html}{here}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{ensemble} \PYG{k+kn}{import} \PYG{n}{IsolationForest}
\end{sphinxVerbatim}

\sphinxAtStartPar
We first create the classifier and train (fit) it with the generic wafer data.
Then for each record of the wafer data we make a prediction, if it thinks its an outlier, we keep them

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{clf} \PYG{o}{=} \PYG{n}{IsolationForest}\PYG{p}{(}\PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{wafer\PYGZus{}df}\PYG{p}{)}
\PYG{n}{wafer\PYGZus{}df}\PYG{p}{[}\PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{wafer\PYGZus{}df}\PYG{p}{)}\PYG{o}{==}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        G1     G2     G3     G4     G5     G6     G7     G8     G9
8    1.396  1.461  1.342  1.122  1.394  1.408  0.924  0.638  1.375
20  \PYGZhy{}0.558 \PYGZhy{}0.705 \PYGZhy{}0.526 \PYGZhy{}0.412 \PYGZhy{}0.753 \PYGZhy{}0.998 \PYGZhy{}0.270  0.598 \PYGZhy{}1.416
38   7.197  8.060  7.223  7.589  7.258  8.310  7.835  8.931  7.824
39   2.190  2.664  2.325  2.430  2.253  3.303  2.502  3.627  2.727
54  \PYGZhy{}0.663 \PYGZhy{}0.695 \PYGZhy{}0.713 \PYGZhy{}0.805 \PYGZhy{}0.749 \PYGZhy{}0.976 \PYGZhy{}0.918 \PYGZhy{}1.168 \PYGZhy{}1.066
56  \PYGZhy{}0.762 \PYGZhy{}0.686 \PYGZhy{}0.863 \PYGZhy{}0.826 \PYGZhy{}0.824 \PYGZhy{}1.252 \PYGZhy{}1.470 \PYGZhy{}1.283 \PYGZhy{}0.992
59  \PYGZhy{}0.892 \PYGZhy{}0.564 \PYGZhy{}0.975 \PYGZhy{}0.329 \PYGZhy{}0.999 \PYGZhy{}0.726 \PYGZhy{}1.374 \PYGZhy{}0.866 \PYGZhy{}1.298
61   1.275  1.549  1.359  1.266  1.403  1.174  0.927  0.992  0.834
65  \PYGZhy{}1.031 \PYGZhy{}0.493 \PYGZhy{}0.861 \PYGZhy{}0.846 \PYGZhy{}0.984 \PYGZhy{}0.097 \PYGZhy{}0.781  0.036 \PYGZhy{}0.677
102 \PYGZhy{}1.171 \PYGZhy{}1.186 \PYGZhy{}0.564 \PYGZhy{}1.186 \PYGZhy{}0.924 \PYGZhy{}1.095 \PYGZhy{}0.660 \PYGZhy{}1.203 \PYGZhy{}0.757
106 \PYGZhy{}0.659 \PYGZhy{}0.451 \PYGZhy{}0.692 \PYGZhy{}0.708 \PYGZhy{}0.595 \PYGZhy{}0.726 \PYGZhy{}1.031 \PYGZhy{}0.877 \PYGZhy{}1.080
110  2.183  1.969  2.017  1.957  1.899  0.518  0.518  0.426  0.637
112  1.466  1.508  1.562  1.394  1.444  1.142  1.330  1.049  1.198
117  0.984  0.872  1.449  1.643  0.666  1.650  1.189  1.020  1.035
120  1.156  1.141  1.681  1.044  1.542  0.927  1.342  1.174  1.582
121  1.156  1.141  1.681  1.044  1.542  0.927  1.342  1.174  1.582
152  1.602  1.518  1.575  1.626  1.593  1.569  1.692  1.767  1.383
\end{sphinxVerbatim}

\sphinxAtStartPar
Comparing the results with our IQR approach we see a lot of similarities, here the id 154 record did not show up as we already realised this was perhaps not a strong enough outlier.
You could enhance our IQR technique by checking the amount of measurements that are above the threshold and respond accordingly, I will leave you a little hint.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{relative\PYGZus{}spread\PYGZus{}df}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
G1    7
G2    7
G3    8
G4    6
G5    6
G6    3
G7    3
G8    2
G9    2
dtype: int64
\end{sphinxVerbatim}


\section{Distillation column}
\label{\detokenize{c2_data_preparation/outliers:distillation-column}}
\sphinxAtStartPar
As an exercise you can try the same technique to this dataset and see what you would find, good luck!
Be mindful that you do not incorporate the date as a variable in your outlier algorithm.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{distil\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/distillation\PYGZhy{}tower.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{distil\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date     Temp1    FlowC1     Temp2    TempC1     Temp3    TempC2  \PYGZbs{}
0    2000\PYGZhy{}08\PYGZhy{}21  139.9857  432.0636  377.8119  100.2204  492.1353  490.1459   
1    2000\PYGZhy{}08\PYGZhy{}23  131.0470  487.4029  371.3060  100.2297  482.2100  480.3128   
2    2000\PYGZhy{}08\PYGZhy{}26  118.2666  437.3516  378.4483  100.3084  488.7266  487.0040   
3    2000\PYGZhy{}08\PYGZhy{}29  118.1769  481.8314  378.0028   95.5766  493.1481  491.1137   
4    2000\PYGZhy{}08\PYGZhy{}30  120.7891  412.6471  377.8871   92.9052  490.2486  488.6641   
..          ...       ...       ...       ...       ...       ...       ...   
248  2003\PYGZhy{}01\PYGZhy{}26  130.8138  212.6385  341.5964  121.4354  468.3401  467.0299   
249  2003\PYGZhy{}01\PYGZhy{}28  128.9673  225.1412  349.8965  118.8604  479.7665  478.4652   
250  2003\PYGZhy{}01\PYGZhy{}31  130.5328  223.5965  345.9366  120.4027  474.5378  473.1145   
251  2003\PYGZhy{}02\PYGZhy{}03  128.5248  213.5613  343.4950  119.6989  469.3802  467.9954   
252  2003\PYGZhy{}02\PYGZhy{}04  131.0491  217.4117  346.1960  119.0825  474.6599  473.0381   

       TempC3     Temp4  PressureC1  ...    Temp10  FlowC3   FlowC4   Temp11  \PYGZbs{}
0    180.5578  187.4331    215.0627  ...  513.9653  8.6279  10.5988  30.8983   
1    172.6575  179.5089    205.0999  ...  504.5145  8.7662  10.7560  31.9099   
2    165.9400  172.9262    205.0304  ...  508.9997  8.5319  10.5737  29.9165   
3    167.2085  174.2338    205.2561  ...  514.1794  8.6260  10.6695  30.6229   
4    167.0326  173.9681    205.0883  ...  511.0948  8.5939  10.4922  29.4977   
..        ...       ...         ...  ...       ...     ...      ...      ...   
248  174.7639  180.7649    229.7393  ...  479.0290  5.5590   6.4470  16.4131   
249  176.2176  182.3646    230.5049  ...  491.2362  5.6342   6.4360  17.2385   
250  176.3310  182.2578    230.6638  ...  485.8786  5.4810   6.3575  16.9866   
251  174.6435  180.5093    230.5226  ...  480.2879  5.4727   6.4175  16.6778   
252  177.1088  183.1810    225.6420  ...  486.0253  5.4597   6.3291  16.8766   

       Temp12  InvTemp1  InvTemp2  InvTemp3  InvPressure1  VapourPressure  
0    489.9900    2.0409    2.6468    2.1681        4.3524         32.5026  
1    480.2888    2.0821    2.6932    2.2207        4.5497         34.8598  
2    486.6190    2.0550    2.6424    2.1796        4.5511         32.1666  
3    491.1304    2.0361    2.6455    2.1620        4.5464         30.4064  
4    487.6475    2.0507    2.6463    2.1704        4.5499         30.9238  
..        ...       ...       ...       ...           ...             ...  
248  466.3347    2.1444    2.9274    2.2127        4.0911         38.8507  
249  477.8816    2.0926    2.8580    2.1620        4.0783         34.2653  
250  472.3176    2.1172    2.8907    2.1855        4.0756         36.5717  
251  467.0001    2.1413    2.9113    2.2090        4.0780         38.1054  
252  472.2701    2.1174    2.8885    2.1844        4.1608         35.6298  

[253 rows x 28 columns]
\end{sphinxVerbatim}


\chapter{String operations}
\label{\detokenize{c2_data_preparation/string_operations:string-operations}}\label{\detokenize{c2_data_preparation/string_operations::doc}}

\chapter{Datetime operations}
\label{\detokenize{c2_data_preparation/datetime_operations:datetime-operations}}\label{\detokenize{c2_data_preparation/datetime_operations::doc}}
\sphinxAtStartPar
When our dataset contains time\sphinxhyphen{}related data, datetime operations are a great asset to our data science toolkit.
For this exercise we obtain a public covid dataset containing A LOT of information on infection cases, deaths, tests and vaccinations.

\sphinxAtStartPar
Let’s start by importing the data, as the dataset is about 60MB at the time of writing, this might take some time.
Perhaps you could think of a method to make this more efficient, do we always need all of the data?

\sphinxAtStartPar
More info about the data can be found \sphinxhref{https://github.com/owid/covid-19-data/tree/master/public/data}{here}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/owid/covid\PYGZhy{}19\PYGZhy{}data/master/public/data/owid\PYGZhy{}covid\PYGZhy{}data.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{on\PYGZus{}bad\PYGZus{}lines}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{skip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  iso\PYGZus{}code continent     location        date  total\PYGZus{}cases  new\PYGZus{}cases  \PYGZbs{}
0      AFG      Asia  Afghanistan  2020\PYGZhy{}02\PYGZhy{}24          5.0        5.0   
1      AFG      Asia  Afghanistan  2020\PYGZhy{}02\PYGZhy{}25          5.0        0.0   
2      AFG      Asia  Afghanistan  2020\PYGZhy{}02\PYGZhy{}26          5.0        0.0   
3      AFG      Asia  Afghanistan  2020\PYGZhy{}02\PYGZhy{}27          5.0        0.0   
4      AFG      Asia  Afghanistan  2020\PYGZhy{}02\PYGZhy{}28          5.0        0.0   

   new\PYGZus{}cases\PYGZus{}smoothed  total\PYGZus{}deaths  new\PYGZus{}deaths  new\PYGZus{}deaths\PYGZus{}smoothed  ...  \PYGZbs{}
0                 NaN           NaN         NaN                  NaN  ...   
1                 NaN           NaN         NaN                  NaN  ...   
2                 NaN           NaN         NaN                  NaN  ...   
3                 NaN           NaN         NaN                  NaN  ...   
4                 NaN           NaN         NaN                  NaN  ...   

   female\PYGZus{}smokers  male\PYGZus{}smokers  handwashing\PYGZus{}facilities  \PYGZbs{}
0             NaN           NaN                  37.746   
1             NaN           NaN                  37.746   
2             NaN           NaN                  37.746   
3             NaN           NaN                  37.746   
4             NaN           NaN                  37.746   

   hospital\PYGZus{}beds\PYGZus{}per\PYGZus{}thousand  life\PYGZus{}expectancy  human\PYGZus{}development\PYGZus{}index  \PYGZbs{}
0                         0.5            64.83                    0.511   
1                         0.5            64.83                    0.511   
2                         0.5            64.83                    0.511   
3                         0.5            64.83                    0.511   
4                         0.5            64.83                    0.511   

   excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}absolute  excess\PYGZus{}mortality\PYGZus{}cumulative  \PYGZbs{}
0                                   NaN                          NaN   
1                                   NaN                          NaN   
2                                   NaN                          NaN   
3                                   NaN                          NaN   
4                                   NaN                          NaN   

   excess\PYGZus{}mortality  excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}per\PYGZus{}million  
0               NaN                                      NaN  
1               NaN                                      NaN  
2               NaN                                      NaN  
3               NaN                                      NaN  
4               NaN                                      NaN  

[5 rows x 65 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
As mentioned a lot of information is present here, about 65 columns. yet for this exercise my main objective is the ‘date’ column.
If we would print out the data types using the info method, we can see that the date is recognized as an ‘object’ stating that it is an ordinary string, not a datetime.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 121744 entries, 0 to 121743
Data columns (total 65 columns):
 \PYGZsh{}   Column                                   Non\PYGZhy{}Null Count   Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}                                   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   iso\PYGZus{}code                                 121744 non\PYGZhy{}null  object 
 1   continent                                116202 non\PYGZhy{}null  object 
 2   location                                 121744 non\PYGZhy{}null  object 
 3   date                                     121744 non\PYGZhy{}null  object 
 4   total\PYGZus{}cases                              115518 non\PYGZhy{}null  float64
 5   new\PYGZus{}cases                                115515 non\PYGZhy{}null  float64
 6   new\PYGZus{}cases\PYGZus{}smoothed                       114500 non\PYGZhy{}null  float64
 7   total\PYGZus{}deaths                             104708 non\PYGZhy{}null  float64
 8   new\PYGZus{}deaths                               104863 non\PYGZhy{}null  float64
 9   new\PYGZus{}deaths\PYGZus{}smoothed                      114500 non\PYGZhy{}null  float64
 10  total\PYGZus{}cases\PYGZus{}per\PYGZus{}million                  114910 non\PYGZhy{}null  float64
 11  new\PYGZus{}cases\PYGZus{}per\PYGZus{}million                    114907 non\PYGZhy{}null  float64
 12  new\PYGZus{}cases\PYGZus{}smoothed\PYGZus{}per\PYGZus{}million           113897 non\PYGZhy{}null  float64
 13  total\PYGZus{}deaths\PYGZus{}per\PYGZus{}million                 104113 non\PYGZhy{}null  float64
 14  new\PYGZus{}deaths\PYGZus{}per\PYGZus{}million                   104268 non\PYGZhy{}null  float64
 15  new\PYGZus{}deaths\PYGZus{}smoothed\PYGZus{}per\PYGZus{}million          113897 non\PYGZhy{}null  float64
 16  reproduction\PYGZus{}rate                        98318 non\PYGZhy{}null   float64
 17  icu\PYGZus{}patients                             14443 non\PYGZhy{}null   float64
 18  icu\PYGZus{}patients\PYGZus{}per\PYGZus{}million                 14443 non\PYGZhy{}null   float64
 19  hosp\PYGZus{}patients                            16504 non\PYGZhy{}null   float64
 20  hosp\PYGZus{}patients\PYGZus{}per\PYGZus{}million                16504 non\PYGZhy{}null   float64
 21  weekly\PYGZus{}icu\PYGZus{}admissions                    1268 non\PYGZhy{}null    float64
 22  weekly\PYGZus{}icu\PYGZus{}admissions\PYGZus{}per\PYGZus{}million        1268 non\PYGZhy{}null    float64
 23  weekly\PYGZus{}hosp\PYGZus{}admissions                   2088 non\PYGZhy{}null    float64
 24  weekly\PYGZus{}hosp\PYGZus{}admissions\PYGZus{}per\PYGZus{}million       2088 non\PYGZhy{}null    float64
 25  new\PYGZus{}tests                                52248 non\PYGZhy{}null   float64
 26  total\PYGZus{}tests                              52352 non\PYGZhy{}null   float64
 27  total\PYGZus{}tests\PYGZus{}per\PYGZus{}thousand                 52352 non\PYGZhy{}null   float64
 28  new\PYGZus{}tests\PYGZus{}per\PYGZus{}thousand                   52248 non\PYGZhy{}null   float64
 29  new\PYGZus{}tests\PYGZus{}smoothed                       62816 non\PYGZhy{}null   float64
 30  new\PYGZus{}tests\PYGZus{}smoothed\PYGZus{}per\PYGZus{}thousand          62816 non\PYGZhy{}null   float64
 31  positive\PYGZus{}rate                            58959 non\PYGZhy{}null   float64
 32  tests\PYGZus{}per\PYGZus{}case                           58319 non\PYGZhy{}null   float64
 33  tests\PYGZus{}units                              64746 non\PYGZhy{}null   object 
 34  total\PYGZus{}vaccinations                       28115 non\PYGZhy{}null   float64
 35  people\PYGZus{}vaccinated                        26746 non\PYGZhy{}null   float64
 36  people\PYGZus{}fully\PYGZus{}vaccinated                  23714 non\PYGZhy{}null   float64
 37  total\PYGZus{}boosters                           3057 non\PYGZhy{}null    float64
 38  new\PYGZus{}vaccinations                         23298 non\PYGZhy{}null   float64
 39  new\PYGZus{}vaccinations\PYGZus{}smoothed                50221 non\PYGZhy{}null   float64
 40  total\PYGZus{}vaccinations\PYGZus{}per\PYGZus{}hundred           28115 non\PYGZhy{}null   float64
 41  people\PYGZus{}vaccinated\PYGZus{}per\PYGZus{}hundred            26746 non\PYGZhy{}null   float64
 42  people\PYGZus{}fully\PYGZus{}vaccinated\PYGZus{}per\PYGZus{}hundred      23714 non\PYGZhy{}null   float64
 43  total\PYGZus{}boosters\PYGZus{}per\PYGZus{}hundred               3057 non\PYGZhy{}null    float64
 44  new\PYGZus{}vaccinations\PYGZus{}smoothed\PYGZus{}per\PYGZus{}million    50221 non\PYGZhy{}null   float64
 45  stringency\PYGZus{}index                         101767 non\PYGZhy{}null  float64
 46  population                               120880 non\PYGZhy{}null  float64
 47  population\PYGZus{}density                       112501 non\PYGZhy{}null  float64
 48  median\PYGZus{}age                               107423 non\PYGZhy{}null  float64
 49  aged\PYGZus{}65\PYGZus{}older                            106229 non\PYGZhy{}null  float64
 50  aged\PYGZus{}70\PYGZus{}older                            106834 non\PYGZhy{}null  float64
 51  gdp\PYGZus{}per\PYGZus{}capita                           108055 non\PYGZhy{}null  float64
 52  extreme\PYGZus{}poverty                          72482 non\PYGZhy{}null   float64
 53  cardiovasc\PYGZus{}death\PYGZus{}rate                    107695 non\PYGZhy{}null  float64
 54  diabetes\PYGZus{}prevalence                      111063 non\PYGZhy{}null  float64
 55  female\PYGZus{}smokers                           84078 non\PYGZhy{}null   float64
 56  male\PYGZus{}smokers                             82858 non\PYGZhy{}null   float64
 57  handwashing\PYGZus{}facilities                   54111 non\PYGZhy{}null   float64
 58  hospital\PYGZus{}beds\PYGZus{}per\PYGZus{}thousand               97911 non\PYGZhy{}null   float64
 59  life\PYGZus{}expectancy                          115458 non\PYGZhy{}null  float64
 60  human\PYGZus{}development\PYGZus{}index                  107790 non\PYGZhy{}null  float64
 61  excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}absolute     4317 non\PYGZhy{}null    float64
 62  excess\PYGZus{}mortality\PYGZus{}cumulative              4317 non\PYGZhy{}null    float64
 63  excess\PYGZus{}mortality                         4317 non\PYGZhy{}null    float64
 64  excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}per\PYGZus{}million  4317 non\PYGZhy{}null    float64
dtypes: float64(60), object(5)
memory usage: 60.4+ MB
\end{sphinxVerbatim}

\sphinxAtStartPar
We would like to change that, as we can only perform datetime operations if pandas recognises the datetime format used.
Good for us, pandas has a method to automatically infer the date format, we do that now.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{date} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{date}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 121744 entries, 0 to 121743
Data columns (total 65 columns):
 \PYGZsh{}   Column                                   Non\PYGZhy{}Null Count   Dtype         
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}                                   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}         
 0   iso\PYGZus{}code                                 121744 non\PYGZhy{}null  object        
 1   continent                                116202 non\PYGZhy{}null  object        
 2   location                                 121744 non\PYGZhy{}null  object        
 3   date                                     121744 non\PYGZhy{}null  datetime64[ns]
 4   total\PYGZus{}cases                              115518 non\PYGZhy{}null  float64       
 5   new\PYGZus{}cases                                115515 non\PYGZhy{}null  float64       
 6   new\PYGZus{}cases\PYGZus{}smoothed                       114500 non\PYGZhy{}null  float64       
 7   total\PYGZus{}deaths                             104708 non\PYGZhy{}null  float64       
 8   new\PYGZus{}deaths                               104863 non\PYGZhy{}null  float64       
 9   new\PYGZus{}deaths\PYGZus{}smoothed                      114500 non\PYGZhy{}null  float64       
 10  total\PYGZus{}cases\PYGZus{}per\PYGZus{}million                  114910 non\PYGZhy{}null  float64       
 11  new\PYGZus{}cases\PYGZus{}per\PYGZus{}million                    114907 non\PYGZhy{}null  float64       
 12  new\PYGZus{}cases\PYGZus{}smoothed\PYGZus{}per\PYGZus{}million           113897 non\PYGZhy{}null  float64       
 13  total\PYGZus{}deaths\PYGZus{}per\PYGZus{}million                 104113 non\PYGZhy{}null  float64       
 14  new\PYGZus{}deaths\PYGZus{}per\PYGZus{}million                   104268 non\PYGZhy{}null  float64       
 15  new\PYGZus{}deaths\PYGZus{}smoothed\PYGZus{}per\PYGZus{}million          113897 non\PYGZhy{}null  float64       
 16  reproduction\PYGZus{}rate                        98318 non\PYGZhy{}null   float64       
 17  icu\PYGZus{}patients                             14443 non\PYGZhy{}null   float64       
 18  icu\PYGZus{}patients\PYGZus{}per\PYGZus{}million                 14443 non\PYGZhy{}null   float64       
 19  hosp\PYGZus{}patients                            16504 non\PYGZhy{}null   float64       
 20  hosp\PYGZus{}patients\PYGZus{}per\PYGZus{}million                16504 non\PYGZhy{}null   float64       
 21  weekly\PYGZus{}icu\PYGZus{}admissions                    1268 non\PYGZhy{}null    float64       
 22  weekly\PYGZus{}icu\PYGZus{}admissions\PYGZus{}per\PYGZus{}million        1268 non\PYGZhy{}null    float64       
 23  weekly\PYGZus{}hosp\PYGZus{}admissions                   2088 non\PYGZhy{}null    float64       
 24  weekly\PYGZus{}hosp\PYGZus{}admissions\PYGZus{}per\PYGZus{}million       2088 non\PYGZhy{}null    float64       
 25  new\PYGZus{}tests                                52248 non\PYGZhy{}null   float64       
 26  total\PYGZus{}tests                              52352 non\PYGZhy{}null   float64       
 27  total\PYGZus{}tests\PYGZus{}per\PYGZus{}thousand                 52352 non\PYGZhy{}null   float64       
 28  new\PYGZus{}tests\PYGZus{}per\PYGZus{}thousand                   52248 non\PYGZhy{}null   float64       
 29  new\PYGZus{}tests\PYGZus{}smoothed                       62816 non\PYGZhy{}null   float64       
 30  new\PYGZus{}tests\PYGZus{}smoothed\PYGZus{}per\PYGZus{}thousand          62816 non\PYGZhy{}null   float64       
 31  positive\PYGZus{}rate                            58959 non\PYGZhy{}null   float64       
 32  tests\PYGZus{}per\PYGZus{}case                           58319 non\PYGZhy{}null   float64       
 33  tests\PYGZus{}units                              64746 non\PYGZhy{}null   object        
 34  total\PYGZus{}vaccinations                       28115 non\PYGZhy{}null   float64       
 35  people\PYGZus{}vaccinated                        26746 non\PYGZhy{}null   float64       
 36  people\PYGZus{}fully\PYGZus{}vaccinated                  23714 non\PYGZhy{}null   float64       
 37  total\PYGZus{}boosters                           3057 non\PYGZhy{}null    float64       
 38  new\PYGZus{}vaccinations                         23298 non\PYGZhy{}null   float64       
 39  new\PYGZus{}vaccinations\PYGZus{}smoothed                50221 non\PYGZhy{}null   float64       
 40  total\PYGZus{}vaccinations\PYGZus{}per\PYGZus{}hundred           28115 non\PYGZhy{}null   float64       
 41  people\PYGZus{}vaccinated\PYGZus{}per\PYGZus{}hundred            26746 non\PYGZhy{}null   float64       
 42  people\PYGZus{}fully\PYGZus{}vaccinated\PYGZus{}per\PYGZus{}hundred      23714 non\PYGZhy{}null   float64       
 43  total\PYGZus{}boosters\PYGZus{}per\PYGZus{}hundred               3057 non\PYGZhy{}null    float64       
 44  new\PYGZus{}vaccinations\PYGZus{}smoothed\PYGZus{}per\PYGZus{}million    50221 non\PYGZhy{}null   float64       
 45  stringency\PYGZus{}index                         101767 non\PYGZhy{}null  float64       
 46  population                               120880 non\PYGZhy{}null  float64       
 47  population\PYGZus{}density                       112501 non\PYGZhy{}null  float64       
 48  median\PYGZus{}age                               107423 non\PYGZhy{}null  float64       
 49  aged\PYGZus{}65\PYGZus{}older                            106229 non\PYGZhy{}null  float64       
 50  aged\PYGZus{}70\PYGZus{}older                            106834 non\PYGZhy{}null  float64       
 51  gdp\PYGZus{}per\PYGZus{}capita                           108055 non\PYGZhy{}null  float64       
 52  extreme\PYGZus{}poverty                          72482 non\PYGZhy{}null   float64       
 53  cardiovasc\PYGZus{}death\PYGZus{}rate                    107695 non\PYGZhy{}null  float64       
 54  diabetes\PYGZus{}prevalence                      111063 non\PYGZhy{}null  float64       
 55  female\PYGZus{}smokers                           84078 non\PYGZhy{}null   float64       
 56  male\PYGZus{}smokers                             82858 non\PYGZhy{}null   float64       
 57  handwashing\PYGZus{}facilities                   54111 non\PYGZhy{}null   float64       
 58  hospital\PYGZus{}beds\PYGZus{}per\PYGZus{}thousand               97911 non\PYGZhy{}null   float64       
 59  life\PYGZus{}expectancy                          115458 non\PYGZhy{}null  float64       
 60  human\PYGZus{}development\PYGZus{}index                  107790 non\PYGZhy{}null  float64       
 61  excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}absolute     4317 non\PYGZhy{}null    float64       
 62  excess\PYGZus{}mortality\PYGZus{}cumulative              4317 non\PYGZhy{}null    float64       
 63  excess\PYGZus{}mortality                         4317 non\PYGZhy{}null    float64       
 64  excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}per\PYGZus{}million  4317 non\PYGZhy{}null    float64       
dtypes: datetime64[ns](1), float64(60), object(4)
memory usage: 60.4+ MB
\end{sphinxVerbatim}

\sphinxAtStartPar
now we are ready to perform datetime operations, however we can see that dates are appearing multiple times, this because we have records for multiple countries.
I live in Belgium, so decided to isolate that subsection of the data.
If they had used a data lake and partitioned into countries, reading out the data would have been much more efficient, but efficiency is not something I would expect from government as a Belgian.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df} \PYG{o}{=} \PYG{n}{covid\PYGZus{}df}\PYG{p}{[}\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{location}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Belgium}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           iso\PYGZus{}code continent location  total\PYGZus{}cases  new\PYGZus{}cases  \PYGZbs{}
date                                                             
2020\PYGZhy{}02\PYGZhy{}04      BEL    Europe  Belgium          1.0        1.0   
2020\PYGZhy{}02\PYGZhy{}05      BEL    Europe  Belgium          1.0        0.0   
2020\PYGZhy{}02\PYGZhy{}06      BEL    Europe  Belgium          1.0        0.0   
2020\PYGZhy{}02\PYGZhy{}07      BEL    Europe  Belgium          1.0        0.0   
2020\PYGZhy{}02\PYGZhy{}08      BEL    Europe  Belgium          1.0        0.0   

            new\PYGZus{}cases\PYGZus{}smoothed  total\PYGZus{}deaths  new\PYGZus{}deaths  new\PYGZus{}deaths\PYGZus{}smoothed  \PYGZbs{}
date                                                                            
2020\PYGZhy{}02\PYGZhy{}04                 NaN           NaN         NaN                  NaN   
2020\PYGZhy{}02\PYGZhy{}05                 NaN           NaN         NaN                  NaN   
2020\PYGZhy{}02\PYGZhy{}06                 NaN           NaN         NaN                  NaN   
2020\PYGZhy{}02\PYGZhy{}07                 NaN           NaN         NaN                  NaN   
2020\PYGZhy{}02\PYGZhy{}08                 NaN           NaN         NaN                  NaN   

            total\PYGZus{}cases\PYGZus{}per\PYGZus{}million  ...  female\PYGZus{}smokers  male\PYGZus{}smokers  \PYGZbs{}
date                                 ...                                 
2020\PYGZhy{}02\PYGZhy{}04                    0.086  ...            25.1          31.4   
2020\PYGZhy{}02\PYGZhy{}05                    0.086  ...            25.1          31.4   
2020\PYGZhy{}02\PYGZhy{}06                    0.086  ...            25.1          31.4   
2020\PYGZhy{}02\PYGZhy{}07                    0.086  ...            25.1          31.4   
2020\PYGZhy{}02\PYGZhy{}08                    0.086  ...            25.1          31.4   

            handwashing\PYGZus{}facilities  hospital\PYGZus{}beds\PYGZus{}per\PYGZus{}thousand  \PYGZbs{}
date                                                             
2020\PYGZhy{}02\PYGZhy{}04                     NaN                        5.64   
2020\PYGZhy{}02\PYGZhy{}05                     NaN                        5.64   
2020\PYGZhy{}02\PYGZhy{}06                     NaN                        5.64   
2020\PYGZhy{}02\PYGZhy{}07                     NaN                        5.64   
2020\PYGZhy{}02\PYGZhy{}08                     NaN                        5.64   

            life\PYGZus{}expectancy  human\PYGZus{}development\PYGZus{}index  \PYGZbs{}
date                                                   
2020\PYGZhy{}02\PYGZhy{}04            81.63                    0.931   
2020\PYGZhy{}02\PYGZhy{}05            81.63                    0.931   
2020\PYGZhy{}02\PYGZhy{}06            81.63                    0.931   
2020\PYGZhy{}02\PYGZhy{}07            81.63                    0.931   
2020\PYGZhy{}02\PYGZhy{}08            81.63                    0.931   

            excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}absolute  excess\PYGZus{}mortality\PYGZus{}cumulative  \PYGZbs{}
date                                                                            
2020\PYGZhy{}02\PYGZhy{}04                                   NaN                          NaN   
2020\PYGZhy{}02\PYGZhy{}05                                   NaN                          NaN   
2020\PYGZhy{}02\PYGZhy{}06                                   NaN                          NaN   
2020\PYGZhy{}02\PYGZhy{}07                                   NaN                          NaN   
2020\PYGZhy{}02\PYGZhy{}08                                   NaN                          NaN   

            excess\PYGZus{}mortality  excess\PYGZus{}mortality\PYGZus{}cumulative\PYGZus{}per\PYGZus{}million  
date                                                                   
2020\PYGZhy{}02\PYGZhy{}04               NaN                                      NaN  
2020\PYGZhy{}02\PYGZhy{}05               NaN                                      NaN  
2020\PYGZhy{}02\PYGZhy{}06               NaN                                      NaN  
2020\PYGZhy{}02\PYGZhy{}07               NaN                                      NaN  
2020\PYGZhy{}02\PYGZhy{}08               NaN                                      NaN  

[5 rows x 64 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Now that we have our dataset containing only Belgium I would like to emphasize another aspect, for features such as population density we would not expect a ‘head count’ to differ each day, and as we can see this number is steady over the whole line (results may vary for those who execute this in the future).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{population}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
11632334.0    611
Name: population, dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
we only have a single value (in my case 11.6M) that is repeated over the whole dataset, would this look optimal to you? How would you perhaps approach this to improve data management? If you would like to go hands\sphinxhyphen{}on I left you a blank cell to experiment.

\sphinxAtStartPar
Optimalizations aside, we can not do that which we came for! Datetime operations, the first thing that I have in mind is that due to weekends, the cases might fluctuate a lot per day, so it is not optimal to view it on a daily basis.

\sphinxAtStartPar
First we create a simple line plot with the raw daily cases, then we perform a weekly sum to create a more smooth version of the new cases.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{new\PYGZus{}cases}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{daily cases are fluctuating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}daily cases are fluctuating\PYGZsq{}\PYGZcb{}, xlabel=\PYGZsq{}date\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{datetime_operations_14_1}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weekly\PYGZus{}cases\PYGZus{}df} \PYG{o}{=} \PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{new\PYGZus{}cases}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{resample}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{W}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{weekly\PYGZus{}cases\PYGZus{}df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weekly cases are smoother}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}weekly cases are smoother\PYGZsq{}\PYGZcb{}, xlabel=\PYGZsq{}date\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{datetime_operations_15_1}.png}

\sphinxAtStartPar
That looks great! Those who inspected carefully saw that the x\sphinxhyphen{}axis was correclty identified as datetimes and that the y\sphinxhyphen{}axis for weekly sums have a much higher range.

\sphinxAtStartPar
In a next example we would like to have the relative changes from week to week, this can be done using the shift operation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weekly\PYGZus{}cases\PYGZus{}df}\PYG{o}{.}\PYG{n}{shift}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
date
2020\PYGZhy{}02\PYGZhy{}09        NaN
2020\PYGZhy{}02\PYGZhy{}16        1.0
2020\PYGZhy{}02\PYGZhy{}23        0.0
2020\PYGZhy{}03\PYGZhy{}01        0.0
2020\PYGZhy{}03\PYGZhy{}08        1.0
               ...   
2021\PYGZhy{}09\PYGZhy{}12    14099.0
2021\PYGZhy{}09\PYGZhy{}19    13508.0
2021\PYGZhy{}09\PYGZhy{}26    14298.0
2021\PYGZhy{}10\PYGZhy{}03    13909.0
2021\PYGZhy{}10\PYGZhy{}10    13474.0
Freq: W\PYGZhy{}SUN, Name: new\PYGZus{}cases, Length: 88, dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
This method shifted our data by 1 week forwards, this way we can subtract these results from our original data creating a relative increase (this\_week\_cases \sphinxhyphen{} last\_week\_cases).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{weekly\PYGZus{}cases\PYGZus{}df}\PYG{o}{\PYGZhy{}}\PYG{n}{weekly\PYGZus{}cases\PYGZus{}df}\PYG{o}{.}\PYG{n}{shift}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relative increase p week}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}relative increase p week\PYGZsq{}\PYGZcb{}, xlabel=\PYGZsq{}date\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{datetime_operations_19_1}.png}

\sphinxAtStartPar
Another powerfull asset of datetimes is that we can utilize the concepts of days, weeks, months and years.
In Belgium they speak about a phenomenon called ‘the weekend effect’ where a lot of reports are delayed and therefore Sundays have less cases whereas Mondays have more.

\sphinxAtStartPar
Do we see that in our data? let us seperate the Sundays and Mondays and take a mean!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean deaths on Monday}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{dayofweek}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{new\PYGZus{}deaths}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mean deaths on Monday
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
39.02439024390244
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean deaths on Sunday}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{index}\PYG{o}{.}\PYG{n}{dayofweek}\PYG{o}{==}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{new\PYGZus{}deaths}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mean deaths on Sunday
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
36.646341463414636
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems indeed that more people are reported to pass away no a Monday than on a Sunday, it would be optimal to verify this with statistics, but for now we keep it simple.

\sphinxAtStartPar
As a last example I would like to use slicing of our dataset to demonstrate we can also take a subset of our data and handle this, here we took the months of dec2020\sphinxhyphen{}jan2021 for belgium and calculated the total deaths.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}belgium\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2020\PYGZhy{}12\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2021\PYGZhy{}01\PYGZhy{}31}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{new\PYGZus{}deaths}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
4447.0
\end{sphinxVerbatim}

\sphinxAtStartPar
Now let’s compare this to our neighbours, the Netherlands and France, we do exactly the same operations by selecting exaclty the same time window.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}netherlands\PYGZus{}df} \PYG{o}{=} \PYG{n}{covid\PYGZus{}df}\PYG{p}{[}\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{location}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Netherlands}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}netherlands\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2020\PYGZhy{}12\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2021\PYGZhy{}01\PYGZhy{}31}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{new\PYGZus{}deaths}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
4655.0
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covid\PYGZus{}france\PYGZus{}df} \PYG{o}{=} \PYG{n}{covid\PYGZus{}df}\PYG{p}{[}\PYG{n}{covid\PYGZus{}df}\PYG{o}{.}\PYG{n}{location}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{France}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{covid\PYGZus{}france\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2020\PYGZhy{}12\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2021\PYGZhy{}01\PYGZhy{}31}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{new\PYGZus{}deaths}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
23382.0
\end{sphinxVerbatim}

\sphinxAtStartPar
You can see that Belgium has the lowest of total deaths in that time interval, so you could assume we performed the best!
However this approach is a bit simplified as there are not as many Belgians as French and Dutch. Could you perhaps think if an improvement to create a better understanding?


\chapter{Categorical encoding}
\label{\detokenize{c2_data_preparation/categorical_encoding:categorical-encoding}}\label{\detokenize{c2_data_preparation/categorical_encoding::doc}}
\sphinxAtStartPar
Often we deal with categorical data and this kind of data is something computer algorithms are not able to understand.
On the other hand long categorical features might take up unnecessary memory in our dataset, so converting to a categorical feature is optimal.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}


\section{Raw Material Charaterization}
\label{\detokenize{c2_data_preparation/categorical_encoding:raw-material-charaterization}}
\sphinxAtStartPar
In this dataset, we have a few numerical features describing characteristics of our material, next to that we also have an Outcome feature describing the state of our material in a category.

\sphinxAtStartPar
Let’s have a look at the data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/raw\PYGZhy{}material\PYGZhy{}characterization.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  Lot number   Outcome  Size5  Size10  Size15    TGA   DSC   TMA
0       B370  Adequate   13.8     9.2    41.2  787.3  18.0  65.0
1       B880  Adequate   11.2     5.8    27.6  772.2  17.7  68.8
2       B452  Adequate    9.9     5.8    28.3  602.3  18.3  50.7
3       B287  Adequate   10.4     4.0    24.7  677.9  17.7  56.5
4       B576  Adequate   12.3     9.3    22.0  593.5  19.5  52.0
\end{sphinxVerbatim}

\sphinxAtStartPar
So we can see that the outcome is indeed a text field with a human interpretable value.
The different values are:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([\PYGZsq{}Adequate\PYGZsq{}, \PYGZsq{}Poor\PYGZsq{}], dtype=object)
\end{sphinxVerbatim}

\sphinxAtStartPar
Image that we would like to get all records where the Outcome is less than adequate, using strings this is not possible as the computer does not understand relations of Adequate and Poor when they are denoted as text.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{p}{[}\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{o}{\PYGZlt{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Adequate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Empty DataFrame
Columns: [Lot number, Outcome, Size5, Size10, Size15, TGA, DSC, TMA]
Index: []
\end{sphinxVerbatim}

\sphinxAtStartPar
To overcome this we can change the type of the feature from ‘object’ (string) to ‘category’ let us look at the data types of our data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 24 entries, 0 to 23
Data columns (total 8 columns):
 \PYGZsh{}   Column      Non\PYGZhy{}Null Count  Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   Lot number  24 non\PYGZhy{}null     object 
 1   Outcome     24 non\PYGZhy{}null     object 
 2   Size5       24 non\PYGZhy{}null     float64
 3   Size10      24 non\PYGZhy{}null     float64
 4   Size15      24 non\PYGZhy{}null     float64
 5   TGA         24 non\PYGZhy{}null     float64
 6   DSC         24 non\PYGZhy{}null     float64
 7   TMA         24 non\PYGZhy{}null     float64
dtypes: float64(6), object(2)
memory usage: 1.6+ KB
\end{sphinxVerbatim}

\sphinxAtStartPar
Now we can change that of Outcome to category using the astype method

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome} \PYG{o}{=} \PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{category}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 24 entries, 0 to 23
Data columns (total 8 columns):
 \PYGZsh{}   Column      Non\PYGZhy{}Null Count  Dtype   
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}      \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}   
 0   Lot number  24 non\PYGZhy{}null     object  
 1   Outcome     24 non\PYGZhy{}null     category
 2   Size5       24 non\PYGZhy{}null     float64 
 3   Size10      24 non\PYGZhy{}null     float64 
 4   Size15      24 non\PYGZhy{}null     float64 
 5   TGA         24 non\PYGZhy{}null     float64 
 6   DSC         24 non\PYGZhy{}null     float64 
 7   TMA         24 non\PYGZhy{}null     float64 
dtypes: category(1), float64(6), object(1)
memory usage: 1.6+ KB
\end{sphinxVerbatim}

\sphinxAtStartPar
Our feature might be of categorical nature now, however we still have to define it is an ordinal category and has an order.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome} \PYG{o}{=} \PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{o}{.}\PYG{n}{cat}\PYG{o}{.}\PYG{n}{as\PYGZus{}ordered}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cat}\PYG{o}{.}\PYG{n}{reorder\PYGZus{}categories}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Poor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Adequate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
If we retry to effort to only take the records where the Outcome is less than Adequate, we now get an outcome!
Since we only have 2 categories this is a bit unfortunate, but you should get the idea behind it.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{p}{[}\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{o}{\PYGZlt{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Adequate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Lot number Outcome  Size5  Size10  Size15    TGA   DSC   TMA
5        B914    Poor   13.7     7.8    27.0  597.9  18.1  49.8
6        B404    Poor   15.5    10.7    34.3  668.5  19.6  55.7
7        B694    Poor   15.4    10.7    35.9  602.8  19.2  53.6
8        B875    Poor   14.9    11.3    41.0  614.6  18.5  50.0
10       B517    Poor   16.1    11.6    39.2  682.8  17.5  56.4
13       B430    Poor   12.9     9.7    36.3  642.4  19.1  55.0
21       B745    Poor   10.2     5.8    24.7  575.9  18.5  46.2
\end{sphinxVerbatim}

\sphinxAtStartPar
Let’s take this a step further, since computer algorithms still have no idea what the numerical relation is between Adequate and Poor, we could use a Label Encoder for that.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{LabelEncoder}
\end{sphinxVerbatim}

\sphinxAtStartPar
the label encoder is inputted with the Outcome feature and recognises 2 types, it chooses a numerical value for each while fitting.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{le} \PYG{o}{=} \PYG{n}{LabelEncoder}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{le}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
LabelEncoder()
\end{sphinxVerbatim}

\sphinxAtStartPar
After fitting we can use this encoder to transform our dataset!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{outcome\PYGZus{}label}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{le}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{p}{)}
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  Lot number   Outcome  Size5  Size10  Size15    TGA   DSC   TMA  \PYGZbs{}
0       B370  Adequate   13.8     9.2    41.2  787.3  18.0  65.0   
1       B880  Adequate   11.2     5.8    27.6  772.2  17.7  68.8   
2       B452  Adequate    9.9     5.8    28.3  602.3  18.3  50.7   
3       B287  Adequate   10.4     4.0    24.7  677.9  17.7  56.5   
4       B576  Adequate   12.3     9.3    22.0  593.5  19.5  52.0   

   outcome\PYGZus{}label  
0              0  
1              0  
2              0  
3              0  
4              0  
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems something unfortunate has happened, the encoder gave the Adequate an outcome label of 0, which is lower than the label of Poor (1), this might be bad if we would like to give a score as our outcome.

\sphinxAtStartPar
There is in pandas another method of mapping a label to a category albeit less automated, as you would have to know the categories in your feature.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{outcome\PYGZus{}label} \PYG{o}{=} \PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{Outcome}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Poor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Adequate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{raw\PYGZus{}material\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  Lot number   Outcome  Size5  Size10  Size15    TGA   DSC   TMA outcome\PYGZus{}label
0       B370  Adequate   13.8     9.2    41.2  787.3  18.0  65.0             1
1       B880  Adequate   11.2     5.8    27.6  772.2  17.7  68.8             1
2       B452  Adequate    9.9     5.8    28.3  602.3  18.3  50.7             1
3       B287  Adequate   10.4     4.0    24.7  677.9  17.7  56.5             1
4       B576  Adequate   12.3     9.3    22.0  593.5  19.5  52.0             1
\end{sphinxVerbatim}

\sphinxAtStartPar
Yes! This did the trick, now we can use that outcome label to predict an outcome for future samples.


\chapter{Restaurant tips}
\label{\detokenize{c2_data_preparation/categorical_encoding:restaurant-tips}}
\sphinxAtStartPar
Now we are going to look at a dataset of tips, here a restaurant tracked the table bills and tips for a few days in the week whilst also noting the gender, smoking habit and time of day.
This led to a small yet very interesting dataset, let’s have a look!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tips\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/mwaskom/seaborn\PYGZhy{}data/master/tips.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{tips\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     total\PYGZus{}bill   tip     sex smoker   day    time  size
0         16.99  1.01  Female     No   Sun  Dinner     2
1         10.34  1.66    Male     No   Sun  Dinner     3
2         21.01  3.50    Male     No   Sun  Dinner     3
3         23.68  3.31    Male     No   Sun  Dinner     2
4         24.59  3.61  Female     No   Sun  Dinner     4
..          ...   ...     ...    ...   ...     ...   ...
239       29.03  5.92    Male     No   Sat  Dinner     3
240       27.18  2.00  Female    Yes   Sat  Dinner     2
241       22.67  2.00    Male    Yes   Sat  Dinner     2
242       17.82  1.75    Male     No   Sat  Dinner     2
243       18.78  3.00  Female     No  Thur  Dinner     2

[244 rows x 7 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
We can see here that we have a lot of categorical variables: gender, smoker, the day and the time.
In later sections we will see how we can aggregate on these categorical variables.
Now however we would like to process them for a machine learning exercise, where we need numbers not text.
For the features smoker and day, you could argue there is a clear numbering between them, smoking is 1 if the person was smoking and e.g. Sun relates to 7 as it is the seventh day of the week.

\sphinxAtStartPar
But for the gender this is different, we can’t really say that women are 1 and Men are 0 or vice versa (although in this binary case it might work).
The same theory applies for time, if we would say that breakfast, lunch and dinner equal to 0, 1 and 2 this would give our algorithm a bad impression as it would think dinner is twice lunch…

\sphinxAtStartPar
We use One Hot Encoding for this, the idea is that for each value of the feature we create a new column.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{OneHotEncoder}
\end{sphinxVerbatim}

\sphinxAtStartPar
First we create our encoder, then we give it the day column to learn and see which values of categories there are.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ohe} \PYG{o}{=} \PYG{n}{OneHotEncoder}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{ohe}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{tips\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
OneHotEncoder()
\end{sphinxVerbatim}

\sphinxAtStartPar
Now we can perform an encoding, here we insert the day column and it returns a matrix with 4 columns corresponding to the 4 days in our feature.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ohe}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{tips\PYGZus{}df}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{todense}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
matrix([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 0., 0., 1.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 0., 1.]])
\end{sphinxVerbatim}

\sphinxAtStartPar
As this is a rather mathematical approach for this simple problem I prefer to use the pandas approach for this, which is the get\_dummies method.
The outcome is much more pleasing yet completely the same.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{get\PYGZus{}dummies}\PYG{p}{(}\PYG{n}{tips\PYGZus{}df}\PYG{o}{.}\PYG{n}{day}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     Fri  Sat  Sun  Thur
0      0    0    1     0
1      0    0    1     0
2      0    0    1     0
3      0    0    1     0
4      0    0    1     0
..   ...  ...  ...   ...
239    0    1    0     0
240    0    1    0     0
241    0    1    0     0
242    0    1    0     0
243    0    0    0     1

[244 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
As an exercise you could create a script that given a specific feature (e.g. day):
\begin{itemize}
\item {} 
\sphinxAtStartPar
extracts that feature

\item {} 
\sphinxAtStartPar
creates dummies

\item {} 
\sphinxAtStartPar
concattenates it to the dataframe

\end{itemize}

\sphinxAtStartPar
Good luck!


\chapter{Scaling and Normalization}
\label{\detokenize{c2_data_preparation/scaling_normalization:scaling-and-normalization}}\label{\detokenize{c2_data_preparation/scaling_normalization::doc}}
\sphinxAtStartPar
In this notebook we are going to look into 2 rather mathematical concepts, Scaling and Normalization.
The idea is that we can scale the values and shape the distribution of feature in our dataset.

\sphinxAtStartPar
As an example we take a dataset containing samples from a low density polyethylene production process, containing several numerical features such as temperatures, Forces, Pressure,…

\sphinxAtStartPar
The idea is that by using Scaling and normalization, the ‘range of motion’ for these sensors is equal and we can compare the fluxtuations not only inbetween records, but also inbetween sensors.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ldpe\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://openmv.net/file/LDPE.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Unnamed: 0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ldpe\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
      Tin   Tmax1   Tout1   Tmax2   Tout2   Tcin1   Tcin2     z1     z2  \PYGZbs{}
0  208.17  296.35  233.81  283.41  239.05  117.14  117.20  0.029  0.581   
1  207.26  298.26  230.88  287.55  242.55  116.39  117.23  0.028  0.574   
2  205.30  296.57  235.38  284.35  245.19  117.33  118.42  0.031  0.578   
3  209.29  294.11  225.61  283.31  242.04  116.15  117.94  0.030  0.581   
4  206.76  295.13  230.26  283.74  244.92  116.75  118.49  0.030  0.579   

      Fi1     Fi2     Fs1     Fs2  Press    Conv     Mn      Mw    LCB    SCB  
0  0.4507  0.4518  666.42  248.95   3021  0.1322  27379  160326  0.781  26.11  
1  0.4765  0.5091  658.61  246.36   3033  0.1365  27043  165044  0.819  26.29  
2  0.4744  0.4505  666.51  244.65   3004  0.1335  27344  165621  0.801  26.13  
3  0.4429  0.4516  667.31  242.28   2980  0.1300  27502  160497  0.778  25.92  
4  0.4394  0.4414  670.83  244.31   2997  0.1316  27518  165713  0.786  26.02  
\end{sphinxVerbatim}

\sphinxAtStartPar
We can see that our features clearly have different ranges, but lets try to visualise these ranges using a density plot

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ldpe\PYGZus{}df}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:ylabel=\PYGZsq{}Density\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{scaling_normalization_4_1}.png}

\sphinxAtStartPar
Ouch, this is clearly not working! Because the ‘Mw’ feature is in the range of 150k\sphinxhyphen{}175k our plot is so dilluted the rest are pinned to 0.
We can use the sklearn library to perform a min max scaling, this scaling will shift the distribution of each feature between 0 and 1, but that can also be adjusted.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{MinMaxScaler}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{scaler} \PYG{o}{=} \PYG{n}{MinMaxScaler}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{scaler}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{ldpe\PYGZus{}df}\PYG{p}{)}
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{scaler}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{ldpe\PYGZus{}df}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{n}{ldpe\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:ylabel=\PYGZsq{}Density\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{scaling_normalization_7_1}.png}

\sphinxAtStartPar
That makes a lot more sense, you can now see all of the distribution at once.
Also there seems to be one (yellow) feature that has some outliers perhaps something weird is going on there…

\sphinxAtStartPar
Taking it a step further we could also alter the distributions by using a standard scaler instead of a min max scaler, redistributing the values mathematically into a normal distribution.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{StandardScaler}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{scaler} \PYG{o}{=} \PYG{n}{StandardScaler}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{scaler}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{ldpe\PYGZus{}df}\PYG{p}{)}
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{scaler}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{ldpe\PYGZus{}df}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{n}{ldpe\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:ylabel=\PYGZsq{}Density\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{scaling_normalization_10_1}.png}

\sphinxAtStartPar
You can see it had some trouble fitting our special feature into the normal distribution but it did work out in the end.
With this we are ready to perform machine learning algorithms on this data, but first why not try and figure out where those outliers are I mentioned earlier?


\chapter{Binning and ranking}
\label{\detokenize{c2_data_preparation/binning_ranking:binning-and-ranking}}\label{\detokenize{c2_data_preparation/binning_ranking::doc}}
\sphinxAtStartPar
When dealing with numerical data the trouble can sometimes be that numbers can have a wide variety.

\sphinxAtStartPar
Here we apply 2 methods to deal with that, binning and ranking.
With binning we change the numerical feature into a categorical/ordinal feature.
Ranking is used when our numerical feature contains a non normal distribution that fails to be normalized.

\sphinxAtStartPar
For this example we use a food consumption dataset, where european countries are listed and the relative percentage of each country is given that consumes the type of food, e.g. a value of 67 means that 67\% of that country eats that type of food.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{set\PYGZus{}option}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{display.max\PYGZus{}columns}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://openmv.net/file/food\PYGZhy{}consumption.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{food\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        Country  Real coffee  Instant coffee  Tea  Sweetener  Biscuits  \PYGZbs{}
0       Germany           90              49   88       19.0      57.0   
1         Italy           82              10   60        2.0      55.0   
2        France           88              42   63        4.0      76.0   
3       Holland           96              62   98       32.0      62.0   
4       Belgium           94              38   48       11.0      74.0   
5    Luxembourg           97              61   86       28.0      79.0   
6       England           27              86   99       22.0      91.0   
7      Portugal           72              26   77        2.0      22.0   
8       Austria           55              31   61       15.0      29.0   
9   Switzerland           73              72   85       25.0      31.0   
10       Sweden           97              13   93       31.0       NaN   
11      Denmark           96              17   92       35.0      66.0   
12       Norway           92              17   83       13.0      62.0   
13      Finland           98              12   84       20.0      64.0   
14        Spain           70              40   40        NaN      62.0   
15      Ireland           30              52   99       11.0      80.0   

    Powder soup  Tin soup  Potatoes  Frozen fish  Frozen veggies  Apples  \PYGZbs{}
0            51        19        21           27              21      81   
1            41         3         2            4               2      67   
2            53        11        23           11               5      87   
3            67        43         7           14              14      83   
4            37        23         9           13              12      76   
5            73        12         7           26              23      85   
6            55        76        17           20              24      76   
7            34         1         5           20               3      22   
8            33         1         5           15              11      49   
9            69        10        17           19              15      79   
10           43        43        39           54              45      56   
11           32        17        11           51              42      81   
12           51         4        17           30              15      61   
13           27        10         8           18              12      50   
14           43         2        14           23               7      59   
15           75        18         2            5               3      57   

    Oranges  Tinned fruit  Jam  Garlic  Butter  Margarine  Olive oil  Yoghurt  \PYGZbs{}
0        75            44   71      22      91         85         74     30.0   
1        71             9   46      80      66         24         94      5.0   
2        84            40   45      88      94         47         36     57.0   
3        89            61   81      15      31         97         13     53.0   
4        76            42   57      29      84         80         83     20.0   
5        94            83   20      91      94         94         84     31.0   
6        68            89   91      11      95         94         57     11.0   
7        51             8   16      89      65         78         92      6.0   
8        42            14   41      51      51         72         28     13.0   
9        70            46   61      64      82         48         61     48.0   
10       78            53   75       9      68         32         48      2.0   
11       72            50   64      11      92         91         30     11.0   
12       72            34   51      11      63         94         28      2.0   
13       57            22   37      15      96         94         17      NaN   
14       77            30   38      86      44         51         91     16.0   
15       52            46   89       5      97         25         31      3.0   

    Crisp bread  
0            26  
1            18  
2             3  
3            15  
4             5  
5            24  
6            28  
7             9  
8            11  
9            30  
10           93  
11           34  
12           62  
13           64  
14           13  
15            9  
\end{sphinxVerbatim}

\sphinxAtStartPar
Here you could do some data validity, where we check if all values are between 0 and 100, or we check for missing values.
I will leave that up to you


\section{Binning}
\label{\detokenize{c2_data_preparation/binning_ranking:binning}}
\sphinxAtStartPar
the first thing we want to do is seperate the countries based on their coffee consumption, instead of creating arbitrary values we can perform a quantitative cut.
This means we create a number of equally sized groups using the qcut function, we give them the labels low, medium and high.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bin\PYGZus{}coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{qcut}\PYG{p}{(}\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{q}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{low}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{medium}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{high}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{food\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        Country  Real coffee  Instant coffee  Tea  Sweetener  Biscuits  \PYGZbs{}
0       Germany           90              49   88       19.0      57.0   
1         Italy           82              10   60        2.0      55.0   
2        France           88              42   63        4.0      76.0   
3       Holland           96              62   98       32.0      62.0   
4       Belgium           94              38   48       11.0      74.0   
5    Luxembourg           97              61   86       28.0      79.0   
6       England           27              86   99       22.0      91.0   
7      Portugal           72              26   77        2.0      22.0   
8       Austria           55              31   61       15.0      29.0   
9   Switzerland           73              72   85       25.0      31.0   
10       Sweden           97              13   93       31.0       NaN   
11      Denmark           96              17   92       35.0      66.0   
12       Norway           92              17   83       13.0      62.0   
13      Finland           98              12   84       20.0      64.0   
14        Spain           70              40   40        NaN      62.0   
15      Ireland           30              52   99       11.0      80.0   

    Powder soup  Tin soup  Potatoes  Frozen fish  Frozen veggies  Apples  \PYGZbs{}
0            51        19        21           27              21      81   
1            41         3         2            4               2      67   
2            53        11        23           11               5      87   
3            67        43         7           14              14      83   
4            37        23         9           13              12      76   
5            73        12         7           26              23      85   
6            55        76        17           20              24      76   
7            34         1         5           20               3      22   
8            33         1         5           15              11      49   
9            69        10        17           19              15      79   
10           43        43        39           54              45      56   
11           32        17        11           51              42      81   
12           51         4        17           30              15      61   
13           27        10         8           18              12      50   
14           43         2        14           23               7      59   
15           75        18         2            5               3      57   

    Oranges  Tinned fruit  Jam  Garlic  Butter  Margarine  Olive oil  Yoghurt  \PYGZbs{}
0        75            44   71      22      91         85         74     30.0   
1        71             9   46      80      66         24         94      5.0   
2        84            40   45      88      94         47         36     57.0   
3        89            61   81      15      31         97         13     53.0   
4        76            42   57      29      84         80         83     20.0   
5        94            83   20      91      94         94         84     31.0   
6        68            89   91      11      95         94         57     11.0   
7        51             8   16      89      65         78         92      6.0   
8        42            14   41      51      51         72         28     13.0   
9        70            46   61      64      82         48         61     48.0   
10       78            53   75       9      68         32         48      2.0   
11       72            50   64      11      92         91         30     11.0   
12       72            34   51      11      63         94         28      2.0   
13       57            22   37      15      96         94         17      NaN   
14       77            30   38      86      44         51         91     16.0   
15       52            46   89       5      97         25         31      3.0   

    Crisp bread bin\PYGZus{}coffee  
0            26     medium  
1            18     medium  
2             3     medium  
3            15       high  
4             5     medium  
5            24       high  
6            28        low  
7             9        low  
8            11        low  
9            30        low  
10           93       high  
11           34       high  
12           62     medium  
13           64       high  
14           13        low  
15            9        low  
\end{sphinxVerbatim}

\sphinxAtStartPar
a new column has appeared at the end of our dataframe, containing the labels of our binning, countries with low coffee consumption are put in the low category and vice versa.
Now we can seperate the countries with low coffee consumption from the rest

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{n}{food\PYGZus{}df}\PYG{o}{.}\PYG{n}{bin\PYGZus{}coffee} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{low}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        Country  Real coffee  Instant coffee  Tea  Sweetener  Biscuits  \PYGZbs{}
6       England           27              86   99       22.0      91.0   
7      Portugal           72              26   77        2.0      22.0   
8       Austria           55              31   61       15.0      29.0   
9   Switzerland           73              72   85       25.0      31.0   
14        Spain           70              40   40        NaN      62.0   
15      Ireland           30              52   99       11.0      80.0   

    Powder soup  Tin soup  Potatoes  Frozen fish  Frozen veggies  Apples  \PYGZbs{}
6            55        76        17           20              24      76   
7            34         1         5           20               3      22   
8            33         1         5           15              11      49   
9            69        10        17           19              15      79   
14           43         2        14           23               7      59   
15           75        18         2            5               3      57   

    Oranges  Tinned fruit  Jam  Garlic  Butter  Margarine  Olive oil  Yoghurt  \PYGZbs{}
6        68            89   91      11      95         94         57     11.0   
7        51             8   16      89      65         78         92      6.0   
8        42            14   41      51      51         72         28     13.0   
9        70            46   61      64      82         48         61     48.0   
14       77            30   38      86      44         51         91     16.0   
15       52            46   89       5      97         25         31      3.0   

    Crisp bread bin\PYGZus{}coffee  
6            28        low  
7             9        low  
8            11        low  
9            30        low  
14           13        low  
15            9        low  
\end{sphinxVerbatim}

\sphinxAtStartPar
You can already see the England/Ireland stereotype here, note that those are the only 2 with really low coffee consumption, the others are only in this low binning because we requested equally spaced bins in our qcut function. using the cut function would result in a different outcome.
Perhaps you could try that out?

\sphinxAtStartPar
I tried to think of some metric to quantify the status of coffee drinkers, since we also have the instant coffee consumption we could create a metric where we subtract the amount of instant coffe drinkers from the amount of real coffee drinkers.
This way we can measure that difference between them, I already went ahead and made equal quantity bins for them with labels low, medium and high ‘quality coffee’.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bin\PYGZus{}qual\PYGZus{}coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{qcut}\PYG{p}{(}\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Instant coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{q}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{low}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{medium}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{high}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{n}{food\PYGZus{}df}\PYG{o}{.}\PYG{n}{bin\PYGZus{}qual\PYGZus{}coffee}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{high}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    Country  Real coffee  Instant coffee  Tea  Sweetener  Biscuits  \PYGZbs{}
1     Italy           82              10   60        2.0      55.0   
10   Sweden           97              13   93       31.0       NaN   
11  Denmark           96              17   92       35.0      66.0   
12   Norway           92              17   83       13.0      62.0   
13  Finland           98              12   84       20.0      64.0   

    Powder soup  Tin soup  Potatoes  Frozen fish  Frozen veggies  Apples  \PYGZbs{}
1            41         3         2            4               2      67   
10           43        43        39           54              45      56   
11           32        17        11           51              42      81   
12           51         4        17           30              15      61   
13           27        10         8           18              12      50   

    Oranges  Tinned fruit  Jam  Garlic  Butter  Margarine  Olive oil  Yoghurt  \PYGZbs{}
1        71             9   46      80      66         24         94      5.0   
10       78            53   75       9      68         32         48      2.0   
11       72            50   64      11      92         91         30     11.0   
12       72            34   51      11      63         94         28      2.0   
13       57            22   37      15      96         94         17      NaN   

    Crisp bread bin\PYGZus{}coffee bin\PYGZus{}qual\PYGZus{}coffee  
1            18     medium            high  
10           93       high            high  
11           34       high            high  
12           62     medium            high  
13           64       high            high  
\end{sphinxVerbatim}

\sphinxAtStartPar
Aha! you can see here which countries prefer the real coffee over the instant version.
It seems the scandinavian countries together with obviously Italy are the true Caffeine connoisseur of Europe.
Another intersting thing we can do now is take the mean for each product for both group high and low and take the difference for high \sphinxhyphen{} low.
We can see the result below

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{n}{food\PYGZus{}df}\PYG{o}{.}\PYG{n}{bin\PYGZus{}qual\PYGZus{}coffee}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{high}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{n}{food\PYGZus{}df}\PYG{o}{.}\PYG{n}{bin\PYGZus{}qual\PYGZus{}coffee}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{low}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/tmp/ipykernel\PYGZus{}16521/3908782487.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with \PYGZsq{}numeric\PYGZus{}only=None\PYGZsq{}) is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.
  food\PYGZus{}df[food\PYGZus{}df.bin\PYGZus{}qual\PYGZus{}coffee==\PYGZsq{}high\PYGZsq{}].mean()\PYGZhy{}food\PYGZus{}df[food\PYGZus{}df.bin\PYGZus{}qual\PYGZus{}coffee==\PYGZsq{}low\PYGZsq{}].mean()
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Real coffee       34.500000
Instant coffee   \PYGZhy{}43.366667
Tea                2.066667
Sweetener         \PYGZhy{}0.800000
Biscuits           2.583333
Powder soup      \PYGZhy{}18.200000
Tin soup          \PYGZhy{}9.600000
Potatoes           5.066667
Frozen fish       15.400000
Frozen veggies    10.866667
Apples            \PYGZhy{}4.166667
Oranges            3.666667
Tinned fruit     \PYGZhy{}14.066667
Jam              \PYGZhy{}12.233333
Garlic           \PYGZhy{}13.466667
Butter            10.333333
Margarine          2.500000
Olive oil         \PYGZhy{}3.433333
Yoghurt          \PYGZhy{}19.000000
Crisp bread       36.533333
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems a preference for quality coffee also pairs with crisp bread, who knew?
Do you think scaling/normalization might be interesting here? why (not)?


\section{Ranking}
\label{\detokenize{c2_data_preparation/binning_ranking:ranking}}
\sphinxAtStartPar
In case normalization fails us and we are for some reason not able to get a normal distribution out of a feature, we can still resort to ranking.
Note that non linear machine learning techniques often use a ranking functionality under the hood, therefore this technique is often not required, yet for educational purposes we are going to use it here anyway.
Let’s see how the distribution for Real coffee consumption looks like.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        Country  Real coffee  Instant coffee  Tea  Sweetener  Biscuits  \PYGZbs{}
6       England           27              86   99       22.0      91.0   
15      Ireland           30              52   99       11.0      80.0   
8       Austria           55              31   61       15.0      29.0   
14        Spain           70              40   40        NaN      62.0   
7      Portugal           72              26   77        2.0      22.0   
9   Switzerland           73              72   85       25.0      31.0   
1         Italy           82              10   60        2.0      55.0   
2        France           88              42   63        4.0      76.0   
0       Germany           90              49   88       19.0      57.0   
12       Norway           92              17   83       13.0      62.0   
4       Belgium           94              38   48       11.0      74.0   
3       Holland           96              62   98       32.0      62.0   
11      Denmark           96              17   92       35.0      66.0   
5    Luxembourg           97              61   86       28.0      79.0   
10       Sweden           97              13   93       31.0       NaN   
13      Finland           98              12   84       20.0      64.0   

    Powder soup  Tin soup  Potatoes  Frozen fish  Frozen veggies  Apples  \PYGZbs{}
6            55        76        17           20              24      76   
15           75        18         2            5               3      57   
8            33         1         5           15              11      49   
14           43         2        14           23               7      59   
7            34         1         5           20               3      22   
9            69        10        17           19              15      79   
1            41         3         2            4               2      67   
2            53        11        23           11               5      87   
0            51        19        21           27              21      81   
12           51         4        17           30              15      61   
4            37        23         9           13              12      76   
3            67        43         7           14              14      83   
11           32        17        11           51              42      81   
5            73        12         7           26              23      85   
10           43        43        39           54              45      56   
13           27        10         8           18              12      50   

    Oranges  Tinned fruit  Jam  Garlic  Butter  Margarine  Olive oil  Yoghurt  \PYGZbs{}
6        68            89   91      11      95         94         57     11.0   
15       52            46   89       5      97         25         31      3.0   
8        42            14   41      51      51         72         28     13.0   
14       77            30   38      86      44         51         91     16.0   
7        51             8   16      89      65         78         92      6.0   
9        70            46   61      64      82         48         61     48.0   
1        71             9   46      80      66         24         94      5.0   
2        84            40   45      88      94         47         36     57.0   
0        75            44   71      22      91         85         74     30.0   
12       72            34   51      11      63         94         28      2.0   
4        76            42   57      29      84         80         83     20.0   
3        89            61   81      15      31         97         13     53.0   
11       72            50   64      11      92         91         30     11.0   
5        94            83   20      91      94         94         84     31.0   
10       78            53   75       9      68         32         48      2.0   
13       57            22   37      15      96         94         17      NaN   

    Crisp bread bin\PYGZus{}coffee bin\PYGZus{}qual\PYGZus{}coffee  
6            28        low             low  
15            9        low             low  
8            11        low             low  
14           13        low             low  
7             9        low          medium  
9            30        low             low  
1            18     medium            high  
2             3     medium          medium  
0            26     medium          medium  
12           62     medium            high  
4             5     medium          medium  
3            15       high             low  
11           34       high            high  
5            24       high          medium  
10           93       high            high  
13           64       high            high  
\end{sphinxVerbatim}

\sphinxAtStartPar
Ah yes, about half of the values are 90 or higher, not really optimal as the range is between 0 and 100!
We can also view this in a visual way using a density plot.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee (raw)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}Real coffee (raw)\PYGZsq{}\PYGZcb{}, ylabel=\PYGZsq{}Density\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{binning_ranking_21_1}.png}

\sphinxAtStartPar
For larger datasets this would be more useful as we cannot see our whole dataset, it is clear we have to do something about this, now imagine we can not use regular normalization techniques.
The rank method now comes in handy!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rank\PYGZus{}coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{rank}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{food\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        Country  Real coffee  Instant coffee  Tea  Sweetener  Biscuits  \PYGZbs{}
0       Germany           90              49   88       19.0      57.0   
1         Italy           82              10   60        2.0      55.0   
2        France           88              42   63        4.0      76.0   
3       Holland           96              62   98       32.0      62.0   
4       Belgium           94              38   48       11.0      74.0   
5    Luxembourg           97              61   86       28.0      79.0   
6       England           27              86   99       22.0      91.0   
7      Portugal           72              26   77        2.0      22.0   
8       Austria           55              31   61       15.0      29.0   
9   Switzerland           73              72   85       25.0      31.0   
10       Sweden           97              13   93       31.0       NaN   
11      Denmark           96              17   92       35.0      66.0   
12       Norway           92              17   83       13.0      62.0   
13      Finland           98              12   84       20.0      64.0   
14        Spain           70              40   40        NaN      62.0   
15      Ireland           30              52   99       11.0      80.0   

    Powder soup  Tin soup  Potatoes  Frozen fish  Frozen veggies  Apples  \PYGZbs{}
0            51        19        21           27              21      81   
1            41         3         2            4               2      67   
2            53        11        23           11               5      87   
3            67        43         7           14              14      83   
4            37        23         9           13              12      76   
5            73        12         7           26              23      85   
6            55        76        17           20              24      76   
7            34         1         5           20               3      22   
8            33         1         5           15              11      49   
9            69        10        17           19              15      79   
10           43        43        39           54              45      56   
11           32        17        11           51              42      81   
12           51         4        17           30              15      61   
13           27        10         8           18              12      50   
14           43         2        14           23               7      59   
15           75        18         2            5               3      57   

    Oranges  Tinned fruit  Jam  Garlic  Butter  Margarine  Olive oil  Yoghurt  \PYGZbs{}
0        75            44   71      22      91         85         74     30.0   
1        71             9   46      80      66         24         94      5.0   
2        84            40   45      88      94         47         36     57.0   
3        89            61   81      15      31         97         13     53.0   
4        76            42   57      29      84         80         83     20.0   
5        94            83   20      91      94         94         84     31.0   
6        68            89   91      11      95         94         57     11.0   
7        51             8   16      89      65         78         92      6.0   
8        42            14   41      51      51         72         28     13.0   
9        70            46   61      64      82         48         61     48.0   
10       78            53   75       9      68         32         48      2.0   
11       72            50   64      11      92         91         30     11.0   
12       72            34   51      11      63         94         28      2.0   
13       57            22   37      15      96         94         17      NaN   
14       77            30   38      86      44         51         91     16.0   
15       52            46   89       5      97         25         31      3.0   

    Crisp bread bin\PYGZus{}coffee bin\PYGZus{}qual\PYGZus{}coffee  rank\PYGZus{}coffee  
0            26     medium          medium          9.0  
1            18     medium            high          7.0  
2             3     medium          medium          8.0  
3            15       high             low         12.5  
4             5     medium          medium         11.0  
5            24       high          medium         14.5  
6            28        low             low          1.0  
7             9        low          medium          5.0  
8            11        low             low          3.0  
9            30        low             low          6.0  
10           93       high            high         14.5  
11           34       high            high         12.5  
12           62     medium            high         10.0  
13           64       high            high         16.0  
14           13        low             low          4.0  
15            9        low             low          2.0  
\end{sphinxVerbatim}

\sphinxAtStartPar
At the end of our data a new column was appended, containing the ranking of each country with the lowest being 1 and the highest equal to the amount of countries.
When we visualise this distribution we get a uniform distribution, not normal but still better than before!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{food\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rank\PYGZus{}coffee}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{kind}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Real coffee (ranked)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}AxesSubplot:title=\PYGZob{}\PYGZsq{}center\PYGZsq{}:\PYGZsq{}Real coffee (ranked)\PYGZsq{}\PYGZcb{}, ylabel=\PYGZsq{}Density\PYGZsq{}\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{binning_ranking_25_1}.png}


\chapter{Some practice}
\label{\detokenize{c2_data_preparation/some_practice:some-practice}}\label{\detokenize{c2_data_preparation/some_practice::doc}}
\sphinxAtStartPar
Now that you have learned techniques in data preparation, why don’t you put them to use in this wonderfully horrifying dataset. Good luck!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{json}

\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kaggle\PYGZus{}dir} \PYG{o}{=} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{expanduser}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZti{}/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{n}{kaggle\PYGZus{}dir}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{mkdir}\PYG{p}{(}\PYG{n}{kaggle\PYGZus{}dir}\PYG{p}{)}

\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{kaggle\PYGZus{}dir}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/kaggle.json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}
        \PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{username}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lorenzf}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{7a44a9e99b27e796177d793a3d85b8cf}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{\PYGZcb{}}
        \PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kaggle}
\PYG{n}{kaggle}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{dataset\PYGZus{}download\PYGZus{}files}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PromptCloudHQ/us\PYGZhy{}jobs\PYGZhy{}on\PYGZhy{}monstercom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{unzip}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ModuleNotFoundError}\PYG{g+gWhitespace}{                       }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}25600}\PYG{o}{/}\PYG{l+m+mf}{39646943.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{k+kn}{import} \PYG{n+nn}{kaggle}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{2} \PYG{n}{kaggle}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{dataset\PYGZus{}download\PYGZus{}files}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PromptCloudHQ/us\PYGZhy{}jobs\PYGZhy{}on\PYGZhy{}monstercom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{unzip}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n+ne}{ModuleNotFoundError}: No module named \PYGZsq{}kaggle\PYGZsq{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/monster\PYGZus{}com\PYGZhy{}job\PYGZus{}sample.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                    country country\PYGZus{}code date\PYGZus{}added has\PYGZus{}expired  \PYGZbs{}
0  United States of America           US        NaN          No   
1  United States of America           US        NaN          No   
2  United States of America           US        NaN          No   
3  United States of America           US        NaN          No   
4  United States of America           US        NaN          No   

          job\PYGZus{}board                                    job\PYGZus{}description  \PYGZbs{}
0  jobs.monster.com  TeamSoft is seeing an IT Support Specialist to...   
1  jobs.monster.com  The Wisconsin State Journal is seeking a flexi...   
2  jobs.monster.com  Report this job About the Job DePuy Synthes Co...   
3  jobs.monster.com  Why Join Altec? If you’re considering a career...   
4  jobs.monster.com  Position ID\PYGZsh{}  76162 \PYGZsh{} Positions  1 State  CT C...   

                                           job\PYGZus{}title             job\PYGZus{}type  \PYGZbs{}
0               IT Support Technician Job in Madison   Full Time Employee   
1            Business Reporter/Editor Job in Madison            Full Time   
2  Johnson \PYGZam{} Johnson Family of Companies Job Appl...  Full Time, Employee   
3                    Engineer \PYGZhy{} Quality Job in Dixon            Full Time   
4       Shift Supervisor \PYGZhy{} Part\PYGZhy{}Time Job in Camphill   Full Time Employee   

                                            location  \PYGZbs{}
0                                  Madison, WI 53702   
1                                  Madison, WI 53708   
2  DePuy Synthes Companies is a member of Johnson...   
3                                          Dixon, CA   
4                                       Camphill, PA   

                      organization  \PYGZbs{}
0                              NaN   
1          Printing and Publishing   
2  Personal and Household Services   
3                 Altec Industries   
4                           Retail   

                                            page\PYGZus{}url salary  \PYGZbs{}
0  http://jobview.monster.com/it\PYGZhy{}support\PYGZhy{}technici...    NaN   
1  http://jobview.monster.com/business\PYGZhy{}reporter\PYGZhy{}e...    NaN   
2  http://jobview.monster.com/senior\PYGZhy{}training\PYGZhy{}lea...    NaN   
3  http://jobview.monster.com/engineer\PYGZhy{}quality\PYGZhy{}jo...    NaN   
4  http://jobview.monster.com/shift\PYGZhy{}supervisor\PYGZhy{}pa...    NaN   

                       sector                           uniq\PYGZus{}id  
0     IT/Software Development  11d599f229a80023d2f40e7c52cd941e  
1                         NaN  e4cbb126dabf22159aff90223243ff2a  
2                         NaN  839106b353877fa3d896ffb9c1fe01c0  
3   Experienced (Non\PYGZhy{}Manager)  58435fcab804439efdcaa7ecca0fd783  
4  Project/Program Management  64d0272dc8496abfd9523a8df63c184c  
\end{sphinxVerbatim}

\sphinxAtStartPar
Need some inspiration? perhaps \sphinxhref{https://www.kaggle.com/ankkur13/perfect-dataset-to-get-the-hands-dirty}{this} might help!


\part{3. Data Preprocessing}


\chapter{Data Preprocessing}
\label{\detokenize{c3_data_preprocessing/introduction:data-preprocessing}}\label{\detokenize{c3_data_preprocessing/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\chapter{Indexing and slicing}
\label{\detokenize{c3_data_preprocessing/indexing_slicing:indexing-and-slicing}}\label{\detokenize{c3_data_preprocessing/indexing_slicing::doc}}
\sphinxAtStartPar
In

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/temperatures/australia/melbourne/1981.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date  Temp
0    1981\PYGZhy{}01\PYGZhy{}01  20.7
1    1981\PYGZhy{}01\PYGZhy{}02  17.9
2    1981\PYGZhy{}01\PYGZhy{}03  18.8
3    1981\PYGZhy{}01\PYGZhy{}04  14.6
4    1981\PYGZhy{}01\PYGZhy{}05  15.8
..          ...   ...
360  1981\PYGZhy{}12\PYGZhy{}27  15.5
361  1981\PYGZhy{}12\PYGZhy{}28  13.3
362  1981\PYGZhy{}12\PYGZhy{}29  15.6
363  1981\PYGZhy{}12\PYGZhy{}30  15.2
364  1981\PYGZhy{}12\PYGZhy{}31  17.4

[365 rows x 2 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{Date} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{Date}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df} \PYG{o}{=} \PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1981\PYGZhy{}06\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1981\PYGZhy{}06\PYGZhy{}30}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            Temp
Date            
1981\PYGZhy{}06\PYGZhy{}01  11.6
1981\PYGZhy{}06\PYGZhy{}02  10.6
1981\PYGZhy{}06\PYGZhy{}03   9.8
1981\PYGZhy{}06\PYGZhy{}04  11.2
1981\PYGZhy{}06\PYGZhy{}05   5.7
1981\PYGZhy{}06\PYGZhy{}06   7.1
1981\PYGZhy{}06\PYGZhy{}07   2.5
1981\PYGZhy{}06\PYGZhy{}08   3.5
1981\PYGZhy{}06\PYGZhy{}09   4.6
1981\PYGZhy{}06\PYGZhy{}10  11.0
1981\PYGZhy{}06\PYGZhy{}11   5.7
1981\PYGZhy{}06\PYGZhy{}12   7.7
1981\PYGZhy{}06\PYGZhy{}13  10.4
1981\PYGZhy{}06\PYGZhy{}14  11.4
1981\PYGZhy{}06\PYGZhy{}15   9.2
1981\PYGZhy{}06\PYGZhy{}16   6.1
1981\PYGZhy{}06\PYGZhy{}17   2.7
1981\PYGZhy{}06\PYGZhy{}18   4.3
1981\PYGZhy{}06\PYGZhy{}19   6.3
1981\PYGZhy{}06\PYGZhy{}20   3.8
1981\PYGZhy{}06\PYGZhy{}21   4.4
1981\PYGZhy{}06\PYGZhy{}22   7.1
1981\PYGZhy{}06\PYGZhy{}23   4.8
1981\PYGZhy{}06\PYGZhy{}24   5.8
1981\PYGZhy{}06\PYGZhy{}25   6.2
1981\PYGZhy{}06\PYGZhy{}26   7.3
1981\PYGZhy{}06\PYGZhy{}27   9.2
1981\PYGZhy{}06\PYGZhy{}28  10.2
1981\PYGZhy{}06\PYGZhy{}29   9.5
1981\PYGZhy{}06\PYGZhy{}30   9.5
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1989\PYGZhy{}06\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1989\PYGZhy{}06\PYGZhy{}30}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Temp   NaN
dtype: float64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{resample}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MS}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                 Temp
Date                 
1981\PYGZhy{}01\PYGZhy{}01  17.712903
1981\PYGZhy{}02\PYGZhy{}01  17.678571
1981\PYGZhy{}03\PYGZhy{}01  13.500000
1981\PYGZhy{}04\PYGZhy{}01  12.356667
1981\PYGZhy{}05\PYGZhy{}01   9.490323
1981\PYGZhy{}06\PYGZhy{}01   7.306667
1981\PYGZhy{}07\PYGZhy{}01   7.577419
1981\PYGZhy{}08\PYGZhy{}01   7.238710
1981\PYGZhy{}09\PYGZhy{}01  10.143333
1981\PYGZhy{}10\PYGZhy{}01  10.087097
1981\PYGZhy{}11\PYGZhy{}01  11.890000
1981\PYGZhy{}12\PYGZhy{}01  13.680645
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}df} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tips}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{tip\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   total\PYGZus{}bill   tip     sex smoker  day    time  size
0       16.99  1.01  Female     No  Sun  Dinner     2
1       10.34  1.66    Male     No  Sun  Dinner     3
2       21.01  3.50    Male     No  Sun  Dinner     3
3       23.68  3.31    Male     No  Sun  Dinner     2
4       24.59  3.61  Female     No  Sun  Dinner     4
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}index\PYGZus{}df} \PYG{o}{=} \PYG{n}{tip\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}index\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     total\PYGZus{}bill   tip     sex smoker    time  size
day                                               
Sun       16.99  1.01  Female     No  Dinner     2
Sun       10.34  1.66    Male     No  Dinner     3
Sun       21.01  3.50    Male     No  Dinner     3
Sun       23.68  3.31    Male     No  Dinner     2
Sun       24.59  3.61  Female     No  Dinner     4
..          ...   ...     ...    ...     ...   ...
Sun       20.90  3.50  Female    Yes  Dinner     3
Sun       30.46  2.00    Male    Yes  Dinner     5
Sun       18.15  3.50  Female    Yes  Dinner     3
Sun       23.10  4.00    Male    Yes  Dinner     3
Sun       15.69  1.50    Male    Yes  Dinner     2

[76 rows x 6 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}index\PYGZus{}df} \PYG{o}{=} \PYG{n}{tip\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}index\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Thur}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Lunch}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tip}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/tmp/ipykernel\PYGZus{}25625/2537502835.py:1: PerformanceWarning: indexing past lexsort depth may impact performance.
  tip\PYGZus{}index\PYGZus{}df.loc[(\PYGZsq{}Thur\PYGZsq{},\PYGZsq{}Lunch\PYGZsq{})].tip.mean()
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2.767704918032786
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{pivot\PYGZus{}table}\PYG{p}{(}\PYG{n}{tip\PYGZus{}df}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{total\PYGZus{}bill}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{aggfunc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{median}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
time  Lunch  Dinner
day                
Thur  16.00  18.780
Fri   13.42  18.665
Sat     NaN  18.240
Sun     NaN  19.630
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoker}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Male}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dinner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/tmp/ipykernel\PYGZus{}25625/3467525553.py:1: PerformanceWarning: indexing past lexsort depth may impact performance.
  tip\PYGZus{}df.set\PYGZus{}index([\PYGZsq{}sex\PYGZsq{}, \PYGZsq{}time\PYGZsq{},\PYGZsq{}smoker\PYGZsq{}]).loc[(\PYGZsq{}Male\PYGZsq{}, \PYGZsq{}Dinner\PYGZsq{},\PYGZsq{}Yes\PYGZsq{})][\PYGZsq{}tip\PYGZsq{}].mean()
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3.123191489361702
\end{sphinxVerbatim}


\chapter{Merge}
\label{\detokenize{c3_data_preprocessing/merge:merge}}\label{\detokenize{c3_data_preprocessing/merge::doc}}
\sphinxAtStartPar
\sphinxurl{https://www.kaggle.com/uciml/restaurant-data-with-consumer-ratings}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rating\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/cuisine/rating\PYGZus{}final.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rating\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     userID  placeID  rating  food\PYGZus{}rating  service\PYGZus{}rating
0     U1077   135085       2            2               2
1     U1077   135038       2            2               1
2     U1077   132825       2            2               2
3     U1077   135060       1            2               2
4     U1068   135104       1            1               2
...     ...      ...     ...          ...             ...
1156  U1043   132630       1            1               1
1157  U1011   132715       1            1               0
1158  U1068   132733       1            1               0
1159  U1068   132594       1            1               1
1160  U1068   132660       0            0               0

[1161 rows x 5 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{cuisine\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/cuisine/chefmozcuisine.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{cuisine\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     placeID        Rcuisine
0     135110         Spanish
1     135109         Italian
2     135107  Latin\PYGZus{}American
3     135106         Mexican
4     135105       Fast\PYGZus{}Food
..       ...             ...
911   132005         Seafood
912   132004         Seafood
913   132003   International
914   132002         Seafood
915   132001   Dutch\PYGZhy{}Belgian

[916 rows x 2 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{rating\PYGZus{}df}\PYG{p}{,} \PYG{n}{cuisine\PYGZus{}df}\PYG{p}{,} \PYG{n}{on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{placeID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{inner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{merged\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     userID  placeID  rating  food\PYGZus{}rating  service\PYGZus{}rating   Rcuisine
0     U1077   135085       2            2               2  Fast\PYGZus{}Food
1     U1108   135085       1            2               1  Fast\PYGZus{}Food
2     U1081   135085       1            2               1  Fast\PYGZus{}Food
3     U1056   135085       2            2               2  Fast\PYGZus{}Food
4     U1134   135085       2            1               2  Fast\PYGZus{}Food
...     ...      ...     ...          ...             ...        ...
1038  U1061   132958       2            2               2   American
1039  U1025   132958       1            0               0   American
1040  U1097   132958       2            1               1   American
1041  U1096   132958       1            2               2   American
1042  U1136   132958       2            2               2   American

[1043 rows x 6 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}df}\PYG{p}{[}\PYG{n}{merged\PYGZus{}df}\PYG{o}{.}\PYG{n}{Rcuisine}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Empty DataFrame
Columns: [userID, placeID, rating, food\PYGZus{}rating, service\PYGZus{}rating, Rcuisine]
Index: []
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}df}\PYG{o}{.}\PYG{n}{Rcuisine}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([\PYGZsq{}Fast\PYGZus{}Food\PYGZsq{}, \PYGZsq{}Mexican\PYGZsq{}, \PYGZsq{}Seafood\PYGZsq{}, \PYGZsq{}Pizzeria\PYGZsq{}, \PYGZsq{}Regional\PYGZsq{},
       \PYGZsq{}Armenian\PYGZsq{}, \PYGZsq{}Cafeteria\PYGZsq{}, \PYGZsq{}American\PYGZsq{}, \PYGZsq{}Bar\PYGZsq{}, \PYGZsq{}Bar\PYGZus{}Pub\PYGZus{}Brewery\PYGZsq{},
       \PYGZsq{}Italian\PYGZsq{}, \PYGZsq{}Japanese\PYGZsq{}, \PYGZsq{}Contemporary\PYGZsq{}, \PYGZsq{}Burgers\PYGZsq{}, \PYGZsq{}Mediterranean\PYGZsq{},
       \PYGZsq{}International\PYGZsq{}, \PYGZsq{}Vietnamese\PYGZsq{}, \PYGZsq{}Family\PYGZsq{}, \PYGZsq{}Chinese\PYGZsq{},
       \PYGZsq{}Cafe\PYGZhy{}Coffee\PYGZus{}Shop\PYGZsq{}, \PYGZsq{}Game\PYGZsq{}, \PYGZsq{}Bakery\PYGZsq{}, \PYGZsq{}Breakfast\PYGZhy{}Brunch\PYGZsq{}],
      dtype=object)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}df}\PYG{p}{[}\PYG{n}{merged\PYGZus{}df}\PYG{o}{.}\PYG{n}{Rcuisine}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bar\PYGZus{}Pub\PYGZus{}Brewery}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
rating            1.305085
food\PYGZus{}rating       1.169492
service\PYGZus{}rating    1.203390
dtype: float64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}df}\PYG{p}{[}\PYG{n}{merged\PYGZus{}df}\PYG{o}{.}\PYG{n}{Rcuisine}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Bar}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
rating            1.200000
food\PYGZus{}rating       1.135714
service\PYGZus{}rating    1.085714
dtype: float64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}df}\PYG{p}{[}\PYG{n}{merged\PYGZus{}df}\PYG{o}{.}\PYG{n}{Rcuisine}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cafeteria}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
rating            1.205882
food\PYGZus{}rating       1.127451
service\PYGZus{}rating    1.078431
dtype: float64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}df}\PYG{p}{[}\PYG{n}{merged\PYGZus{}df}\PYG{o}{.}\PYG{n}{Rcuisine}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cafe\PYGZhy{}Coffee\PYGZus{}Shop}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
rating            1.583333
food\PYGZus{}rating       1.333333
service\PYGZus{}rating    1.416667
dtype: float64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{user\PYGZus{}payment\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/cuisine/userpayment.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{payment\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{rating\PYGZus{}df}\PYG{p}{,} \PYG{n}{user\PYGZus{}payment\PYGZus{}df}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{inner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{payment\PYGZus{}df}\PYG{o}{.}\PYG{n}{Upayment}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([\PYGZsq{}VISA\PYGZsq{}, \PYGZsq{}cash\PYGZsq{}, \PYGZsq{}bank\PYGZus{}debit\PYGZus{}cards\PYGZsq{}, \PYGZsq{}MasterCard\PYGZhy{}Eurocard\PYGZsq{},
       \PYGZsq{}American\PYGZus{}Express\PYGZsq{}], dtype=object)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{payment\PYGZus{}df}\PYG{p}{[}\PYG{n}{payment\PYGZus{}df}\PYG{o}{.}\PYG{n}{Upayment}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cash}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
rating            1.182815
food\PYGZus{}rating       1.200183
service\PYGZus{}rating    1.080439
dtype: float64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{payment\PYGZus{}df}\PYG{p}{[}\PYG{n}{payment\PYGZus{}df}\PYG{o}{.}\PYG{n}{Upayment}\PYG{o}{==}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bank\PYGZus{}debit\PYGZus{}cards}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
rating            1.437908
food\PYGZus{}rating       1.562092
service\PYGZus{}rating    1.398693
dtype: float64
\end{sphinxVerbatim}


\chapter{Groupby}
\label{\detokenize{c3_data_preprocessing/groupby:groupby}}\label{\detokenize{c3_data_preprocessing/groupby::doc}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rating\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/cuisine/rating\PYGZus{}final.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{grouped\PYGZus{}rating\PYGZus{}df} \PYG{o}{=} \PYG{n}{rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{placeID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sort\PYGZus{}values}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{grouped\PYGZus{}rating\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           rating  food\PYGZus{}rating  service\PYGZus{}rating
placeID                                       
132654   0.250000         0.25        0.250000
135040   0.250000         0.25        0.250000
132560   0.500000         1.00        0.250000
132663   0.500000         0.50        0.666667
135069   0.500000         0.50        0.750000
...           ...          ...             ...
132755   1.800000         2.00        1.600000
132922   1.833333         1.50        1.833333
134986   2.000000         2.00        2.000000
135034   2.000000         2.00        1.600000
132955   2.000000         1.80        1.800000

[130 rows x 3 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/cuisine/geoplaces2.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{placeID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{grouped\PYGZus{}rating\PYGZus{}df}\PYG{p}{,} \PYG{n}{geo\PYGZus{}df}\PYG{p}{,} \PYG{n}{left\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{right\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           rating  food\PYGZus{}rating  service\PYGZus{}rating   latitude   longitude  \PYGZbs{}
placeID                                                                 
132654   0.250000         0.25        0.250000  23.735523  \PYGZhy{}99.129588   
135040   0.250000         0.25        0.250000  22.135617 \PYGZhy{}100.969709   
132560   0.500000         1.00        0.250000  23.752304  \PYGZhy{}99.166913   
132663   0.500000         0.50        0.666667  23.752511  \PYGZhy{}99.166954   
135069   0.500000         0.50        0.750000  22.140129 \PYGZhy{}100.944872   
...           ...          ...             ...        ...         ...   
132755   1.800000         2.00        1.600000  22.153324 \PYGZhy{}101.019546   
132922   1.833333         1.50        1.833333  22.151135 \PYGZhy{}100.982311   
134986   2.000000         2.00        2.000000  18.928798  \PYGZhy{}99.239513   
135034   2.000000         2.00        1.600000  22.140517 \PYGZhy{}101.021422   
132955   2.000000         1.80        1.800000  22.147622 \PYGZhy{}101.010275   

                                            the\PYGZus{}geom\PYGZus{}meter  \PYGZbs{}
placeID                                                      
132654   0101000020957F000040E8F628488557C18224E8B94845...   
135040   0101000020957F00001B552189B84A58C15A2AAEFD2CA2...   
132560   0101000020957F0000FC60BDA8E88157C1B2C357D6DA4E...   
132663   0101000020957F0000FDF8D26EE08157C1FEDB6A1FDB4E...   
135069   0101000020957F000038E5D546B74A58C18FD29AD0D29A...   
...                                                    ...   
132755   0101000020957F000026CADE45A14658C1F011EBCA55AF...   
132922   0101000020957F000060A98A38FF4758C146718E41D9A4...   
134986   0101000020957F00002A0D05E2D96D5AC1AB058CB1EC56...   
135034   0101000020957F000026D92BB4894858C161A7552DA2B0...   
132955   0101000020957F000068BE7C87C24758C1920A360A08AD...   

                                          name  \PYGZbs{}
placeID                                          
132654   Carnitas Mata  Calle 16 de Septiembre   
135040                Restaurant los Compadres   
132560                      puesto de gorditas   
132663                               tacos abi   
135069               Abondance Restaurante Bar   
...                                        ...   
132755                    La Estrella de Dimas   
132922                    cafe punta del cielo   
134986                Restaurant Las Mananitas   
135034              Michiko Restaurant Japones   
132955                               emilianos   

                                             address             city  \PYGZbs{}
placeID                                                                 
132654                              16 de Septiembre        victoria    
135040                Camino a Simon Diaz 155 Centro  San Luis Potosi   
132560                         frente al tecnologico         victoria   
132663                                             ?         victoria   
135069                   Industrias 908 Valle Dorado  San Luis Potosi   
...                                              ...              ...   
132755                           Av. de los Pintores  San Luis Potosi   
132922                                             ?                ?   
134986                           Ricardo Linares 107       Cuernavaca   
135034   Cordillera de Los Alpes 160 Lomas 2 Seccion  San Luis Potosi   
132955                           venustiano carranza   san luis potos   

              state  ...            alcohol smoking\PYGZus{}area dress\PYGZus{}code  \PYGZbs{}
placeID              ...                                              
132654   tamaulipas  ...  No\PYGZus{}Alcohol\PYGZus{}Served         none   informal   
135040          SLP  ...          Wine\PYGZhy{}Beer         none   informal   
132560   tamaulipas  ...  No\PYGZus{}Alcohol\PYGZus{}Served    permitted   informal   
132663   tamaulipas  ...  No\PYGZus{}Alcohol\PYGZus{}Served         none   informal   
135069          SLP  ...          Wine\PYGZhy{}Beer         none   informal   
...             ...  ...                ...          ...        ...   
132755       S.L.P.  ...  No\PYGZus{}Alcohol\PYGZus{}Served         none   informal   
132922            ?  ...  No\PYGZus{}Alcohol\PYGZus{}Served    permitted     formal   
134986      Morelos  ...          Wine\PYGZhy{}Beer         none     formal   
135034          SLP  ...  No\PYGZus{}Alcohol\PYGZus{}Served         none   informal   
132955       mexico  ...          Wine\PYGZhy{}Beer         none   informal   

            accessibility   price                  url Rambience franchise  \PYGZbs{}
placeID                                                                      
132654         completely     low                    ?  familiar         f   
135040   no\PYGZus{}accessibility    high                    ?  familiar         f   
132560   no\PYGZus{}accessibility     low                    ?  familiar         f   
132663         completely     low                    ?  familiar         f   
135069   no\PYGZus{}accessibility     low                    ?  familiar         f   
...                   ...     ...                  ...       ...       ...   
132755          partially  medium                    ?  familiar         f   
132922         completely  medium                    ?  familiar         f   
134986   no\PYGZus{}accessibility    high  lasmananitas.com.mx  familiar         f   
135034   no\PYGZus{}accessibility  medium                    ?  familiar         f   
132955         completely     low                    ?  familiar         t   

           area other\PYGZus{}services  
placeID                         
132654   closed           none  
135040   closed           none  
132560     open           none  
132663   closed           none  
135069   closed           none  
...         ...            ...  
132755   closed        variety  
132922   closed           none  
134986   closed           none  
135034   closed           none  
132955   closed        variety  

[130 rows x 23 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Index([\PYGZsq{}rating\PYGZsq{}, \PYGZsq{}food\PYGZus{}rating\PYGZsq{}, \PYGZsq{}service\PYGZus{}rating\PYGZsq{}, \PYGZsq{}latitude\PYGZsq{}, \PYGZsq{}longitude\PYGZsq{},
       \PYGZsq{}the\PYGZus{}geom\PYGZus{}meter\PYGZsq{}, \PYGZsq{}name\PYGZsq{}, \PYGZsq{}address\PYGZsq{}, \PYGZsq{}city\PYGZsq{}, \PYGZsq{}state\PYGZsq{}, \PYGZsq{}country\PYGZsq{}, \PYGZsq{}fax\PYGZsq{},
       \PYGZsq{}zip\PYGZsq{}, \PYGZsq{}alcohol\PYGZsq{}, \PYGZsq{}smoking\PYGZus{}area\PYGZsq{}, \PYGZsq{}dress\PYGZus{}code\PYGZsq{}, \PYGZsq{}accessibility\PYGZsq{},
       \PYGZsq{}price\PYGZsq{}, \PYGZsq{}url\PYGZsq{}, \PYGZsq{}Rambience\PYGZsq{}, \PYGZsq{}franchise\PYGZsq{}, \PYGZsq{}area\PYGZsq{}, \PYGZsq{}other\PYGZus{}services\PYGZsq{}],
      dtype=\PYGZsq{}object\PYGZsq{})
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{country}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           rating  food\PYGZus{}rating  service\PYGZus{}rating
country                                       
?        1.166045     1.232946        1.069169
Mexico   1.200977     1.229093        1.118162
mexico   1.062660     1.069006        0.900064
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{can} \PYG{n}{you} \PYG{n}{fix} \PYG{n}{this} \PYG{n}{string} problem\PYG{o}{?}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Object `problem` not found.
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                     rating  food\PYGZus{}rating  service\PYGZus{}rating
alcohol                                                 
Full\PYGZus{}Bar           1.287124     1.218315        1.170311
No\PYGZus{}Alcohol\PYGZus{}Served  1.148075     1.194730        1.042417
Wine\PYGZhy{}Beer          1.231887     1.261840        1.174437
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{accessibility}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                    rating  food\PYGZus{}rating  service\PYGZus{}rating
accessibility                                          
completely        1.132494     1.203597        1.049709
no\PYGZus{}accessibility  1.196189     1.206242        1.091278
partially         1.275356     1.330294        1.219991
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{accessibility}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
no\PYGZus{}accessibility    76
completely          45
partially            9
Name: accessibility, dtype: int64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{food\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{service\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
          rating  food\PYGZus{}rating  service\PYGZus{}rating
price                                        
high    1.258106     1.253816        1.174754
low     1.063059     1.135805        0.935632
medium  1.234342     1.255871        1.161361
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{can} \PYG{n}{you} \PYG{n}{solve} \PYG{n}{the} \PYG{n}{mean}\PYG{o}{\PYGZhy{}}\PYG{n}{mean} problem\PYG{o}{?}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Object `problem` not found.
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o+ow}{in} \PYG{n}{the} \PYG{n}{merge} \PYG{n}{example} \PYG{n}{we} \PYG{n}{added} \PYG{n}{the} \PYG{n}{cuisine} \PYG{n+nb}{type}\PYG{p}{,} \PYG{n}{could} \PYG{n}{you} \PYG{n}{perform} \PYG{n}{a} \PYG{n}{groupby} \PYG{n}{analysis} \PYG{n}{on} this\PYG{o}{?}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Object `this` not found.
\end{sphinxVerbatim}


\chapter{Pivot}
\label{\detokenize{c3_data_preprocessing/pivot:pivot}}\label{\detokenize{c3_data_preprocessing/pivot::doc}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rating\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/cuisine/rating\PYGZus{}final.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rating\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     userID  placeID  rating  food\PYGZus{}rating  service\PYGZus{}rating
0     U1077   135085       2            2               2
1     U1077   135038       2            2               1
2     U1077   132825       2            2               2
3     U1077   135060       1            2               2
4     U1068   135104       1            1               2
...     ...      ...     ...          ...             ...
1156  U1043   132630       1            1               1
1157  U1011   132715       1            1               0
1158  U1068   132733       1            1               0
1159  U1068   132594       1            1               1
1160  U1068   132660       0            0               0

[1161 rows x 5 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/cuisine/geoplaces2.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{placeID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{rating\PYGZus{}df}\PYG{p}{,} \PYG{n}{geo\PYGZus{}df}\PYG{p}{,} \PYG{n}{on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{placeID}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     userID  placeID  rating  food\PYGZus{}rating  service\PYGZus{}rating   latitude  \PYGZbs{}
0     U1077   135085       2            2               2  22.150802   
1     U1108   135085       1            2               1  22.150802   
2     U1081   135085       1            2               1  22.150802   
3     U1056   135085       2            2               2  22.150802   
4     U1134   135085       2            1               2  22.150802   
...     ...      ...     ...          ...             ...        ...   
1156  U1061   132958       2            2               2  22.144979   
1157  U1025   132958       1            0               0  22.144979   
1158  U1097   132958       2            1               1  22.144979   
1159  U1096   132958       1            2               2  22.144979   
1160  U1136   132958       2            2               2  22.144979   

       longitude                                     the\PYGZus{}geom\PYGZus{}meter  \PYGZbs{}
0    \PYGZhy{}100.982680  0101000020957F00009F823DA6094858C18A2D4D37F9A4...   
1    \PYGZhy{}100.982680  0101000020957F00009F823DA6094858C18A2D4D37F9A4...   
2    \PYGZhy{}100.982680  0101000020957F00009F823DA6094858C18A2D4D37F9A4...   
3    \PYGZhy{}100.982680  0101000020957F00009F823DA6094858C18A2D4D37F9A4...   
4    \PYGZhy{}100.982680  0101000020957F00009F823DA6094858C18A2D4D37F9A4...   
...          ...                                                ...   
1156 \PYGZhy{}101.005683  0101000020957F000049095EB34A4858C15CB4BD1EE1AB...   
1157 \PYGZhy{}101.005683  0101000020957F000049095EB34A4858C15CB4BD1EE1AB...   
1158 \PYGZhy{}101.005683  0101000020957F000049095EB34A4858C15CB4BD1EE1AB...   
1159 \PYGZhy{}101.005683  0101000020957F000049095EB34A4858C15CB4BD1EE1AB...   
1160 \PYGZhy{}101.005683  0101000020957F000049095EB34A4858C15CB4BD1EE1AB...   

                        name                         address  ...  \PYGZbs{}
0     Tortas Locas Hipocampo  Venustiano Carranza 719 Centro  ...   
1     Tortas Locas Hipocampo  Venustiano Carranza 719 Centro  ...   
2     Tortas Locas Hipocampo  Venustiano Carranza 719 Centro  ...   
3     Tortas Locas Hipocampo  Venustiano Carranza 719 Centro  ...   
4     Tortas Locas Hipocampo  Venustiano Carranza 719 Centro  ...   
...                      ...                             ...  ...   
1156      tacos los volcanes          avenida hivno nacional  ...   
1157      tacos los volcanes          avenida hivno nacional  ...   
1158      tacos los volcanes          avenida hivno nacional  ...   
1159      tacos los volcanes          avenida hivno nacional  ...   
1160      tacos los volcanes          avenida hivno nacional  ...   

                alcohol   smoking\PYGZus{}area dress\PYGZus{}code     accessibility   price  \PYGZbs{}
0     No\PYGZus{}Alcohol\PYGZus{}Served  not permitted   informal  no\PYGZus{}accessibility  medium   
1     No\PYGZus{}Alcohol\PYGZus{}Served  not permitted   informal  no\PYGZus{}accessibility  medium   
2     No\PYGZus{}Alcohol\PYGZus{}Served  not permitted   informal  no\PYGZus{}accessibility  medium   
3     No\PYGZus{}Alcohol\PYGZus{}Served  not permitted   informal  no\PYGZus{}accessibility  medium   
4     No\PYGZus{}Alcohol\PYGZus{}Served  not permitted   informal  no\PYGZus{}accessibility  medium   
...                 ...            ...        ...               ...     ...   
1156  No\PYGZus{}Alcohol\PYGZus{}Served           none   informal        completely     low   
1157  No\PYGZus{}Alcohol\PYGZus{}Served           none   informal        completely     low   
1158  No\PYGZus{}Alcohol\PYGZus{}Served           none   informal        completely     low   
1159  No\PYGZus{}Alcohol\PYGZus{}Served           none   informal        completely     low   
1160  No\PYGZus{}Alcohol\PYGZus{}Served           none   informal        completely     low   

     url Rambience franchise    area other\PYGZus{}services  
0      ?  familiar         f  closed           none  
1      ?  familiar         f  closed           none  
2      ?  familiar         f  closed           none  
3      ?  familiar         f  closed           none  
4      ?  familiar         f  closed           none  
...   ..       ...       ...     ...            ...  
1156   ?     quiet         t  closed           none  
1157   ?     quiet         t  closed           none  
1158   ?     quiet         t  closed           none  
1159   ?     quiet         t  closed           none  
1160   ?     quiet         t  closed           none  

[1161 rows x 25 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{pivot}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ValueError}\PYG{g+gWhitespace}{                                }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}5491}\PYG{o}{/}\PYG{l+m+mf}{1397663245.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{n}{merged\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{pivot}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/frame.py} in \PYG{n+ni}{pivot}\PYG{n+nt}{(self, index, columns, values)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{7791}         \PYG{k+kn}{from} \PYG{n+nn}{pandas}\PYG{n+nn}{.}\PYG{n+nn}{core}\PYG{n+nn}{.}\PYG{n+nn}{reshape}\PYG{n+nn}{.}\PYG{n+nn}{pivot} \PYG{k+kn}{import} \PYG{n}{pivot}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{7792} 
\PYG{n+ne}{\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{7793}         \PYG{k}{return} \PYG{n}{pivot}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{n}{index}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{n}{columns}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{n}{values}\PYG{p}{)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{7794} 
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{7795}     \PYG{n}{\PYGZus{}shared\PYGZus{}docs}\PYG{p}{[}

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/reshape/pivot.py} in \PYG{n+ni}{pivot}\PYG{n+nt}{(data, index, columns, values)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{515}         \PYG{k}{else}\PYG{p}{:}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{516}             \PYG{n}{indexed} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{\PYGZus{}constructor\PYGZus{}sliced}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{n}{values}\PYG{p}{]}\PYG{o}{.}\PYG{n}{\PYGZus{}values}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{n}{multiindex}\PYG{p}{)}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{517}     \PYG{k}{return} \PYG{n}{indexed}\PYG{o}{.}\PYG{n}{unstack}\PYG{p}{(}\PYG{n}{columns\PYGZus{}listlike}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{518} 
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{519} 

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/series.py} in \PYG{n+ni}{unstack}\PYG{n+nt}{(self, level, fill\PYGZus{}value)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{4079}         \PYG{k+kn}{from} \PYG{n+nn}{pandas}\PYG{n+nn}{.}\PYG{n+nn}{core}\PYG{n+nn}{.}\PYG{n+nn}{reshape}\PYG{n+nn}{.}\PYG{n+nn}{reshape} \PYG{k+kn}{import} \PYG{n}{unstack}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{4080} 
\PYG{n+ne}{\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{4081}         \PYG{k}{return} \PYG{n}{unstack}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{level}\PYG{p}{,} \PYG{n}{fill\PYGZus{}value}\PYG{p}{)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{4082} 
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{4083}     \PYG{c+c1}{\PYGZsh{} \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/reshape/reshape.py} in \PYG{n+ni}{unstack}\PYG{n+nt}{(obj, level, fill\PYGZus{}value)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{458}         \PYG{k}{if} \PYG{n}{is\PYGZus{}1d\PYGZus{}only\PYGZus{}ea\PYGZus{}dtype}\PYG{p}{(}\PYG{n}{obj}\PYG{o}{.}\PYG{n}{dtype}\PYG{p}{)}\PYG{p}{:}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{459}             \PYG{k}{return} \PYG{n}{\PYGZus{}unstack\PYGZus{}extension\PYGZus{}series}\PYG{p}{(}\PYG{n}{obj}\PYG{p}{,} \PYG{n}{level}\PYG{p}{,} \PYG{n}{fill\PYGZus{}value}\PYG{p}{)}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{460}         \PYG{n}{unstacker} \PYG{o}{=} \PYG{n}{\PYGZus{}Unstacker}\PYG{p}{(}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{461}             \PYG{n}{obj}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{level}\PYG{o}{=}\PYG{n}{level}\PYG{p}{,} \PYG{n}{constructor}\PYG{o}{=}\PYG{n}{obj}\PYG{o}{.}\PYG{n}{\PYGZus{}constructor\PYGZus{}expanddim}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{462}         \PYG{p}{)}

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/reshape/reshape.py} in \PYG{n+ni}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{n+nt}{(self, index, level, constructor)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{131}             \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Unstacked DataFrame is too big, causing int32 overflow}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{132} 
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{133}         \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}make\PYGZus{}selectors}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{134} 
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{135}     \PYG{n+nd}{@cache\PYGZus{}readonly}

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/reshape/reshape.py} in \PYG{n+ni}{\PYGZus{}make\PYGZus{}selectors}\PYG{n+nt}{(self)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{183} 
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{184}         \PYG{k}{if} \PYG{n}{mask}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{index}\PYG{p}{)}\PYG{p}{:}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{185}             \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Index contains duplicate entries, cannot reshape}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{186} 
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{187}         \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{group\PYGZus{}index} \PYG{o}{=} \PYG{n}{comp\PYGZus{}index}

\PYG{n+ne}{ValueError}: Index contains duplicate entries, cannot reshape
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{pivot}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
smoking\PYGZus{}area           none  not permitted  only at bar  permitted   section
alcohol                                                                     
Full\PYGZus{}Bar           1.305556       0.857143          NaN   1.500000  1.272727
No\PYGZus{}Alcohol\PYGZus{}Served  1.186788       1.124402          NaN   1.114286  1.265823
Wine\PYGZhy{}Beer          1.217391       1.000000     1.368421   1.300000  1.275000
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{count}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{pivot}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{alcohol}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
smoking\PYGZus{}area        none  not permitted  only at bar  permitted  section
alcohol                                                                 
Full\PYGZus{}Bar            36.0            7.0          NaN        4.0     33.0
No\PYGZus{}Alcohol\PYGZus{}Served  439.0          209.0          NaN       35.0     79.0
Wine\PYGZhy{}Beer          161.0            9.0         19.0       10.0    120.0
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{geo\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Index([\PYGZsq{}latitude\PYGZsq{}, \PYGZsq{}longitude\PYGZsq{}, \PYGZsq{}the\PYGZus{}geom\PYGZus{}meter\PYGZsq{}, \PYGZsq{}name\PYGZsq{}, \PYGZsq{}address\PYGZsq{}, \PYGZsq{}city\PYGZsq{},
       \PYGZsq{}state\PYGZsq{}, \PYGZsq{}country\PYGZsq{}, \PYGZsq{}fax\PYGZsq{}, \PYGZsq{}zip\PYGZsq{}, \PYGZsq{}alcohol\PYGZsq{}, \PYGZsq{}smoking\PYGZus{}area\PYGZsq{},
       \PYGZsq{}dress\PYGZus{}code\PYGZsq{}, \PYGZsq{}accessibility\PYGZsq{}, \PYGZsq{}price\PYGZsq{}, \PYGZsq{}url\PYGZsq{}, \PYGZsq{}Rambience\PYGZsq{}, \PYGZsq{}franchise\PYGZsq{},
       \PYGZsq{}area\PYGZsq{}, \PYGZsq{}other\PYGZus{}services\PYGZsq{}],
      dtype=\PYGZsq{}object\PYGZsq{})
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{merged\PYGZus{}rating\PYGZus{}df}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{city}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{pivot}\PYG{p}{(}\PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{city}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoking\PYGZus{}area}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
smoking\PYGZus{}area          none  not permitted  only at bar  permitted   section
city                                                                       
?                 1.052632       1.000000          NaN   1.833333  1.300000
Cd Victoria       0.625000            NaN          NaN        NaN       NaN
Cd. Victoria           NaN       0.750000          NaN        NaN       NaN
Ciudad Victoria        NaN       1.300000          NaN   0.750000       NaN
Cuernavaca        1.411765       1.157895     1.555556   1.500000  1.000000
Jiutepec          1.461538            NaN          NaN   1.166667       NaN
San Luis Potosi   1.211494       1.168317     1.200000        NaN  1.273743
Soledad           1.058824            NaN          NaN        NaN       NaN
cuernavaca             NaN       1.600000          NaN        NaN       NaN
s.l.p                  NaN       0.916667          NaN        NaN       NaN
s.l.p.            1.281250            NaN          NaN        NaN       NaN
san luis potos    2.000000            NaN          NaN        NaN       NaN
san luis potosi   1.272727       1.000000          NaN        NaN  1.500000
san luis potosi        NaN            NaN          NaN   1.400000       NaN
slp               1.166667            NaN          NaN        NaN       NaN
victoria          1.000000       0.600000          NaN   0.937500       NaN
victoria          0.750000            NaN          NaN        NaN       NaN
\end{sphinxVerbatim}


\part{4. Data Exploration}


\chapter{Data Exploration}
\label{\detokenize{c4_data_exploration/introduction:data-exploration}}\label{\detokenize{c4_data_exploration/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\part{5. Data Visualisation}


\chapter{Data Visualisation}
\label{\detokenize{c5_data_visualisation/introduction:data-visualisation}}\label{\detokenize{c5_data_visualisation/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\part{6. Machine Learning}


\chapter{Machine Learning}
\label{\detokenize{c6_machine_learning/introduction:machine-learning}}\label{\detokenize{c6_machine_learning/introduction::doc}}
\sphinxAtStartPar
this is an introduction







\renewcommand{\indexname}{Index}
\printindex
\end{document}