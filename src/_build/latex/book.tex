%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=0,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{1. Introduction}}

\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Data Science - A practical Approach}
\date{Sep 27, 2021}
\release{}
\author{Lorenz Feyen}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{foreword::doc}}


\sphinxAtStartPar
this is a foreword

\sphinxAtStartPar
pdf version can be found \sphinxhref{https://github.com/LorenzF/data-science-practical-approach/raw/main/src/\_build/latex/book.pdf}{here}.


\part{1. Introduction}


\chapter{Introduction}
\label{\detokenize{c1_introduction/introduction:introduction}}\label{\detokenize{c1_introduction/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\part{2. Data Preparation}


\chapter{Data Preparation}
\label{\detokenize{c2_data_preparation/introduction:data-preparation}}\label{\detokenize{c2_data_preparation/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\chapter{Missing Data}
\label{\detokenize{c2_data_preparation/missing_data:missing-data}}\label{\detokenize{c2_data_preparation/missing_data::doc}}
\sphinxAtStartPar
In this notebook we will look at a few datasets where values from columns are missing.
It is crucial for data science and machine learning to have a dataset where no values are missing as algorithms are usually not able to handle data with information missing.

\sphinxAtStartPar
For python, we will be using the pandas library to handle our dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}


\section{Kamyr digester}
\label{\detokenize{c2_data_preparation/missing_data:kamyr-digester}}
\sphinxAtStartPar
The first dataset we will be looking at is taken from a psysical device equiped with numerous sensors, each timepoint (1 hour) these sensors are read out and the data is collected. Let’s have a look at the general structure

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/kamyr\PYGZhy{}digester.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
  Observation  Y\PYGZhy{}Kappa  ChipRate  BF\PYGZhy{}CMratio  BlowFlow  ChipLevel4   \PYGZbs{}
0    31\PYGZhy{}00:00    23.10    16.520     121.717  1177.607      169.805   
1    31\PYGZhy{}01:00    27.60    16.810      79.022  1328.360      341.327   
2    31\PYGZhy{}02:00    23.19    16.709      79.562  1329.407      239.161   
3    31\PYGZhy{}03:00    23.60    16.478      81.011  1334.877      213.527   
4    31\PYGZhy{}04:00    22.90    15.618      93.244  1334.168      243.131   

   T\PYGZhy{}upperExt\PYGZhy{}2   T\PYGZhy{}lowerExt\PYGZhy{}2    UCZAA  WhiteFlow\PYGZhy{}4   ...  SteamFlow\PYGZhy{}4   \PYGZbs{}
0        358.282         329.545  1.443       599.253  ...        67.122   
1        351.050         329.067  1.549       537.201  ...        60.012   
2        350.022         329.260  1.600       549.611  ...        61.304   
3        350.938         331.142  1.604       623.362  ...        68.496   
4        351.640         332.709    NaN       638.672  ...        70.022   

   Lower\PYGZhy{}HeatT\PYGZhy{}3  Upper\PYGZhy{}HeatT\PYGZhy{}3   ChipMass\PYGZhy{}4   WeakLiquorF   BlackFlow\PYGZhy{}2   \PYGZbs{}
0        329.432         303.099      175.964      1127.197      1319.039   
1        330.823         304.879      163.202       665.975      1297.317   
2        329.140         303.383      164.013       677.534      1327.072   
3        328.875         302.254      181.487       767.853      1324.461   
4        328.352         300.954      183.929       888.448      1343.424   

   WeakWashF   SteamHeatF\PYGZhy{}3   T\PYGZhy{}Top\PYGZhy{}Chips\PYGZhy{}4   SulphidityL\PYGZhy{}4   
0     257.325         54.612         252.077             NaN  
1     241.182         46.603         251.406           29.11  
2     237.272         51.795         251.335             NaN  
3     239.478         54.846         250.312           29.02  
4     215.372         54.186         249.916           29.01  

[5 rows x 23 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Interesting, there seem to be 22 sensor values and 1 timestamp for each record. As mechanical devices are prone to noise and dropouts of sensors we would be foolish to assume no missing values are present.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{divide}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{kamyr\PYGZus{}df}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{100}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Observation         0.00
Y\PYGZhy{}Kappa             0.00
ChipRate            1.33
BF\PYGZhy{}CMratio          4.65
BlowFlow            4.32
ChipLevel4          0.33
T\PYGZhy{}upperExt\PYGZhy{}2        0.33
T\PYGZhy{}lowerExt\PYGZhy{}2        0.33
UCZAA               7.97
WhiteFlow\PYGZhy{}4         0.33
AAWhiteSt\PYGZhy{}4        46.84
AA\PYGZhy{}Wood\PYGZhy{}4           0.33
ChipMoisture\PYGZhy{}4      0.33
SteamFlow\PYGZhy{}4         0.33
Lower\PYGZhy{}HeatT\PYGZhy{}3       0.33
Upper\PYGZhy{}HeatT\PYGZhy{}3       0.33
ChipMass\PYGZhy{}4          0.33
WeakLiquorF         0.33
BlackFlow\PYGZhy{}2         0.33
WeakWashF           0.33
SteamHeatF\PYGZhy{}3        0.33
T\PYGZhy{}Top\PYGZhy{}Chips\PYGZhy{}4       0.33
SulphidityL\PYGZhy{}4      46.84
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
As expected, the datapoint ‘AAWhiteSt\sphinxhyphen{}4’ even has 46\% of data missing!
It seems we only have 300 datapoints and presumably these missing values occur in different records our dataset will be decimated if we just drop all rows with missing values.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(301, 23)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(131, 23)
\end{sphinxVerbatim}

\sphinxAtStartPar
As we drop all rows with missing values, we are left with only 131 records.
Whilst this might be good enough for some purposes, there are more viable options.

\sphinxAtStartPar
Perhaps we can first remove the column with the most missing values and then drop all remaining

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{AAWhiteSt\PYGZhy{}4 }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SulphidityL\PYGZhy{}4 }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{shape}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
(263, 21)
\end{sphinxVerbatim}

\sphinxAtStartPar
Significantly better, although we lost the information of 2 sensors we now have a complete dataset with 263 records. For purposes where those 2 sensors are irrelevant this is a viable option, keep in mind that this dataset is still 100\% truthful, as we have not imputed any values.

\sphinxAtStartPar
Another option, where we retain all our records would be using the timely nature of our dataset, each record is a measurement with an interval of 1 hour. I have no knowledge of this dataset but one might make the assumption that the interval of 1 hour is taken as the state of the machine does not alter much in 1 hour. Therefore we could do what is called a forward fill, where we fill in the missing values with the same value of the sensor for the previous measurement.

\sphinxAtStartPar
This would solve nearly all nan values as there might be a problem where the first value is missing. This is shown below.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kamyr\PYGZus{}df}\PYG{o}{.}\PYG{n}{fillna}\PYG{p}{(}\PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ffill}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SulphidityL\PYGZhy{}4 }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
0        NaN
1      29.11
2      29.11
3      29.02
4      29.01
       ...  
296    30.43
297    30.29
298    30.47
299    30.47
300    30.46
Name: SulphidityL\PYGZhy{}4 , Length: 301, dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
Although our dataset is not fully the truth, we can see that little to no changes occur in the sensor and using a forward fill is arguably the most suitable option.


\section{Travel times}
\label{\detokenize{c2_data_preparation/missing_data:travel-times}}
\sphinxAtStartPar
Another dataset from the same source contains a collection of recorded travel times and specific information about the travel itself as e.g.: the day of the week, where they were going, …

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/travel\PYGZhy{}times.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{travel\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
          Date StartTime  DayOfWeek GoingTo  Distance  MaxSpeed  AvgSpeed  \PYGZbs{}
0     1/6/2012     16:37     Friday    Home     51.29     127.4      78.3   
1     1/6/2012     08:20     Friday     GSK     51.63     130.3      81.8   
2     1/4/2012     16:17  Wednesday    Home     51.27     127.4      82.0   
3     1/4/2012     07:53  Wednesday     GSK     49.17     132.3      74.2   
4     1/3/2012     18:57    Tuesday    Home     51.15     136.2      83.4   
..         ...       ...        ...     ...       ...       ...       ...   
200  7/18/2011     08:09     Monday     GSK     54.52     125.6      49.9   
201  7/14/2011     08:03   Thursday     GSK     50.90     123.7      76.2   
202  7/13/2011     17:08  Wednesday    Home     51.96     132.6      57.5   
203  7/12/2011     17:51    Tuesday    Home     53.28     125.8      61.6   
204  7/11/2011     16:56     Monday    Home     51.73     125.0      62.8   

     AvgMovingSpeed FuelEconomy  TotalTime  MovingTime Take407All Comments  
0              84.8         NaN       39.3        36.3         No      NaN  
1              88.9         NaN       37.9        34.9         No      NaN  
2              85.8         NaN       37.5        35.9         No      NaN  
3              82.9         NaN       39.8        35.6         No      NaN  
4              88.1         NaN       36.8        34.8         No      NaN  
..              ...         ...        ...         ...        ...      ...  
200            82.4        7.89       65.5        39.7         No      NaN  
201            95.1        7.89       40.1        32.1        Yes      NaN  
202            76.7         NaN       54.2        40.6        Yes      NaN  
203            87.6         NaN       51.9        36.5        Yes      NaN  
204            92.5         NaN       49.5        33.6        Yes      NaN  

[205 rows x 13 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
we have a total of 205 records and we can already see that the FuelEconomy column seems pretty bad, let’s quantify that.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{divide}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{travel\PYGZus{}df}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{100}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Date               0.00
StartTime          0.00
DayOfWeek          0.00
GoingTo            0.00
Distance           0.00
MaxSpeed           0.00
AvgSpeed           0.00
AvgMovingSpeed     0.00
FuelEconomy        8.29
TotalTime          0.00
MovingTime         0.00
Take407All         0.00
Comments          88.29
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
In the end, it doesn’t seem that bad, but there are comments and nearly none of them are filled in. Which in perspective is understandable. Let’s see what the comments look like

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{o}{\PYGZti{}}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{Comments}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
15                                  Put snow tires on
39                                         Heavy rain
49                                Huge traffic backup
50      Pumped tires up: check fuel economy improved?
52                                Backed up at Bronte
54                                Backed up at Bronte
60                                              Rainy
78                                   Rain, rain, rain
91                                   Rain, rain, rain
92         Accident: backup from Hamilton to 407 ramp
110                                           Raining
132                           Back to school traffic?
133                Took 407 all the way (to McMaster)
150                             Heavy volume on Derry
156                        Start early to run a batch
158    Accident at 403/highway 6; detour along Dundas
165                                      Detour taken
166                                    Must be Friday
172                             Medium amount of rain
174                                         New tires
182                              Turn around on Derry
184                                       Empty roads
187                            Police slowdown on 403
189                         Accident blocked 407 exit
Name: Comments, dtype: object
\end{sphinxVerbatim}

\sphinxAtStartPar
As you would expect, these comments are text based. Now imagine we would like to run some Natural Language Processing (NLP) on these, it would be a pain to perform string operations on it when it is riddled with missing values.

\sphinxAtStartPar
Here a simple example where we select all records containing the word ‘rain’, with no avail.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{lower}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ValueError}\PYG{g+gWhitespace}{                                }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}25543}\PYG{o}{/}\PYG{l+m+mf}{1298831137.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{lower}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/frame.py} in \PYG{n+ni}{\PYGZus{}\PYGZus{}getitem\PYGZus{}\PYGZus{}}\PYG{n+nt}{(self, key)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{3446} 
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{3447}         \PYG{c+c1}{\PYGZsh{} Do we have a (boolean) 1d indexer?}
\PYG{n+ne}{\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{3448}         \PYG{k}{if} \PYG{n}{com}\PYG{o}{.}\PYG{n}{is\PYGZus{}bool\PYGZus{}indexer}\PYG{p}{(}\PYG{n}{key}\PYG{p}{)}\PYG{p}{:}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{3449}             \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}getitem\PYGZus{}bool\PYGZus{}array}\PYG{p}{(}\PYG{n}{key}\PYG{p}{)}
\PYG{g+gWhitespace}{   }\PYG{l+m+mi}{3450} 

\PYG{n+nn}{\PYGZti{}/git/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/venv/lib/python3.8/site\PYGZhy{}packages/pandas/core/common.py} in \PYG{n+ni}{is\PYGZus{}bool\PYGZus{}indexer}\PYG{n+nt}{(key)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{137}                     \PYG{c+c1}{\PYGZsh{} Don\PYGZsq{}t raise on e.g. [\PYGZdq{}A\PYGZdq{}, \PYGZdq{}B\PYGZdq{}, np.nan], see}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{138}                     \PYG{c+c1}{\PYGZsh{}  test\PYGZus{}loc\PYGZus{}getitem\PYGZus{}list\PYGZus{}of\PYGZus{}labels\PYGZus{}categoricalindex\PYGZus{}with\PYGZus{}na}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{139}                     \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{n}{na\PYGZus{}msg}\PYG{p}{)}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{140}                 \PYG{k}{return} \PYG{k+kc}{False}
\PYG{g+gWhitespace}{    }\PYG{l+m+mi}{141}             \PYG{k}{return} \PYG{k+kc}{True}

\PYG{n+ne}{ValueError}: Cannot mask with non\PYGZhy{}boolean array containing NA / NaN values
\end{sphinxVerbatim}

\sphinxAtStartPar
The last line of the python error traceback gives us the reason it failed, because there were NaN values present.

\sphinxAtStartPar
Luckily the string variable has more or less it’s on ‘null’ value, being an empty string, this way these operations are still possible, most of the comments will just contain nothing.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments} \PYG{o}{=} \PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{fillna}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{Comments}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{lower}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{str}\PYG{o}{.}\PYG{n}{contains}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rain}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date StartTime  DayOfWeek GoingTo  Distance  MaxSpeed  AvgSpeed  \PYGZbs{}
39   11/29/2011     07:23    Tuesday     GSK     51.74     112.2      55.3   
60    11/9/2011     16:15  Wednesday    Home     51.28     121.4      65.9   
78   10/25/2011     17:24    Tuesday    Home     52.87     123.5      65.1   
91   10/12/2011     17:47  Wednesday    Home     51.40     114.4      59.7   
110   9/27/2011     07:36    Tuesday     GSK     50.65     128.1      86.3   
172    8/9/2011     08:15    Tuesday     GSK     49.08     134.8      60.5   

     AvgMovingSpeed FuelEconomy  TotalTime  MovingTime Take407All  \PYGZbs{}
39             61.0         NaN       56.2        50.9         No   
60             71.8        9.35       46.7        42.1         No   
78             72.4        8.97       48.7        43.8         No   
91             65.8        8.75       51.7        46.9         No   
110            88.6        8.31       35.2        34.3        Yes   
172            67.2        8.54       48.7        43.8         No   

                  Comments  
39              Heavy rain  
60                   Rainy  
78        Rain, rain, rain  
91        Rain, rain, rain  
110                Raining  
172  Medium amount of rain  
\end{sphinxVerbatim}

\sphinxAtStartPar
Fixed! now we can use the comments for analysis.

\sphinxAtStartPar
We still have to fix the FuelEconomy, let us take a look at the non NaN values

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{p}{[}\PYG{o}{\PYGZti{}}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{FuelEconomy}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date StartTime  DayOfWeek GoingTo  Distance  MaxSpeed  AvgSpeed  \PYGZbs{}
6      1/2/2012     17:31     Monday    Home     51.37     123.2      82.9   
7      1/2/2012     07:34     Monday     GSK     49.01     128.3      77.5   
8    12/23/2011     08:01     Friday     GSK     52.91     130.3      80.9   
9    12/22/2011     17:19   Thursday    Home     51.17     122.3      70.6   
10   12/22/2011     08:16   Thursday     GSK     49.15     129.4      74.0   
..          ...       ...        ...     ...       ...       ...       ...   
197   7/20/2011     08:24  Wednesday     GSK     48.50     125.8      75.7   
198   7/19/2011     17:17    Tuesday    Home     51.16     126.7      92.2   
199   7/19/2011     08:11    Tuesday     GSK     50.96     124.3      82.3   
200   7/18/2011     08:09     Monday     GSK     54.52     125.6      49.9   
201   7/14/2011     08:03   Thursday     GSK     50.90     123.7      76.2   

     AvgMovingSpeed FuelEconomy  TotalTime  MovingTime Take407All Comments  
6              87.3           \PYGZhy{}       37.2        35.3         No           
7              85.9           \PYGZhy{}       37.9        34.3         No           
8              88.3        8.89       39.3        36.0         No           
9              78.1        8.89       43.5        39.3         No           
10             81.4        8.89       39.8        36.2         No           
..              ...         ...        ...         ...        ...      ...  
197            87.3        7.89       38.5        33.3        Yes           
198           102.6        7.89       33.3        29.9        Yes           
199            96.4        7.89       37.2        31.7        Yes           
200            82.4        7.89       65.5        39.7         No           
201            95.1        7.89       40.1        32.1        Yes           

[188 rows x 13 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems that aside NaN values there are also other intruders, a quick check on the data type (Dtype) reveils it is not recognised as a number!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 205 entries, 0 to 204
Data columns (total 13 columns):
 \PYGZsh{}   Column          Non\PYGZhy{}Null Count  Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}          \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   Date            205 non\PYGZhy{}null    object 
 1   StartTime       205 non\PYGZhy{}null    object 
 2   DayOfWeek       205 non\PYGZhy{}null    object 
 3   GoingTo         205 non\PYGZhy{}null    object 
 4   Distance        205 non\PYGZhy{}null    float64
 5   MaxSpeed        205 non\PYGZhy{}null    float64
 6   AvgSpeed        205 non\PYGZhy{}null    float64
 7   AvgMovingSpeed  205 non\PYGZhy{}null    float64
 8   FuelEconomy     188 non\PYGZhy{}null    object 
 9   TotalTime       205 non\PYGZhy{}null    float64
 10  MovingTime      205 non\PYGZhy{}null    float64
 11  Take407All      205 non\PYGZhy{}null    object 
 12  Comments        205 non\PYGZhy{}null    object 
dtypes: float64(6), object(7)
memory usage: 20.9+ KB
\end{sphinxVerbatim}

\sphinxAtStartPar
The column is noted as an object or string type, meaning that these numbers are given as ‘9.24’ instead of 9.24 and numerical operations are not possible.
We can cast them to numeric but have to warn pandas to coerce errors, meaning errors will be converted to NaN values.
Later we’ll handle the NaN’s.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{FuelEconomy} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}numeric}\PYG{p}{(}\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{FuelEconomy}\PYG{p}{,} \PYG{n}{errors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{coerce}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}class \PYGZsq{}pandas.core.frame.DataFrame\PYGZsq{}\PYGZgt{}
RangeIndex: 205 entries, 0 to 204
Data columns (total 13 columns):
 \PYGZsh{}   Column          Non\PYGZhy{}Null Count  Dtype  
\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}          \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}  
 0   Date            205 non\PYGZhy{}null    object 
 1   StartTime       205 non\PYGZhy{}null    object 
 2   DayOfWeek       205 non\PYGZhy{}null    object 
 3   GoingTo         205 non\PYGZhy{}null    object 
 4   Distance        205 non\PYGZhy{}null    float64
 5   MaxSpeed        205 non\PYGZhy{}null    float64
 6   AvgSpeed        205 non\PYGZhy{}null    float64
 7   AvgMovingSpeed  205 non\PYGZhy{}null    float64
 8   FuelEconomy     186 non\PYGZhy{}null    float64
 9   TotalTime       205 non\PYGZhy{}null    float64
 10  MovingTime      205 non\PYGZhy{}null    float64
 11  Take407All      205 non\PYGZhy{}null    object 
 12  Comments        205 non\PYGZhy{}null    object 
dtypes: float64(7), object(6)
memory usage: 20.9+ KB
\end{sphinxVerbatim}

\sphinxAtStartPar
Wonderful, now the column is numerical and we can see 2 more missing values have popped up!
We could easily drop these 19 records and have a complete dataset.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{travel\PYGZus{}df}\PYG{o}{.}\PYG{n}{dropna}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date StartTime  DayOfWeek GoingTo  Distance  MaxSpeed  AvgSpeed  \PYGZbs{}
8    12/23/2011     08:01     Friday     GSK     52.91     130.3      80.9   
9    12/22/2011     17:19   Thursday    Home     51.17     122.3      70.6   
10   12/22/2011     08:16   Thursday     GSK     49.15     129.4      74.0   
11   12/21/2011     07:45  Wednesday     GSK     51.77     124.8      71.7   
12   12/20/2011     16:05    Tuesday    Home     51.45     130.1      75.2   
..          ...       ...        ...     ...       ...       ...       ...   
197   7/20/2011     08:24  Wednesday     GSK     48.50     125.8      75.7   
198   7/19/2011     17:17    Tuesday    Home     51.16     126.7      92.2   
199   7/19/2011     08:11    Tuesday     GSK     50.96     124.3      82.3   
200   7/18/2011     08:09     Monday     GSK     54.52     125.6      49.9   
201   7/14/2011     08:03   Thursday     GSK     50.90     123.7      76.2   

     AvgMovingSpeed  FuelEconomy  TotalTime  MovingTime Take407All Comments  
8              88.3         8.89       39.3        36.0         No           
9              78.1         8.89       43.5        39.3         No           
10             81.4         8.89       39.8        36.2         No           
11             78.9         8.89       43.3        39.4         No           
12             82.7         8.89       41.1        37.3         No           
..              ...          ...        ...         ...        ...      ...  
197            87.3         7.89       38.5        33.3        Yes           
198           102.6         7.89       33.3        29.9        Yes           
199            96.4         7.89       37.2        31.7        Yes           
200            82.4         7.89       65.5        39.7         No           
201            95.1         7.89       40.1        32.1        Yes           

[186 rows x 13 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
However im leaving them as an excercise for you to apply a technique we will see in the next part


\section{Material properties}
\label{\detokenize{c2_data_preparation/missing_data:material-properties}}
\sphinxAtStartPar
Another dataset from the same source contains the material properties from 30 samples, this time there is not timestamp as the samples are not related in time with each other.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{material\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/raw\PYGZhy{}material\PYGZhy{}properties.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{material\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    Sample  size1  size2  size3  density1  density2  density3
0   X12558  0.696   2.69   6.38      41.8     17.18      3.90
1   X14728  0.636   2.30   5.14      38.1     12.73      3.89
2   X15468  0.841   2.85   5.20      37.6     13.58      3.98
3   X21364  0.609   2.13   4.62      34.2     11.12      4.02
4   X23671  0.684   2.16   4.87      36.4     12.24      3.92
5   X24055  0.762   2.81   6.36      38.1     13.28      3.89
6   X24905  0.552   2.34   5.03      41.3     16.71      3.86
7   X25917  0.501   2.17   5.09       NaN       NaN       NaN
8   X27871  0.619   2.11   5.13       NaN       NaN       NaN
9   X28690  0.610   2.10   4.18      35.0     12.15      3.86
10  X31385  0.532   2.09   4.93       NaN       NaN       NaN
11  X31813  0.738   2.29   5.47       NaN       NaN       NaN
12  X32807  0.779   2.62   5.59       NaN       NaN       NaN
13  X33943  0.537   2.23   5.41      35.2     11.34      3.99
14  X35035  0.702   2.05   5.10      34.2     10.54      4.02
15  X39223  0.768   2.51   5.09      34.9     12.55      3.90
16  X40503  0.714   2.56   6.03      35.6     12.20      4.02
17  X41400  0.621   2.42   5.10      38.7     14.27      3.98
18  X42988  0.726   2.11   4.69      37.1     13.14      3.98
19  X44749  0.698   2.36   5.40      36.6     12.16      4.01
20  X45295    NaN    NaN    NaN      38.1     13.34      3.89
21  X46965  0.759   2.47   4.83      38.7     14.83      3.89
22  X49666  0.535   2.13   5.23       NaN       NaN       NaN
23  X50678  0.716   2.29   5.45      37.3     13.70      3.92
24  X52894  0.635   2.08   4.94       NaN       NaN       NaN
25  X53925  0.598   2.12   4.69      37.9     13.45      3.78
26  X54254  0.700   2.47   5.22      38.8     14.72      3.92
27  X54272  0.957   2.96   7.37      36.2     13.38      4.20
28  X54394  0.759   2.66   5.36      35.2     12.19      3.98
29  X55408  0.661   2.10   4.27       NaN       NaN       NaN
30  X56952  0.646   2.38   4.51      40.1     15.68      3.86
31  X57095  0.662   2.34   4.71      35.0     12.37      3.90
32  X57128  0.749   2.43   5.16      37.3     13.04      3.92
33  X61870  0.598   2.21   4.90       NaN       NaN       NaN
34  X61888  0.619   2.59   5.81       NaN       NaN       NaN
35  X72736  0.693   2.05   5.02      39.6     15.55      3.94
\end{sphinxVerbatim}

\sphinxAtStartPar
let us quantify the amount of missing data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{material\PYGZus{}df}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{divide}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{material\PYGZus{}df}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{100}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Sample       0.00
size1        2.78
size2        2.78
size3        2.78
density1    27.78
density2    27.78
density3    27.78
dtype: float64
\end{sphinxVerbatim}

\sphinxAtStartPar
Unfortunately that is a lot of missing data, covered in all records, dropping here seems almost impossible if we want to keep a healthy amount of records.

\sphinxAtStartPar
Here it would be wise to go for a more elaborate method of imputation, I opted for the K\sphinxhyphen{}nearest neighbours method, which looks at the K most similar records in the dataset to make an educated guess on what the missing value could be, this because we can assume that records with similar data are also similar over all the properties (columns).

\sphinxAtStartPar
Im using the sklearn library for this, which has more imputation techniques such as MICE.
More info can be found \sphinxhref{https://scikit-learn.org/stable/modules/impute.html}{here}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{impute} \PYG{k+kn}{import} \PYG{n}{KNNImputer}
\end{sphinxVerbatim}

\sphinxAtStartPar
im creating an imputer object and specify that i want to use the 5 most similar records and weigh them by distance from the to imputed record, meaning closer neighbours are more important.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{imputer} \PYG{o}{=} \PYG{n}{KNNImputer}\PYG{p}{(}\PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{weights}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{distance}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
As the imputer only takes numerical values I had to do some pandas magic and drop the first column, which I then added again. The result is a fully filled dataset, you can recognise the new values as they are not rounded.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}
    \PYG{n}{imputer}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{material\PYGZus{}df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} 
    \PYG{n}{columns}\PYG{o}{=}\PYG{n}{material\PYGZus{}df}\PYG{o}{.}\PYG{n}{columns}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       size1     size2     size3   density1   density2  density3
0   0.696000  2.690000  6.380000  41.800000  17.180000  3.900000
1   0.636000  2.300000  5.140000  38.100000  12.730000  3.890000
2   0.841000  2.850000  5.200000  37.600000  13.580000  3.980000
3   0.609000  2.130000  4.620000  34.200000  11.120000  4.020000
4   0.684000  2.160000  4.870000  36.400000  12.240000  3.920000
5   0.762000  2.810000  6.360000  38.100000  13.280000  3.890000
6   0.552000  2.340000  5.030000  41.300000  16.710000  3.860000
7   0.501000  2.170000  5.090000  38.495282  14.029399  3.931180
8   0.619000  2.110000  5.130000  37.405275  13.157346  3.943667
9   0.610000  2.100000  4.180000  35.000000  12.150000  3.860000
10  0.532000  2.090000  4.930000  37.811132  13.646072  3.908364
11  0.738000  2.290000  5.470000  37.088833  13.255412  3.941654
12  0.779000  2.620000  5.590000  36.540567  12.889902  3.970973
13  0.537000  2.230000  5.410000  35.200000  11.340000  3.990000
14  0.702000  2.050000  5.100000  34.200000  10.540000  4.020000
15  0.768000  2.510000  5.090000  34.900000  12.550000  3.900000
16  0.714000  2.560000  6.030000  35.600000  12.200000  4.020000
17  0.621000  2.420000  5.100000  38.700000  14.270000  3.980000
18  0.726000  2.110000  4.690000  37.100000  13.140000  3.980000
19  0.698000  2.360000  5.400000  36.600000  12.160000  4.010000
20  0.733097  2.653959  5.881504  38.100000  13.340000  3.890000
21  0.759000  2.470000  4.830000  38.700000  14.830000  3.890000
22  0.535000  2.130000  5.230000  37.391815  13.089536  3.944335
23  0.716000  2.290000  5.450000  37.300000  13.700000  3.920000
24  0.635000  2.080000  4.940000  37.254724  13.206262  3.933904
25  0.598000  2.120000  4.690000  37.900000  13.450000  3.780000
26  0.700000  2.470000  5.220000  38.800000  14.720000  3.920000
27  0.957000  2.960000  7.370000  36.200000  13.380000  4.200000
28  0.759000  2.660000  5.360000  35.200000  12.190000  3.980000
29  0.661000  2.100000  4.270000  36.172345  12.755632  3.887375
30  0.646000  2.380000  4.510000  40.100000  15.680000  3.860000
31  0.662000  2.340000  4.710000  35.000000  12.370000  3.900000
32  0.749000  2.430000  5.160000  37.300000  13.040000  3.920000
33  0.598000  2.210000  4.900000  37.865882  13.826029  3.887021
34  0.619000  2.590000  5.810000  35.932339  12.318210  3.989911
35  0.693000  2.050000  5.020000  39.600000  15.550000  3.940000
\end{sphinxVerbatim}

\sphinxAtStartPar
This concludes the part of missing values, perhaps you can try yourself and impute the missing values for the FuelEconomy using the SimpleImputer or even the IterativeImputer.


\chapter{Concatenation and deduplication}
\label{\detokenize{c2_data_preparation/concatenation_deduplication:concatenation-and-deduplication}}\label{\detokenize{c2_data_preparation/concatenation_deduplication::doc}}
\sphinxAtStartPar
In this notebook we are going to investigate the concepts of stitching data files (concatenation) and verifying the integrity of our data concercing duplicates


\section{Concatenation}
\label{\detokenize{c2_data_preparation/concatenation_deduplication:concatenation}}
\sphinxAtStartPar
When dealing with large amounts of data, fractioning is often the only solution.
Not only does this tidy up your data space, but it also benefits computation.
Aside from that, appending new data to your data lake is independent of the historical data.
However if you want to perform historical analysis this means you will need to perform additional operations.

\sphinxAtStartPar
In this notebook we have a setup of a very small data lake containing daily minimal temperatures.
If you would look closely in the url you would see the following structure.
\begin{quote}

\sphinxAtStartPar
data/temperature/australia/melbourne/1981.csv
\end{quote}

\sphinxAtStartPar
This is a straight\sphinxhyphen{}forward but perfect example on how fragmentation works, in our data lake we have:
temperatures data fractioned by country, city and year. As we are working with daily temperatures further fractioning would not be interesting, but you could fraction e.g. per month.

\sphinxAtStartPar
In the cells below, we read our both 1981 and 1982 data and concatenate them using python.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{melbourne\PYGZus{}1981\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/temperatures/australia/melbourne/1981.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{melbourne\PYGZus{}1982\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/temperatures/australia/melbourne/1982.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}
    \PYG{p}{[}
        \PYG{n}{melbourne\PYGZus{}1981\PYGZus{}df}\PYG{p}{,}
        \PYG{n}{melbourne\PYGZus{}1982\PYGZus{}df}\PYG{p}{,}
    \PYG{p}{]}
\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date  Temp
0    1981\PYGZhy{}01\PYGZhy{}01  20.7
1    1981\PYGZhy{}01\PYGZhy{}02  17.9
2    1981\PYGZhy{}01\PYGZhy{}03  18.8
3    1981\PYGZhy{}01\PYGZhy{}04  14.6
4    1981\PYGZhy{}01\PYGZhy{}05  15.8
..          ...   ...
360  1982\PYGZhy{}12\PYGZhy{}27  15.3
361  1982\PYGZhy{}12\PYGZhy{}28  16.3
362  1982\PYGZhy{}12\PYGZhy{}29  15.8
363  1982\PYGZhy{}12\PYGZhy{}30  17.7
364  1982\PYGZhy{}12\PYGZhy{}31  16.3

[730 rows x 2 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
And there you have it! we now have a dataframe containing both data from 1981 as 1982.
Can you figure out what I calculated in the next cell? Do you think there might be a more ‘clean’ solution?

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{Date}\PYG{o}{.}\PYG{n}{str}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{:}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{Temp}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
17.140322580645158
\end{sphinxVerbatim}

\sphinxAtStartPar
As an exercise I would ask you now to create a small python script that given a begin and end year (between 1981 and 1990) can automatically concatenate all the necessary data

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1982}\PYG{p}{,}\PYG{l+m+mi}{1987}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{i}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
1982
1983
1984
1985
1986
\end{sphinxVerbatim}


\section{Deduplication}
\label{\detokenize{c2_data_preparation/concatenation_deduplication:deduplication}}
\sphinxAtStartPar
Another important aspect of data cleaning is the removal of duplicates.
Here we fragment of a dataset from activity on a popular games platform.
We can see which user has either bought or played specific games and how often.
Unfortunately for some reason, entries might have duplicates which we have to deal with as otherwise users might have e.g. bought a game twice.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/steam.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game    action  freq
0      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
1      11373749                   Sid Meier\PYGZsq{}s Civilization IV      play   0.1
2      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
3      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
4      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
...         ...                                           ...       ...   ...
1834  112845094                                        Arma 2  purchase   1.0
1835  112845094                  Grand Theft Auto San Andreas  purchase   1.0
1836  112845094                    Grand Theft Auto Vice City  purchase   1.0
1837  112845094                    Grand Theft Auto Vice City  purchase   1.0
1838  112845094                          Grand Theft Auto III  purchase   1.0

[1839 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
We have a dataframe with 1839 interactions, you can see that the freq either notes the amount they bought (which always 1 as there is not use in buying it more) or the amount in hours they played.

\sphinxAtStartPar
Let us straightforward ask pandas to remove all rows that have an exact duplicate

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game    action  freq
0      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
1      11373749                   Sid Meier\PYGZsq{}s Civilization IV      play   0.1
3      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
5      11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
7      56038151                       Tom Clancy\PYGZsq{}s H.A.W.X. 2  purchase   1.0
...         ...                                           ...       ...   ...
1831  112845094                  Grand Theft Auto San Andreas  purchase   1.0
1832  112845094                  Grand Theft Auto San Andreas      play   0.2
1833  112845094                          Grand Theft Auto III  purchase   1.0
1834  112845094                                        Arma 2  purchase   1.0
1836  112845094                    Grand Theft Auto Vice City  purchase   1.0

[1132 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Alright! this seemed to have dropped 707 rows from our dataset, but we would like to know more about those.
Let’s ask which rows the algorithm has dropped:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game    action  freq
2      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
4      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
6      11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
10     56038151                  Grand Theft Auto San Andreas  purchase   1.0
12     56038151                    Grand Theft Auto Vice City  purchase   1.0
...         ...                                           ...       ...   ...
1827   39146470          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
1830   48666962                                      Crysis 2  purchase   1.0
1835  112845094                  Grand Theft Auto San Andreas  purchase   1.0
1837  112845094                    Grand Theft Auto Vice City  purchase   1.0
1838  112845094                          Grand Theft Auto III  purchase   1.0

[707 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Here we can see the duplicates, no particular pattern seems to be present, we could just for curiosity count the games that are duplicated

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{game}\PYG{o}{.}\PYG{n}{value\PYGZus{}counts}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Grand Theft Auto San Andreas                    172
Grand Theft Auto Vice City                      103
Sid Meier\PYGZsq{}s Civilization IV                      98
Grand Theft Auto III                             90
Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword     80
Sid Meier\PYGZsq{}s Civilization IV Warlords             79
Sid Meier\PYGZsq{}s Civilization IV Colonization         75
Crysis 2                                          7
Arma 2                                            1
Tom Clancy\PYGZsq{}s H.A.W.X. 2                           1
TERA                                              1
Name: game, dtype: int64
\end{sphinxVerbatim}

\sphinxAtStartPar
It seems there are some games which are very prone to being duplicated, at this point we could go and ask the IT department why these games are acting weird.

\sphinxAtStartPar
Another thing im interested about is the perspective of a single gamer, here we took a single user\_id and printed all his games

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{user\PYGZus{}id} \PYG{o}{==} \PYG{l+m+mi}{11373749}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    user\PYGZus{}id                                          game    action  freq
0  11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
1  11373749                   Sid Meier\PYGZsq{}s Civilization IV      play   0.1
2  11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
3  11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
4  11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
5  11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
6  11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
\end{sphinxVerbatim}

\sphinxAtStartPar
Ah, you can see all of his three games are somehow duplicated in purchase, also it seems he only played one of them for only 0.1 hours.
Looks like he fell to the bait of a tempting summer sale but didn’t realise he had no time to actually play it.

\sphinxAtStartPar
Another thing I would like to mention here is that this dataset would make a fine recommender system as it contains user ids and hours played.
Add game metadata (description) and reviews to the mix and your data preparation is done!

\sphinxAtStartPar
We can remove all duplicates now by overwriting our dataframe

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
One thing still bothers me, as hours played can change over time it might be that different snapshots have produced different values, therefore more duplicates might be present with different hours\_played.

\sphinxAtStartPar
Time to investigate this by using a subset of columns in the drop\_duplicates algorithm

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{drop\PYGZus{}duplicates}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{game}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{action}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game    action  freq
0      11373749                   Sid Meier\PYGZsq{}s Civilization IV  purchase   1.0
1      11373749                   Sid Meier\PYGZsq{}s Civilization IV      play   0.1
3      11373749  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword  purchase   1.0
5      11373749          Sid Meier\PYGZsq{}s Civilization IV Warlords  purchase   1.0
7      56038151                       Tom Clancy\PYGZsq{}s H.A.W.X. 2  purchase   1.0
...         ...                                           ...       ...   ...
1831  112845094                  Grand Theft Auto San Andreas  purchase   1.0
1832  112845094                  Grand Theft Auto San Andreas      play   0.2
1833  112845094                          Grand Theft Auto III  purchase   1.0
1834  112845094                                        Arma 2  purchase   1.0
1836  112845094                    Grand Theft Auto Vice City  purchase   1.0

[1120 rows x 4 columns]
\end{sphinxVerbatim}

\sphinxAtStartPar
Seems we have shaved off another 12 records, so our intuition was right, again lets see which the duplicates are:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{duplicated}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{user\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{game}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{action}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        user\PYGZus{}id                                          game action  freq
118   118664413                  Grand Theft Auto San Andreas   play   0.2
458    50769696                  Grand Theft Auto San Andreas   play   3.1
521    71411882                          Grand Theft Auto III   play   0.2
607    33865373                   Sid Meier\PYGZsq{}s Civilization IV   play   2.0
898    71510748                  Grand Theft Auto San Andreas   play   0.2
908    28472068                    Grand Theft Auto Vice City   play   0.4
910    28472068                  Grand Theft Auto San Andreas   play   0.2
912    28472068                          Grand Theft Auto III   play   0.1
1506   59925638                       Tom Clancy\PYGZsq{}s H.A.W.X. 2   play   0.3
1553  148362155                  Grand Theft Auto San Andreas   play  12.5
1709  176261926  Sid Meier\PYGZsq{}s Civilization IV Beyond the Sword   play   0.4
1711  176261926                   Sid Meier\PYGZsq{}s Civilization IV   play   0.2
\end{sphinxVerbatim}

\sphinxAtStartPar
As expected the duplicates are all in the ‘play’ action, to complete our view we extract the data of a single user

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{p}{[}\PYG{n}{df}\PYG{o}{.}\PYG{n}{user\PYGZus{}id}\PYG{o}{==}\PYG{l+m+mi}{118664413}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
       user\PYGZus{}id                          game    action  freq
115  118664413  Grand Theft Auto San Andreas  purchase   1.0
116  118664413  Grand Theft Auto San Andreas      play   1.9
118  118664413  Grand Theft Auto San Andreas      play   0.2
\end{sphinxVerbatim}

\sphinxAtStartPar
It looks like we have a problem now, we know these are duplicates and should be removed, but which one?
Personally I would argue here that we keep the highest value, as it is impossible to ‘unplay’ hours on the game.
I will leave this as an exercise for you, but the solution is pretty tricky so i’ll give a hint:

\sphinxAtStartPar
The algorithm always keeps the first record in case of duplicates, so you could sort the rows making sure the higher value is always encountered first, good luck!


\chapter{Outliers and validity}
\label{\detokenize{c2_data_preparation/outliers_validity:outliers-and-validity}}\label{\detokenize{c2_data_preparation/outliers_validity::doc}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{wafer\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://openmv.net/file/silicon\PYGZhy{}wafer\PYGZhy{}thickness.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{wafer\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
      G1     G2     G3     G4     G5     G6     G7     G8     G9
0  0.175  0.188 \PYGZhy{}0.159  0.095  0.374 \PYGZhy{}0.238 \PYGZhy{}0.800  0.158 \PYGZhy{}0.211
1  0.102  0.075  0.141  0.180  0.138 \PYGZhy{}0.057 \PYGZhy{}0.075  0.072  0.072
2  0.607  0.711  0.879  0.765  0.592  0.187  0.431  0.345  0.187
3  0.774  0.823  0.619  0.370  0.725  0.439 \PYGZhy{}0.025 \PYGZhy{}0.259  0.496
4  0.504  0.644  0.845  0.681  0.502  0.151  0.404  0.296  0.260
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{iqr} \PYG{o}{=} \PYG{n}{wafer\PYGZus{}df}\PYG{o}{.}\PYG{n}{quantile}\PYG{p}{(}\PYG{l+m+mf}{0.75}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{wafer\PYGZus{}df}\PYG{o}{.}\PYG{n}{quantile}\PYG{p}{(}\PYG{l+m+mf}{0.25}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{range\PYGZus{}df} \PYG{o}{=} \PYG{p}{(}\PYG{n}{wafer\PYGZus{}df}\PYG{o}{\PYGZhy{}}\PYG{n}{wafer\PYGZus{}df}\PYG{o}{.}\PYG{n}{quantile}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{iqr}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{range\PYGZus{}df}\PYG{p}{[}\PYG{p}{(}\PYG{n}{range\PYGZus{}df}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{columns}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            G1         G2         G3         G4         G5        G6  \PYGZbs{}
8     2.232430   2.009016   1.956542   1.589328   1.843890  1.544669   
38   12.891135  12.827049  12.832178  13.913292  11.429506  9.500865   
39    3.691318   3.981148   3.774387   4.081944   3.248059  3.729107   
61    2.010106   2.153279   1.987980   1.863745   1.858602  1.274928   
110   3.678457   2.841803   3.204808   3.180562   2.669391  0.518732   
112   2.361047   2.086066   2.363384   2.107670   1.925623  1.238040   
117   1.475425   1.043443   2.154415   2.582182   0.653862  1.823631   
120   1.791456   1.484426   2.583449   1.440686   2.085819  0.990202   
121   1.791456   1.484426   2.583449   1.440686   2.085819  0.990202   
152   2.610932   2.102459   2.387425   2.549786   2.169187  1.730259   
154  \PYGZhy{}0.529169  \PYGZhy{}0.538525  \PYGZhy{}0.404993  \PYGZhy{}0.331586  \PYGZhy{}0.552513  4.565994   

            G7        G8        G9  
8     1.233344  0.419604  1.582851  
38   10.305875  9.927200  9.055620  
39    3.304890  3.846374  3.149479  
61    1.237283  0.825451  0.955968  
110   0.700361  0.176555  0.727694  
112   1.766328  0.890800  1.377752  
117   1.581227  0.857552  1.188876  
120   1.782081  1.034107  1.822711  
121   1.782081  1.034107  1.822711  
152   2.241549  1.713958  1.592121  
154  \PYGZhy{}0.051854 \PYGZhy{}0.382918 \PYGZhy{}0.536501  
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{range\PYGZus{}df}\PYG{p}{[}\PYG{p}{(}\PYG{n}{range\PYGZus{}df}\PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{any}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{columns}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           G1        G2        G3        G4        G5        G6        G7  \PYGZbs{}
54  \PYGZhy{}1.550758 \PYGZhy{}1.525410 \PYGZhy{}1.843736 \PYGZhy{}2.082897 \PYGZhy{}1.659174 \PYGZhy{}1.203458 \PYGZhy{}1.184772   
56  \PYGZhy{}1.732660 \PYGZhy{}1.510656 \PYGZhy{}2.121128 \PYGZhy{}2.122916 \PYGZhy{}1.781774 \PYGZhy{}1.521614 \PYGZhy{}1.909419   
59  \PYGZhy{}1.971520 \PYGZhy{}1.310656 \PYGZhy{}2.328248 \PYGZhy{}1.175798 \PYGZhy{}2.067838 \PYGZhy{}0.915274 \PYGZhy{}1.783394   
64  \PYGZhy{}1.234727 \PYGZhy{}1.361475 \PYGZhy{}0.736015 \PYGZhy{}1.055741 \PYGZhy{}2.224765 \PYGZhy{}0.839193 \PYGZhy{}0.679357   
65  \PYGZhy{}2.226918 \PYGZhy{}1.194262 \PYGZhy{}2.117429 \PYGZhy{}2.161029 \PYGZhy{}2.043318 \PYGZhy{}0.190202 \PYGZhy{}1.004923   
102 \PYGZhy{}2.484153 \PYGZhy{}2.330328 \PYGZhy{}1.568192 \PYGZhy{}2.808957 \PYGZhy{}1.945239 \PYGZhy{}1.340634 \PYGZhy{}0.846078   

           G8        G9  
54  \PYGZhy{}1.650903 \PYGZhy{}1.245655  
56  \PYGZhy{}1.782746 \PYGZhy{}1.159907  
59  \PYGZhy{}1.304672 \PYGZhy{}1.514484  
64  \PYGZhy{}0.865578 \PYGZhy{}0.663963  
65  \PYGZhy{}0.270565 \PYGZhy{}0.794902  
102 \PYGZhy{}1.691029 \PYGZhy{}0.887601  
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{ensemble} \PYG{k+kn}{import} \PYG{n}{IsolationForest}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{clf} \PYG{o}{=} \PYG{n}{IsolationForest}\PYG{p}{(}\PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{wafer\PYGZus{}df}\PYG{p}{)}
\PYG{n}{wafer\PYGZus{}df}\PYG{p}{[}\PYG{n}{clf}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{wafer\PYGZus{}df}\PYG{p}{)}\PYG{o}{==}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        G1     G2     G3     G4     G5     G6     G7     G8     G9
8    1.396  1.461  1.342  1.122  1.394  1.408  0.924  0.638  1.375
20  \PYGZhy{}0.558 \PYGZhy{}0.705 \PYGZhy{}0.526 \PYGZhy{}0.412 \PYGZhy{}0.753 \PYGZhy{}0.998 \PYGZhy{}0.270  0.598 \PYGZhy{}1.416
38   7.197  8.060  7.223  7.589  7.258  8.310  7.835  8.931  7.824
39   2.190  2.664  2.325  2.430  2.253  3.303  2.502  3.627  2.727
54  \PYGZhy{}0.663 \PYGZhy{}0.695 \PYGZhy{}0.713 \PYGZhy{}0.805 \PYGZhy{}0.749 \PYGZhy{}0.976 \PYGZhy{}0.918 \PYGZhy{}1.168 \PYGZhy{}1.066
56  \PYGZhy{}0.762 \PYGZhy{}0.686 \PYGZhy{}0.863 \PYGZhy{}0.826 \PYGZhy{}0.824 \PYGZhy{}1.252 \PYGZhy{}1.470 \PYGZhy{}1.283 \PYGZhy{}0.992
59  \PYGZhy{}0.892 \PYGZhy{}0.564 \PYGZhy{}0.975 \PYGZhy{}0.329 \PYGZhy{}0.999 \PYGZhy{}0.726 \PYGZhy{}1.374 \PYGZhy{}0.866 \PYGZhy{}1.298
61   1.275  1.549  1.359  1.266  1.403  1.174  0.927  0.992  0.834
65  \PYGZhy{}1.031 \PYGZhy{}0.493 \PYGZhy{}0.861 \PYGZhy{}0.846 \PYGZhy{}0.984 \PYGZhy{}0.097 \PYGZhy{}0.781  0.036 \PYGZhy{}0.677
102 \PYGZhy{}1.171 \PYGZhy{}1.186 \PYGZhy{}0.564 \PYGZhy{}1.186 \PYGZhy{}0.924 \PYGZhy{}1.095 \PYGZhy{}0.660 \PYGZhy{}1.203 \PYGZhy{}0.757
106 \PYGZhy{}0.659 \PYGZhy{}0.451 \PYGZhy{}0.692 \PYGZhy{}0.708 \PYGZhy{}0.595 \PYGZhy{}0.726 \PYGZhy{}1.031 \PYGZhy{}0.877 \PYGZhy{}1.080
110  2.183  1.969  2.017  1.957  1.899  0.518  0.518  0.426  0.637
112  1.466  1.508  1.562  1.394  1.444  1.142  1.330  1.049  1.198
117  0.984  0.872  1.449  1.643  0.666  1.650  1.189  1.020  1.035
120  1.156  1.141  1.681  1.044  1.542  0.927  1.342  1.174  1.582
121  1.156  1.141  1.681  1.044  1.542  0.927  1.342  1.174  1.582
152  1.602  1.518  1.575  1.626  1.593  1.569  1.692  1.767  1.383
\end{sphinxVerbatim}


\chapter{Some practice}
\label{\detokenize{c2_data_preparation/some_practice:some-practice}}\label{\detokenize{c2_data_preparation/some_practice::doc}}
\sphinxAtStartPar
Now that you have learned techniques in data preparation, why don’t you put them to use in this wonderfully horrifying dataset. Good luck!

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{json}

\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kaggle\PYGZus{}dir} \PYG{o}{=} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{expanduser}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZti{}/.kaggle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{exists}\PYG{p}{(}\PYG{n}{kaggle\PYGZus{}dir}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{mkdir}\PYG{p}{(}\PYG{n}{kaggle\PYGZus{}dir}\PYG{p}{)}

\PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{kaggle\PYGZus{}dir}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{/kaggle.json}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}
        \PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{username}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lorenzf}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{key}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{7a44a9e99b27e796177d793a3d85b8cf}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{\PYGZcb{}}
        \PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kaggle}
\PYG{n}{kaggle}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{dataset\PYGZus{}download\PYGZus{}files}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PromptCloudHQ/us\PYGZhy{}jobs\PYGZhy{}on\PYGZhy{}monstercom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{unzip}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{ModuleNotFoundError}\PYG{g+gWhitespace}{                       }Traceback (most recent call last)
\PYG{o}{/}\PYG{n}{tmp}\PYG{o}{/}\PYG{n}{ipykernel\PYGZus{}25600}\PYG{o}{/}\PYG{l+m+mf}{39646943.}\PYG{n}{py} \PYG{o+ow}{in} \PYG{o}{\PYGZlt{}}\PYG{n}{module}\PYG{o}{\PYGZgt{}}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{1} \PYG{k+kn}{import} \PYG{n+nn}{kaggle}
\PYG{g+gWhitespace}{      }\PYG{l+m+mi}{2} \PYG{n}{kaggle}\PYG{o}{.}\PYG{n}{api}\PYG{o}{.}\PYG{n}{dataset\PYGZus{}download\PYGZus{}files}\PYG{p}{(}\PYG{n}{dataset}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{PromptCloudHQ/us\PYGZhy{}jobs\PYGZhy{}on\PYGZhy{}monstercom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{path}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{unzip}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n+ne}{ModuleNotFoundError}: No module named \PYGZsq{}kaggle\PYGZsq{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./data/monster\PYGZus{}com\PYGZhy{}job\PYGZus{}sample.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                    country country\PYGZus{}code date\PYGZus{}added has\PYGZus{}expired  \PYGZbs{}
0  United States of America           US        NaN          No   
1  United States of America           US        NaN          No   
2  United States of America           US        NaN          No   
3  United States of America           US        NaN          No   
4  United States of America           US        NaN          No   

          job\PYGZus{}board                                    job\PYGZus{}description  \PYGZbs{}
0  jobs.monster.com  TeamSoft is seeing an IT Support Specialist to...   
1  jobs.monster.com  The Wisconsin State Journal is seeking a flexi...   
2  jobs.monster.com  Report this job About the Job DePuy Synthes Co...   
3  jobs.monster.com  Why Join Altec? If you’re considering a career...   
4  jobs.monster.com  Position ID\PYGZsh{}  76162 \PYGZsh{} Positions  1 State  CT C...   

                                           job\PYGZus{}title             job\PYGZus{}type  \PYGZbs{}
0               IT Support Technician Job in Madison   Full Time Employee   
1            Business Reporter/Editor Job in Madison            Full Time   
2  Johnson \PYGZam{} Johnson Family of Companies Job Appl...  Full Time, Employee   
3                    Engineer \PYGZhy{} Quality Job in Dixon            Full Time   
4       Shift Supervisor \PYGZhy{} Part\PYGZhy{}Time Job in Camphill   Full Time Employee   

                                            location  \PYGZbs{}
0                                  Madison, WI 53702   
1                                  Madison, WI 53708   
2  DePuy Synthes Companies is a member of Johnson...   
3                                          Dixon, CA   
4                                       Camphill, PA   

                      organization  \PYGZbs{}
0                              NaN   
1          Printing and Publishing   
2  Personal and Household Services   
3                 Altec Industries   
4                           Retail   

                                            page\PYGZus{}url salary  \PYGZbs{}
0  http://jobview.monster.com/it\PYGZhy{}support\PYGZhy{}technici...    NaN   
1  http://jobview.monster.com/business\PYGZhy{}reporter\PYGZhy{}e...    NaN   
2  http://jobview.monster.com/senior\PYGZhy{}training\PYGZhy{}lea...    NaN   
3  http://jobview.monster.com/engineer\PYGZhy{}quality\PYGZhy{}jo...    NaN   
4  http://jobview.monster.com/shift\PYGZhy{}supervisor\PYGZhy{}pa...    NaN   

                       sector                           uniq\PYGZus{}id  
0     IT/Software Development  11d599f229a80023d2f40e7c52cd941e  
1                         NaN  e4cbb126dabf22159aff90223243ff2a  
2                         NaN  839106b353877fa3d896ffb9c1fe01c0  
3   Experienced (Non\PYGZhy{}Manager)  58435fcab804439efdcaa7ecca0fd783  
4  Project/Program Management  64d0272dc8496abfd9523a8df63c184c  
\end{sphinxVerbatim}

\sphinxAtStartPar
Need some inspiration? perhaps \sphinxhref{https://www.kaggle.com/ankkur13/perfect-dataset-to-get-the-hands-dirty}{this} might help!


\part{3. Data Preprocessing}


\chapter{Data Preprocessing}
\label{\detokenize{c3_data_preprocessing/introduction:data-preprocessing}}\label{\detokenize{c3_data_preprocessing/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\chapter{Indexing and slicing}
\label{\detokenize{c3_data_preprocessing/indexing_slicing:indexing-and-slicing}}\label{\detokenize{c3_data_preprocessing/indexing_slicing::doc}}
\sphinxAtStartPar
In

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{https://raw.githubusercontent.com/LorenzF/data\PYGZhy{}science\PYGZhy{}practical\PYGZhy{}approach/main/src/c2\PYGZus{}data\PYGZus{}preparation/data/temperatures/australia/melbourne/1981.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
           Date  Temp
0    1981\PYGZhy{}01\PYGZhy{}01  20.7
1    1981\PYGZhy{}01\PYGZhy{}02  17.9
2    1981\PYGZhy{}01\PYGZhy{}03  18.8
3    1981\PYGZhy{}01\PYGZhy{}04  14.6
4    1981\PYGZhy{}01\PYGZhy{}05  15.8
..          ...   ...
360  1981\PYGZhy{}12\PYGZhy{}27  15.5
361  1981\PYGZhy{}12\PYGZhy{}28  13.3
362  1981\PYGZhy{}12\PYGZhy{}29  15.6
363  1981\PYGZhy{}12\PYGZhy{}30  15.2
364  1981\PYGZhy{}12\PYGZhy{}31  17.4

[365 rows x 2 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{Date} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{to\PYGZus{}datetime}\PYG{p}{(}\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{Date}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df} \PYG{o}{=} \PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1981\PYGZhy{}06\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1981\PYGZhy{}06\PYGZhy{}30}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
            Temp
Date            
1981\PYGZhy{}06\PYGZhy{}01  11.6
1981\PYGZhy{}06\PYGZhy{}02  10.6
1981\PYGZhy{}06\PYGZhy{}03   9.8
1981\PYGZhy{}06\PYGZhy{}04  11.2
1981\PYGZhy{}06\PYGZhy{}05   5.7
1981\PYGZhy{}06\PYGZhy{}06   7.1
1981\PYGZhy{}06\PYGZhy{}07   2.5
1981\PYGZhy{}06\PYGZhy{}08   3.5
1981\PYGZhy{}06\PYGZhy{}09   4.6
1981\PYGZhy{}06\PYGZhy{}10  11.0
1981\PYGZhy{}06\PYGZhy{}11   5.7
1981\PYGZhy{}06\PYGZhy{}12   7.7
1981\PYGZhy{}06\PYGZhy{}13  10.4
1981\PYGZhy{}06\PYGZhy{}14  11.4
1981\PYGZhy{}06\PYGZhy{}15   9.2
1981\PYGZhy{}06\PYGZhy{}16   6.1
1981\PYGZhy{}06\PYGZhy{}17   2.7
1981\PYGZhy{}06\PYGZhy{}18   4.3
1981\PYGZhy{}06\PYGZhy{}19   6.3
1981\PYGZhy{}06\PYGZhy{}20   3.8
1981\PYGZhy{}06\PYGZhy{}21   4.4
1981\PYGZhy{}06\PYGZhy{}22   7.1
1981\PYGZhy{}06\PYGZhy{}23   4.8
1981\PYGZhy{}06\PYGZhy{}24   5.8
1981\PYGZhy{}06\PYGZhy{}25   6.2
1981\PYGZhy{}06\PYGZhy{}26   7.3
1981\PYGZhy{}06\PYGZhy{}27   9.2
1981\PYGZhy{}06\PYGZhy{}28  10.2
1981\PYGZhy{}06\PYGZhy{}29   9.5
1981\PYGZhy{}06\PYGZhy{}30   9.5
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1989\PYGZhy{}06\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1989\PYGZhy{}06\PYGZhy{}30}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Temp   NaN
dtype: float64
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{min\PYGZus{}temp\PYGZus{}df}\PYG{o}{.}\PYG{n}{resample}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MS}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
                 Temp
Date                 
1981\PYGZhy{}01\PYGZhy{}01  17.712903
1981\PYGZhy{}02\PYGZhy{}01  17.678571
1981\PYGZhy{}03\PYGZhy{}01  13.500000
1981\PYGZhy{}04\PYGZhy{}01  12.356667
1981\PYGZhy{}05\PYGZhy{}01   9.490323
1981\PYGZhy{}06\PYGZhy{}01   7.306667
1981\PYGZhy{}07\PYGZhy{}01   7.577419
1981\PYGZhy{}08\PYGZhy{}01   7.238710
1981\PYGZhy{}09\PYGZhy{}01  10.143333
1981\PYGZhy{}10\PYGZhy{}01  10.087097
1981\PYGZhy{}11\PYGZhy{}01  11.890000
1981\PYGZhy{}12\PYGZhy{}01  13.680645
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}df} \PYG{o}{=} \PYG{n}{sns}\PYG{o}{.}\PYG{n}{load\PYGZus{}dataset}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tips}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{tip\PYGZus{}df}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
   total\PYGZus{}bill   tip     sex smoker  day    time  size
0       16.99  1.01  Female     No  Sun  Dinner     2
1       10.34  1.66    Male     No  Sun  Dinner     3
2       21.01  3.50    Male     No  Sun  Dinner     3
3       23.68  3.31    Male     No  Sun  Dinner     2
4       24.59  3.61  Female     No  Sun  Dinner     4
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}index\PYGZus{}df} \PYG{o}{=} \PYG{n}{tip\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}index\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
     total\PYGZus{}bill   tip     sex smoker    time  size
day                                               
Sun       16.99  1.01  Female     No  Dinner     2
Sun       10.34  1.66    Male     No  Dinner     3
Sun       21.01  3.50    Male     No  Dinner     3
Sun       23.68  3.31    Male     No  Dinner     2
Sun       24.59  3.61  Female     No  Dinner     4
..          ...   ...     ...    ...     ...   ...
Sun       20.90  3.50  Female    Yes  Dinner     3
Sun       30.46  2.00    Male    Yes  Dinner     5
Sun       18.15  3.50  Female    Yes  Dinner     3
Sun       23.10  4.00    Male    Yes  Dinner     3
Sun       15.69  1.50    Male    Yes  Dinner     2

[76 rows x 6 columns]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}index\PYGZus{}df} \PYG{o}{=} \PYG{n}{tip\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}index\PYGZus{}df}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Thur}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Lunch}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tip}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/tmp/ipykernel\PYGZus{}25625/2537502835.py:1: PerformanceWarning: indexing past lexsort depth may impact performance.
  tip\PYGZus{}index\PYGZus{}df.loc[(\PYGZsq{}Thur\PYGZsq{},\PYGZsq{}Lunch\PYGZsq{})].tip.mean()
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
2.767704918032786
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pd}\PYG{o}{.}\PYG{n}{pivot\PYGZus{}table}\PYG{p}{(}\PYG{n}{tip\PYGZus{}df}\PYG{p}{,} \PYG{n}{values}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{total\PYGZus{}bill}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{aggfunc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{median}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
time  Lunch  Dinner
day                
Thur  16.00  18.780
Fri   13.42  18.665
Sat     NaN  18.240
Sun     NaN  19.630
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tip\PYGZus{}df}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sex}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{smoker}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{loc}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Male}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dinner}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Yes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
/tmp/ipykernel\PYGZus{}25625/3467525553.py:1: PerformanceWarning: indexing past lexsort depth may impact performance.
  tip\PYGZus{}df.set\PYGZus{}index([\PYGZsq{}sex\PYGZsq{}, \PYGZsq{}time\PYGZsq{},\PYGZsq{}smoker\PYGZsq{}]).loc[(\PYGZsq{}Male\PYGZsq{}, \PYGZsq{}Dinner\PYGZsq{},\PYGZsq{}Yes\PYGZsq{})][\PYGZsq{}tip\PYGZsq{}].mean()
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
3.123191489361702
\end{sphinxVerbatim}


\part{4. Data Exploration}


\chapter{Data Exploration}
\label{\detokenize{c4_data_exploration/introduction:data-exploration}}\label{\detokenize{c4_data_exploration/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\part{5. Data Visualisation}


\chapter{Data Visualisation}
\label{\detokenize{c5_data_visualisation/introduction:data-visualisation}}\label{\detokenize{c5_data_visualisation/introduction::doc}}
\sphinxAtStartPar
this is an introduction


\part{6. Machine Learning}


\chapter{Machine Learning}
\label{\detokenize{c6_machine_learning/introduction:machine-learning}}\label{\detokenize{c6_machine_learning/introduction::doc}}
\sphinxAtStartPar
this is an introduction







\renewcommand{\indexname}{Index}
\printindex
\end{document}